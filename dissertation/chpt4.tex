\chapter{Metaheuristic Algorithms}
\label{chpt:heuristic}
\section{Introduction}
Metaheuristics is a subdomain of the artificial intelligence domain\cite{AIModernApproach}. It evolved out of a need for more efficient search techniques with regard to hard problems. 

Metaheuristics forms part of a collective body of algorithms that use heuristics to search a particular domain's problem space for the most optimal solution adhering to certain hard and soft constraints\cite{AIModernApproach,NatureInspiredMetaHeuristic}. 

A hard constraint is defined as a certain condition an algorithm or potential solution is not allowed to violate\cite{AIModernApproach,NatureInspiredMetaHeuristic,Karen2004,Eisenblatter}. A soft constraint is allowed to be violated but there is some sort of penalty or cost involved which is imposed onto the potential solution, which lowers its desirability\cite{AIModernApproach,NatureInspiredMetaHeuristic,Karen2004,Eisenblatter}. 

An optimal solution would therefore be any solution that violates no hard constraints and violates no or a minimum number of soft constraints\cite{AIModernApproach,NatureInspiredMetaHeuristic,Karen2004,Eisenblatter}.

Algorithms that are classified as being part of the collective body of algorithms known as metaheuristic algorithms are Tabu Search\cite{TabuVechicleRoutingWithTimeWindows,TabuCSP}, Simulated Annealing \cite{SASingleMultiObj,CurveFittingSA} and Genetic Algorithm\cite{GATSP, GeostatisticalGA}.

The above-mentioned algorithms are not the only algorithms to form part of this subdomain, but they are the algorithms that have received the most attention in the literature and produce good results\cite{SweepMeta}.

The main focus of this chapter will be on each of the above algorithms. Before each of the algorithms is discussed, a brief overview is given of the various characteristics that metaheuristic algorithms exhibit. 

AI search algorithms operate in search spaces, where they occupy various states while they are searching for a solution or goal state in a search space. Before these algorithms can be properly introduced, these concepts need to be discussed briefly.

\section{Search Spaces and States}
A search space is defined as a set of candidate goal states which may or may not be a solution to a problem\cite{AIModernApproach}. The candidate states are defined by the problem definition\cite{AIModernApproach}. The possible states can given by a successor function which generates new states adhering to the problem defined constraints if any\cite{AIModernApproach}. 

The search space can also be explicitly defined with constraints to which as solution must adhere to\cite{AIModernApproach}. When constraints are defined it is up to the algorithm how it moves from one state to another in the search space\cite{AIModernApproach}. Each state the algorithm moves to is then checked whether it adheres to the constraints\cite{AIModernApproach}.

A state is a position in the search space\cite{AIModernApproach}. This position represents a configuration that describes a possible solution to the defined problem\cite{AIModernApproach}. When a algorithm is said to be moving to a new state, the algorithm moves to a new position in the search space\cite{AIModernApproach}. This new position represents a diffirent solution than the previous position that was occupied\cite{AIModernApproach}.

When dealing with optimisation problem there can be more than one solution\cite{AIModernApproach}. Which is why with optimisation problems not only does an algorithm have to find a solution, but also the most optimal solution among the candidate solutions\cite{AIModernApproach}. This subset of solutions in the search space is referred to as the \emph{solution} space.

Both concepts of search spaces and states have now been defined. All search algorithms operate in search spaces and occupy states. The domain of search contains a wide variety of algortihms. In the next section a Metaheuristics Algorithms will be discussed as this body of algorithms are used to solved optimisation problems.

\section{Metaheuristics Algorithms}
NP-Complete\footnote{A discussion of NP-Complete problems is appears in section \ref{sec:NPComplete}} problems have been proven to not be solvable in polynomial time by traditional uninformed search methods such as Breath First Search and Depth-First Search\cite{AIModernApproach}. 

Uninformed algorithms are not able to distinguish whether a particular goal state is more ``correct'' than any other goal state\cite{AIModernApproach}.

Uninformed search algorithms search spaces are represented by a tree structure\cite{AIModernApproach}. A search is performed in the search space by starting at a root node, the algorithm expands successive nodes based on a strategy\cite{AIModernApproach}. Bread-first search continiously expands all successive nodes and Depth-First search expands the deepest node first\cite{AIModernApproach}.

The path taken to a final  goal state node represents a solutioncite\cite{AIModernApproach}. In a worst-case scenario, an uninformed search will expand every single node in its search space, thus each and every single possible solution is evaluated\cite{AIModernApproach}.

It is not always viable to test every possible solution in a given problem search space, especially in NP-Complete problems, since their search spaces are usually huge or infinite. This is why traditional algorithms are not able to produce optimal solutions in polynomial time\cite{AIModernApproach}.

With NP-Complete problems, the path taken from an initial possible solution to a final optimal solution is irrelevant \cite{AIModernApproach}. Only the final optimal solution is needed. Algorithms that disregard the path taken to a solution and that only care about the final solution are classified as \emph{local search} algorithms\cite{AIModernApproach}.

Local search algorithms tend to use a small amount of memory since the algorithm is only concerned with the current state and only moves to a neighbouring state when searching for a potential solution. This makes local search algorithms a good fit for optimisation problems as an optimised solution is just a solution which constitutes to the current best state found by the algorithm \cite{AIModernApproach,NonlinearGlobalTabu}. 

In its most basic form, a local search algorithm is just an algorithm that moves from one state to another. How it generates neighbouring states to move to and how and why it moves to a certain state makes the local search algorithm unique \cite{AIModernApproach,CompuIntelligenceIntro,NonlinearGlobalTabu}.

Metaheuristic algorithms are considered to be \emph{general-purpose} algorithms and can thus be applied to a wide variety of optimisation problems with only small modifications that need to made to the algorithm model\cite{MetaGraph}.

As can be gathered from the name, a metaheuristic algorithm uses some sort of heuristic. A heuristic is a decision rule. Algorithms use heuristics to make decisions by applying the heuristic to the data the algorithm is currently working on\cite{AIModernApproach,NatureInspiredMetaHeuristic}.

When an algorithm utilises a heuristic its forms part of a group known as informred search methods. As the name implies the algorithm is more ``informed'' using the heuristic the algorithm gains additional information about its search space. With this additional information the algorithm is able to performa a better search for a possible solution\cite{AIModernApproach}.

A heuristic is able to dictate what an algorithm must do for its next iteration by evaluating the current internal state of the algorithm, i.e. whether it should move to a different point in the solution space, generate new data, or select the current data as the most optimal solution\cite{AIModernApproach,NatureInspiredMetaHeuristic}.

In general ``meta'' means \emph{beyond} or \emph{higher level}\cite{AIModernApproach,NatureInspiredMetaHeuristic}. A metaheuristic therefore refers to a heuristic that is more complex with regard to the decisions it is able to make compared with a standard heuristic\cite{AIModernApproach,NatureInspiredMetaHeuristic}. With a standard heuristic only the current state is considered\cite{AIModernApproach}. A metaheuristics takes additional information into account when considering a current state\cite{NatureInspiredMetaHeuristic}. This additional information can be previous states, or states that are considering to be close in the search space\cite{AIModernApproach, NatureInspiredMetaHeuristic}.

In the literature algorithms are classified as being of the meta-heuristic variant when the heuristics used by the algorithm, satisfy the following criteria\cite{AIModernApproach,NatureInspiredMetaHeuristic}:
\begin{itemize}
	\item Use a local search
	\item Exhibits non-deterministic behaviour when moving to another state in the search space therefore making stochastic.
\end{itemize}

Metaheuristic algorithms do not search the solution space statically by testing and evaluating every possible permutation in the solution space. This is inefficient and a normal path-based algorithm can be used instead\cite{MetaAgricultural}.

Metaheuristic algorithms make use of strategies and heuristics (specific to the problem domain) to search the solution space intelligently through trial and error \cite{MetaAgricultural}. Heuristics used by the algorithms take more information into account based on the current state of the algorithm which allows the algorithm to make a more informed decision to move and occupy a solution in the search space\cite{AIModernApproach}. Strategies used by these algorithms can include recording good/bad solutions, introducing new search spaces or even adding additional rewarding states\cite{AIModernApproach}. Intelligent searching is accomplished by combining the local search strategy along with a heuristic\cite{NatureInspiredMetaHeuristic,AIModernApproach,CompuIntelligenceIntro}. 

These algorithms iteratively move through the solution space, using a heuristic to guide the search to move to more desirable regions in the solution space where there is a high probability of obtaining high quality candidate solutions \cite{TabuMontemanniSmith,SweepMeta}.

Metaheuristic-based search methods are not guaranteed to find the most optimal solutions in the solution space; instead these methods are usually used to find near-optimal solutions. Thus most algorithmic development in the metaheuristic domain focuses on developing new techniques that will increase the probability that a good solution will be obtained in difficult combinatorial problems \cite{MetaAgricultural}.

Similarly, metaheuristics is not guaranteed to find suitable solutions or perform well in each problem domain it is applied to. The quality of the solution and performance of the metaheuristic is very much dependent upon on the expertise of the algorithm designer \cite{AutoComplexMeta}. 

The standard metaheuristic algorithm will not take advantage of specific domain knowledge as the algorithm is designed to be general purpose. Therefore for the algorithm to exploit the search domain the algorithm designer needs to embed domain-specific knowledge\cite{AutoComplexMeta,AIModernApproach}. Otherwise the algorithm will search the solution space generally and will produce relatively poor results\cite{AutoComplexMeta,AIModernApproach}.

Although heuristics plays a key role in the performance of metaheuristic algorithms, it is not the only factor that has an impact on performance and results. Algorithms also use techniques and concepts from other system paradigms like multi-agent systems\cite{Self-AdaptiveMeta}. 

In multi-agent systems, multiple agents have to communicate with each other and the system as a whole has to perform some sort of autonomous self-organisation\cite{Self-AdaptiveMeta}.

These social and self-organisation concepts enable these systems to be distributed, robust and flexible. This is why in metaheuristic algorithms that are population-based, hybrid and/or distributed, these concepts of social and self-organisation  are used to better exploit the solution space\cite{Self-AdaptiveMeta}.

In this section the characteristics of metaheuristics that set these algorithms apart from the conventional algorithms used on difficult problems was introduced. In the next section of this chapter the tabu search algorithm is discussed.
\section{Tabu Search}
\label{sec:tabusearch}
\subsection{Introduction}
\label{sec:TSIntroduction}
Tabu search (TS) was first proposed by Glover\cite{Glover89} as a new searching technique to help algorithms avoid getting trapped in local optima in combinatorial and optimisation problems \cite{TabuRCAProblem}. Since Glover introduced the algorithm in the 1980s, tabu search has been applied to a wide range of problems such as the vehicle routing problem\cite{TabuVechicleRoutingWithTimeWindows}, FAP\cite{TabuMontemanniSmith}, capacitated-lot sizing problem\cite{TabuCarryOver}, Nurse Scheduling\cite{TabuNurse} and the Resource Constrained Assignment Problem\cite{TabuRCAProblem}. 

Even though the problems mentioned differ by a large margin, the algorithm has been successful in most optimisation problems it has been applied to. If one observes the results obtained in research \cite{TabuMontemanniSmith,tabuglobalplanning3g}, it can be inferred that tabu search has on average obtained the best results compared with previous attempts with other algorithms. 

Tabu search (TS) resembles in its most basic form the hill-climbing search algorithm\cite{TabuBiddingStrats}. The hill-climbing search algorithm starts from an initial solution and then iteratively moves from the current solution to a neighbouring solution\cite{AIModernApproach}. Each neighbour is rated based on its attractiveness as a possible optimal solution that the algorithm is being applied to\cite{AIModernApproach}. 

The hill-climbing algorithm moves to the neighbour with the highest rating without considering whether the neighbour might lead the algorithm astray, to a position where the neighbours are in fact \emph{worse} than previously encountered possible solutions\cite{AIModernApproach}. 

The TS algorithm addresses this shortcoming by introducing the concept of memory\cite{TabuBiddingStrats}. The memory of the algorithm is actually a history of previous solutions that the algorithm has moved to in its search for the problem space for a solution\cite{TabuBiddingStrats}. 

General search algorithms like hill-climbing, random-restart\footnote{Random-restart is a search algorithm where once a certain trend of repeated moves is noticed, the algorithm restarts by generating a new initial solution to start from and then continues its search process from that generated solution\cite{AIModernApproach}.} or scatter search tend to get trapped in local optima \cite{AIModernApproach}. The local optima might be a very attractive solution and thus general search algorithms will not move to better solutions since, according to the algorithm's built-in strategy, it has found the best solution. 

In actual fact the solution that was found is the best solution in the \emph{local} search space, but not in the \emph{global} search space\cite{CompuIntelligenceIntro,AIModernApproach}. Therefore an important characteristic of algorithms being applied to optimisation problems is breaking out of local optima\cite{CompuIntelligenceIntro,AIModernApproach}.

The next section discussed some of the characteristics that make the TS algorithm unique.

\subsection{Important Tabu Search Characteristics}
\label{sec:TScharacteristics}
Various characteristics are important to the TS algorithm. The first characteristic is exactly how the TS algorithm iteratively improves upon the initial start solution.

\subsubsection{Initial Solution Generation}
The core feature of the TS algorithm is to sequentially improve an initial solution \cite{TSHazardous}. An initial possible solution is a point in the problem space where the TS algorithm will \emph{start} exploring in search of a more optimal solution \cite{AIModernApproach,TSHazardous}.

An important consideration one has to make is how initial solutions are generated for the TS algorithm to start on\cite{AIModernApproach,TSHazardous}.

Random initial solutions might seem to be a good starting point, but by introducing randomisation it becomes hard to control the quality of the end solution\cite{TSHazardous}. Hence the generation of starting solutions must be controlled to limit the infeasibility of potential solutions \cite{TSHazardous}. 

Control of the randomly generation solutions can be achieved by simply constraining the random solution generator to only generate initial starting points in a bounded subset of the entire search space. For example: instead of letting the random initial starting point be any number between positive infinity and negative infinity, the random number generator is constrained to only generate numbers between 5 and -5.

\subsubsection{Neighbourhood Search}
The following discussion on neighbourhood search is not meant to be an exhaustive survey on the different methods and how they differ under different problems. Instead the discussion is ment as a general overview to get an idea of neighborhood generation in the TS algorithm context. 

The following neighbourhood discussion will be based on the assumption that the underlying problem the TS is applied to, has a search space with defined boundaries that is suitable for neighborhood generation.

TS uses a neighbourhood local search process to explore the solution space. There is no set process of how neighbourhood candidate solutions are selected as it is problem dependant. The overall quality of the solution produced by TS is also dependent on the neighbourhood search strategy used \cite{TSHazardous}. 

The neighbourhood search phase is the first operation performed after the algorithm has been initialised, which is to say the algorithm has generated an initial starting solution from which the exploration process can start.

The neighbourhood search phase is the primary means for the TS algorithm to search the solution space for an optimal solution. It is within this phase that new possible solutions must be presented for the TS heuristic to allow the algorithm to decide to which solution it must move next.

The new possible solutions that are generated are called neighbouring solutions; hence the TS algorithm always moves to a neighbouring solution. When the TS algorithm moves to a neighbouring solution, the current solution is replaced by the neighbouring solution. Therefore, in the next iteration, neighbours for the new solution need to be generated.

Generation of new neighbours can range from a simple increment option to a complex operation that incorporates additional intelligence by means of a more heuristic approach to generate new neighbours.

The TS algorithm is not limited to just one neighbourhood search strategy. In the paper by Gopalakrishnan et al.\cite{TabuCarryOver} five neighbourhood move strategies are developed and are used interchangeably; in some cases a strategy is used three times in a row due to stagnation in the search space. 

Stagnation occurs when the algorithm does not move to a better solution; instead it opts to stay on the current solution, as no neighbouring solution is better than the current one. 

Other neighbourhood strategies developed is that by N. A. Wassan \cite{ReactiveTabuVHR}. Wassan used a neighbourhood selection strategy that exchanges route nodes from initial vehicle routes for the vehicle routing problem. This route exchange enables the TS algorithm to search much more broadly due to the constant supply of different solutions. 

Since initial solutions are constantly modified, it enables the TS procedure to be a very fined-grained process, because often a small change in a potential solution can have a big impact on the overall proposed solution by the TS algorithm.

In the research done by Zhang et. al.\cite{TSHazardous} a neighbourhood selection scheme called \emph{dynamic penalty} is developed. When the algorithm moves onto an infeasible solution a penalty is imposed. By dynamically changing the penalty that is imposed the ``feasibility'' of solutions produced is influenced. 

Therefore, if and when the algorithm continually produces infeasible solutions, the penalty imposed is increased to guide the algorithm to produce more feasible solutions. Finally, when the algorithm becomes trapped at local optima, the penalty is reduced, which allows the algorithm to consider moving onto infeasible solutions thus escaping local optima.

Considering all the research done to develop new neighbourhood selection strategies that improve TS to search the solution space more efficiently and produce better solutions, TS still has some drawbacks, especially with problems that have very large solution spaces \cite{EvoParallelTabu}.

TS is an iterative algorithm, executing a set of operations sequentially until a stopping criterion is met\cite{EvoParallelTabu,TabuVechicleRoutingWithTimeWindows}. At each iteration the algorithm has to determine feasibility of the immediate neighbourhood candidate solutions \cite{EvoParallelTabu,TabuVechicleRoutingWithTimeWindows}. 

Therefore each candidate must be evaluated by some function, which may be a costly operation in terms of computational cycles as well as in terms of time\cite{EvoParallelTabu,TabuVechicleRoutingWithTimeWindows}. This constant evaluation can drastically reduce the overall performance of the algorithm, since it is spending more time calculating feasibility than actually searching the solution space \cite{EvoParallelTabu,TabuVechicleRoutingWithTimeWindows}. 

\subsubsection{Memory Structures of Tabu Search}
The hill-climbing and random-restart algorithms are able to break out of local minima, but there is nothing stopping these algorithms from avoiding the local optima with their second or n-pass in the search space. TS addresses the shortcoming of these algorithms by incorporating an important concept: the notion of memory.

In its most basic form TS keeps a local memory of all its recent best moves, and puts them into a \emph{tabu list} that has a predefined size. In the literature the Tabu list is also referred to as the \emph{tabu tenure} \cite{TSHazardous,TabuCarryOver,ReactiveTabuVHR,TabuParameterization}. The algorithm is not allowed to move to any solution that is in the tabu list unless a solution that is \emph{tabu} is better than any current moves available in the immediate search neighbourhood \cite{TSHazardous,TabuCarryOver,ReactiveTabuVHR,TabuParameterization}. The process of overriding a solution's tabu status in the tabu tenure is called the \emph{aspiration criterion} \cite{TSHazardous,TabuCarryOver,ReactiveTabuVHR,TabuParameterization}. With the use of the tabu tenure and the aspiration criterion, the algorithm is able to avoid cycling, local optima as well as searching in a too narrow region \cite{TabuSingleMachineScheduling,CircuitTabu}.

Research done by Ashish Sureka and Peter R. Wurman makes an important distinction with regard to the memory scheme that is used in the TS algorithm. Two memory schemes are discussed: \emph{explicit memory} and \emph{attribute-based memory} \cite{TabuBiddingStrats,TabuFormGames}. Of the two memory schemes the explicit memory scheme is the most used in the literature \cite{TabuVechicleRoutingWithTimeWindows}.

With explicit memory the algorithm stores a complete solution in the tabu tenure; hence the algorithm is prohibited from moving to that position in the solution for as long as the solution is in the Tabu tenure\cite{TabuBiddingStrats,TabuFormGames}. With attribute-based memory the algorithm stores the \emph{operation} used to move from the previous solution to the current solution\cite{TabuBiddingStrats,TabuFormGames}. Therefore with attribute-based memory the tabu tenure intended function is changed from prohibiting certain solutions already encountered to rather prohibiting making changes to the current solution that would lead to solutions already present in the tabu tenure \cite{TabuBiddingStrats,TabuFormGames}.

In research conducted by D. M. Jaeggi, G.T. Parks, T. Kipouros and P. J. Clarkson \cite{MultiObjTabu}, the authors add two additional memory structures called \emph{medium term memory} (MTM) and \emph{long term memory} (LTM) besides the standard \emph{short term memory} (STM), typically referred to as the tabu List \cite{MultiObjTabu}. Each additional structure remembers a different set of solutions for use by the diversification and intensification phases in the algorithm.

STM is similar to the traditional tabu list: to store the most recent solutions produced by the algorithm. MTM is designed to remember optimal or near-optimal solutions. These solutions are therefore used later in the intensification phase. Finally, the LTM structure stores all the solutions that the algorithm has already explored and is thus used in the diversification phase of the algorithm \cite{MultiObjTabu}.

\subsubsection{Search Phases}
\label{TSSearchPhases}
As TS searches through the search space, it goes through two cycles of search phases called \emph{diversification} and \emph{intensification} \cite{TabuParameterization,TabuCrewSchedulingProblem,NonlinearGlobalTabu,SelfControllingReactiveTabu}.

The diversification phase in the TS algorithm is the phase where the algorithm is directed to areas in the search space that has not yet been explored. The algorithm usually applies diversification as mechanisms monitoring the memory; note that solutions being produced are being repeated \cite{ReactiveTabuVHR,SelfControllingReactiveTabu}. 

Research by Fescioglu-Unver and Kokar \cite{SelfControllingReactiveTabu} provides a strategy that consists of two components namely the \emph{observer} and the \emph{diversifier}. The goal of the observer is to continually monitor the best solution obtained by the algorithm as to whether it violates the \emph{stagnation period}. The stagnation period is defined as the number of iterations where the current best obtained solution has not changed and the algorithm has not moved to a new solution\cite{SelfControllingReactiveTabu}. 

As soon as the current solution exceeds the stagnation period the observer component activates and transfers the necessary information needed by the diversifier component. The diversifier component dynamically changes the size of the tabu tenure based on the information the observer gathered. The diversifier mainly targets older moves to diversify, but for short bursts of time it decreases the tabu list size to a very small value in an attempt to combine new and old moves \cite{SelfControllingReactiveTabu}.

The specific mechanism used to define a new position where the algorithm can continue to search, should ideally select areas in the search space that have not been explored yet. Therefore, the diversification phase typically makes extensive use of the knowledge present in the long-term memory structures as an indication of what areas of the search space have been previously explored and which areas have not \cite{TabuParameterization,TabuCrewSchedulingProblem,NonlinearGlobalTabu,SelfControllingReactiveTabu}.

Intensification is usually the first phase of the TS algorithm, since it is responsible for building up a history in memory on which the diversification phase can act. Fescioglu-Unver and Kokar also present an intensification strategy based on control theory in their research \cite{SelfControllingReactiveTabu}. The authors identify repetition length as a critical value for their intensification strategy to be based upon. The repetition length is a control measure that defines how many times the algorithm can occupy the same solution within a span of iterations.

In the following section, an overview is presented of the flow the TS algorithm along with pseudo code describing the TS algorithm.
\subsection{Flow of the algorithm}
In this section the general flow of the TS algorithm is described using the algorithm~\ref{alg:TS} presented in pseudo code as a reference point.
\begin{algorithm}[H]
\caption{Basic Tabu Search Algorithm\cite{TabuRCAProblem,TabuMontemanniSmith}}
\label{alg:TS}
	\begin{algorithmic}[1]
		\State Initialize parameters
		\State $x_0 \leftarrow$ Initialize starting solution
		\While{stopping criteria not met}
			\State $y_i \leftarrow$ Determine $x_i$ neighbourhood solutions 
			\State Evaluate neighbouring solutions with fitness function $f(y_i)$
			\State $z_i \leftarrow$Select best neighbour from $y_i$
			\If{Move to $z_i$ is Tabu}
				\If{$z_i$ meets Aspiration Criterion}
					\State $x_i \leftarrow z_i$
				\EndIf
			\algstore{TS}
	\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
\caption{Basic Tabu Search Algorithm (continued) \cite{TabuRCAProblem,TabuMontemanniSmith}}
\label{alg:TS2}
	\begin{algorithmic}[1]
			\algrestore{TS}
			\Else
				\State Add $x_i$ to Tabu List
				\State $x_i \leftarrow z_i$
				\If{$x_i$ repeated $\ge$ max repeats}
					\State diversify()
				\Else
					\State intensify()
				\EndIf
			\EndIf
		\EndWhile
		\State Return $x_i$ as best found solution
	\end{algorithmic}
\end{algorithm}

Before the algorithm can actually start searching, it first needs to initialise various parameters. These parameters include, but are not limited to, the tabu list size, the aspiration criterion and the starting solution. The initialisation can be observed to occur from lines 1 - 2.

Once all the various parameters that are needed by the algorithm have been initialised, the algorithm is ready to enter the actual search phase, which ranges from lines 3 -- 21. 

The search phase starts off by first generating possible solutions that neighbour the current solution $x_i$ as can be observed in line 4. Generating neighbouring solutions are a critical process in the TS algorithm as tehy are the means by which the algorithm is able to move from one possible solution to the next in the search space.

After all the solutions that neighbour the current possible solution have been generated, the algorithm needs to decide which of the possible neighbours is the most rewarding. The algorithm therefore determines the fitness of each neighbour $y_i$ by applying a fitness function $f(y_i)$. 

Once all the neighbours have been evaluated, the algorithm selects the best neighbour that not only has the best fitness out of all the generated neighbours, but also has a better fitness than the current solution held by the algorithm. The best neighbour selection can be seen to occur in line 6.

The algorithm has now determined a possible neighbour $z_i$ to move towards. Before moving on to the next iteration, it first needs to perform a series of checks that will aid it in the search process.

The first check that needs to be performed is whether the neighbour $z_i$ is in the tabu list and this occurs in line 7. 

If the neighbour $z_i$ is in the tabu list, then another check is performed where the aspiration criterion is calculated as can be seen in lines 8 -- 10. The aspiration criterion determines whether the algorithm can make neighbour $z_i$ its current solution once more even though it is tabu. 

If the aspiration criterion has been met, the algorithm makes neighbour $z_i$ its current solution $x_i$. 

In the algorithm, if a neighbour $z_i$ is found not to be in the tabu list, the algorithm then adds the currently held solution $x_i$ to the tabu list. The current solution is added to prohibit future movements to the same solution in an attempt to avoid cycling of solutions. After $x_i$ has been added to the tabu list, the algorithm makes $z_i$ the current solution $x_i$. This process can be observed from lines 12 -- 14.

Before the algorithm continues to the next iteration, it performs one last final check. The purpose of this check is to determine whether the algorithm is repeating solutions. As can be observed from lines 15 -- 19, the algorithm calculates whether the new selected solution has been repeated for a certain number of iterations. 

If the solution has indeed been repeated for a predetermined number of iterations, the algorithm activates its diversification strategy or intensifies its search.

The section that follows presents an overview of the TS being applied to the FAP.
\subsection{Tabu Search on the FAP}
In a study conducted by Robert Montemanni and Derek Smith \cite{TabuMontemanniSmith} the TS algorithm is used on the FS-FAP. The authors had to make some alterations to the algorithm to suit their needs as well as to make the algorithm more efficient in exploring in the FAP solution space.

The TS algorithm used by the authors is the multistart TS algorithm, which randomly starts on different initial solutions \cite{TabuMontemanniSmith}.

The authors developed a technique called heuristic manipulation technique (HMT). HMT first monitors an underlying heuristic being used on the problem by the algorithm\cite{TabuMontemanniSmith}. It then identifies certain characteristics that good solutions exhibit. In the FAP, it is transmitters that are assigned different frequencies which results in an overall lower interference value\cite{TabuMontemanniSmith}.

The HMT then uses the identified characteristics to add \emph{additional} constraints to the problem\cite{TabuMontemanniSmith}. By adding constraints, the search space is reduced. However, by reducing the search space, other near-optimal solutions which might be far better are excluded\cite{TabuMontemanniSmith}. It is for this reason that the authors opted not to add the constraints permanently.

The authors applied their TS algorithm together with HMT to the COST 259 family of benchmarks, specifically the Siemens1, Siemens2, Siemens3 and Siemens4 problems. The results are presented in table~\ref{TSCOST259}. The values presented are scalar and indicate the total interference of the frequency plan. The lower the interference value is, the better the frequency plan.
\begin{table}[H]
\centering
	\begin{tabular}{| c | c | c |}
		\hline
		Problem instance & TS with HMT & Best COST 259 \\ \hline
		Siemens1 & 2.7692 & 2.200 \\ \hline
		Siemens2 & 14.9360 & 14.280 \\ \hline
		Siemens3 & 6.6496 & 5.19 \\ \hline
		Siemens4 & 110.9725 & 81.89 \\ \hline
	\end{tabular}
\caption{Results of applying TS with HMT on COST 259}
\label{TSCOST259}
\end{table}
As can be observed from the results obtained by the authors, the TS algorithm with HMT produces results that rank very favourably against other algorithms also applied to the COST 259.

When critically reasoning about the TS algorithm with regard to applying it to the FAP, the following disadvantages in theory can be identified:
\paragraph{Search based on a single solution}
--- The TS algorithm at any moment in time only searches in the vicinity of \emph{one} current solution for possible neighbours that might be the current solution for the next iteration. FAPs typically have huge search spaces due to their NP-Complete nature. Therefore, only searching for possible rewarding neighbours from only potential solutions seems to be terribly inefficient. A better strategy would be to use the notion of population-based algorithms and have multiple solutions from which more rewarding neighbours are searched.
\paragraph{Neighbourhood generation}
--- The TS algorithm defines no set process for generating a neighbouring solution given a starting solution. Generating neighbours from a solution is a critical process in the TS algorithm, for it is the only means by which the algorithm considers other solutions, i.e. it is the mechanism by which the algorithm searches. Generating a new neighbour can be as simple as changing only one value from the current solution or it can be very complex and incorporate other algorithms together with mathematics formulae. Regardless of the complexity of the neighbour generation that is used, care must be taken to ensure that the algorithm is able to produce a wide diversity of neighbours and is also able to intensify on the most optimal solution.
\paragraph{Tabu lifetime}
--- The TS algorithm only operates on a single solution at a time and at most only considers one potential neighbour as its next possible current solution. Therefore a difficult choice needs to be made as to how long a solution stays tabu. In the FAP a solution might be entered into the tabu list early in the search process of the algorithm. A large majority of the neighbours of this solution are vastly superior solutions compared with any of the current solutions produced by the algorithm. Due to the solution with these neighbours being in the tabu list, these neighbours will not be reconsidered until much later when the solution is removed from the list. A possible option to allow the TS to reconsider the tabu solutions is to increase the aspiration criterion. Increasing the aspiration criterion does have its risks. A high aspiration criterion and the algorithm might be too eager to select just any solution even though it is Tabu. A low aspiration criterion and the algorithm will be too strict in selecting a tabu solution.

In the next section the simulated annealing algorithm is discussed.
\section{Simulated Annealing}
\label{sec:simulatedannealing}

\subsection{Introduction}
\label{sec:SAIntroduction}
Simulated annealing (SA) is a metaheuristic search technique proposed in the 1980s by Kirkpatrick to solve combinatorial optimisation problems. The technique is based on a natural process which is known in metallurgy as annealing \cite{CurveFittingSA,SASingleMultiObj,TempCyclingSA,ChaosSA}. Kirkpatrick was the first to use SA to solve optimisations problems but the basic algorithm structure was defined by Metropolis et al. in 1953 \cite{CurveFittingSA,VeryFastSAImageEnchancement}.

Annealing is the natural process of crystallisation when a solid is heated to a high temperature and then systematically cooled to a lower temperature to reach a crystallised form \cite{CurveFittingSA,NewSAs,MobileRobotSA,ConstantTempSA}. The crystallised form of the solid is known to be the global minimum of the solid's internal energy state. 

When the solid is rapidly cooled from a high temperature, the molecules have no time to reach a thermodynamic equilibrium stage \cite{MobileRobotSA,ConstantTempSA}. Therefore the molecules of the solid have high energy and the resultant structure has no real crystalline form; thus the solid energy is at a local minimum\cite{CurveFittingSA,NewSAs,MobileRobotSA}. When the solid is slowly cooled in a controlled manner, the molecules are able to reach a thermal equilibrium at each temperature \cite{ChaosSA,CurveFittingSA,NewSAs}.

In the algorithm the energy state is the \emph{cost function} that needs to be minimised, and the molecules are the \emph{variables}, which represent the solutions, and thus their state needs to be optimised to reach the desired energy state.

The SA algorithm is able to purposely move to a worse solution in the search space\cite{EcoEquilSA}. In the research when an algorithm moves to a worse solution it is classified as an \emph{uphill} move\cite{AIModernApproach}. Similarly, when a move is made by the algorithm which results in a better solution the move is classified as an \emph{downhill} move\cite{AIModernApproach}.

The following equation is the standard probability function that is used to determine when an uphill move is performed by the algorithm. This function is known in the literature as the \emph{metropolis criterion}. 
\begin{equation}
\label{eq:saprobability}
	M_{AC} =
	\begin{cases}
	1, &\text{if $f(y) \leq f(x)$}\\
	e^{-\frac{\Delta E}{T_k}} , &\text{otherwise}\\
	\end{cases}
\end{equation}
The function $f$ is the objective function or a function that determines the state of a given position in solution space\cite{EcoEquilSA}. The parameter $T_k$ is the temperature of the algorithm at iteration $k$ \cite{EcoEquilSA}. Finally, $\Delta E$ is the change in ``energy'' between two solutions $x$ and $y$ \cite{EcoEquilSA}.

The main purpose of the SA algorithm (like most optimisation algorithms) is to minimise or maximise the cost function \cite{SASingleMultiObj}. This cost function typically evaluates a solution desirability compared with other solutions in the immediate \emph{neighbourhood} of the algorithm's current position \cite{TheoPraticalSA}. 

The immediate neighbourhood of solutions is generated based on some heuristic implemented by the algorithm designer\cite{AIModernApproach}. This heuristic, as with the TS algorithm, can be simple or complex. The neighbourhood generated is the first step right after the starting solution has been generated by the algorithm.

Typically a neighbouring solution is only selected as the new best state if its desirability ranks higher than the current solution. When the algorithm moves to a better solution from the previous solution, the move is typically referred in the literature as a \emph{downhill} move \cite{CurveFittingSA}.

The best state is not always selected; in some cases the algorithm is also able to move to solutions that are worse than the current solution. A worse solution is only selected based on some probability which is controlled by the \emph{annealing temperature} of the algorithm \cite{TheoPraticalSA}. 

At a high annealing temperature the probability that the algorithm will select a bad solution is very good. As the annealing temperature decreases so does the probability that a bad solution will be selected \cite{CurveFittingSA}. When the algorithm moves to a worse solution, the move is typically referred to in the literature as an \emph{uphill} move \cite{CurveFittingSA}. Uphill moves allow the algorithm to break out of local minima and can lead the algorithm down a different path, which may ultimately result in obtaining the global optimum \cite{SASingleMultiObj}. 

As with the TS algorithm, the standard SA algorithm does not define a set neighbourhood generation mechanism; instead it is up to the algorithm designer to implement a suitable generation mechanism that will allow the algorithm to adequately explore the search space\cite{VariousCoolingSA}. 

The following section presents a discussion on various characteristics of the SA algortihm.
\subsection{Important Simulated Annealing Characteristics}
There are four characteristics of the SA algorithm that make the algorithm unique. One of the most important is the cooling schedule. 

%\subsubsection{Markov Chain}
%The SA Algorithm is typically modelled by using Markov chains due to each Markov chain represents a set of trials that the algorithm has executed at the same temperature. 

%A Markov chain\footnote{Also known as Markov process} defines a chain of states or processes that satisfy the \emph{Markov assumption}. Which is defined as, that any state only depends on a set amount of previous states that have been encountered previously\cite{AIModernApproach}.
%%
%It has been proven with the use of Markov Chain theory that SA will find the global minimum in the solution space \cite{ClusterSA}. This proof is only valid when the following properties for the underlying Markov chain hold \cite{VeryFastSAImageEnchancement}:
%\begin{itemize}
%\item It must be irreducible
%\item It mustn't be periodic
%\item The detailed balance condition must hold
%\end{itemize}
%According to the proof presented in \emph{Image enhancement using very fast simulated reannealing}\cite{ClusterSA}, if the SA algorithm designer can uphold the above properties, the algorithm is able to find the global optimum. Even though the algorithm will eventually find the global optimum, the algorithm is known to take a very long time to do so.
\subsubsection{Cooling Schedule}
The cooling schedule/annealing Schedule is the most defining characteristic of the SA algorithm. It is the procedure where the natural annealing process is mimicked. The temperature of the SA algorithm is a control parameter that defines how much the algorithm moves around in the solution space.

After each iteration, whether the algorithm has selected a new best solution or not, the temperature is reduced by a certain amount. This amount is determined by the cooling schedule.

In general, when the SA algorithm temperature has a very high value most solutions that are produced from the neighbourhood are accepted \cite{ClusterSA}. Thus the algorithm moves freely in the solution space with little constraint. As the temperature decreases, the probability that the algorithm will select a bad or just any solution decreases. When the temperature is very low, the SA algorithm is similar to a greedy algorithm in the sense that it only accepts downhill movements\cite{ClusterSA}.

The cooling schedule provides the SA algorithm with the critical ability to control the rate the algorithm transitions from the diversification phase (high temperature) to the intensification phase (low temperature).

In the literature there are three annealing schedules in common use, namely \emph{the logarithmic schedule}, the \emph{geometric schedule} and the \emph{Cauchy schedule}\cite{VeryFastSAImageEnchancement,SASingleMultiObj}. 

The standard and most commonly used schedule is known as the logarithmic schedule and is based on Boltzmann annealing \cite{VeryFastSAImageEnchancement}. The main disadvantage of this schedule is that is slow due to its logarithmic nature \cite{VeryFastSAImageEnchancement}. It also requires moves to be generated from a Gaussian distribution for it to be able to reach the global minimum\cite{SASingleMultiObj}. The logarithmic annealing function has the following form:
\begin{equation}
\label{eq:logcooling}
	T_k = \frac{T_0}{ln(k)},\text{where k is the iteration value and } k \neq 0\\
\end{equation}
Where $T_k$ is the temperature at iteration $k$.

The Cauchy schedule is faster than the logarithmic schedule. Similar to the logarithmic, this schedule also has a movement requirement. Moves must be generated from a Cauchy distribution for the algorithm to be able to reach the global minimum \cite{SASingleMultiObj,VeryFastSAImageEnchancement}. The Cauchy schedule is also typically referred to as fast annealing\cite{VeryFastSAImageEnchancement}. The schedule has the following form:
\begin{equation}
\label{eq:cauchycooling}
	T_k = \frac{T_0}{k} ~, k \neq 0
\end{equation}

Finally, the fastest annealing schedule is known as the geometric or exponential annealing schedule \cite{SASingleMultiObj}. The schedule has no move generation requirement to reach the global minimum, since there is no regorous proof in the literature \cite{SASingleMultiObj}. By using the geometric schedule the SA temperatures are rescaled which is called \emph{re-annealing}\cite{VeryFastSAImageEnchancement}.

The geometric schedule has the following form:
\begin{equation}
\label{eq:geocooling}
	T(k)=T_0e^{-C_k},\text{where C is a constant}
\end{equation}

\subsubsection{Initial Temperature}
The initial temperature is a very important parameter to define in the SA algorithm, since it defines a point from which the cooling schedule will start\cite{VariousCoolingSA}. Therefore, depending on what the initial value of the temperature, is the final result that the algorithm will produce can be influenced\cite{SALongestCommon,AutoConfigSA}.

When the initial temperature is set to a very high value, the algorithm takes a long time to reach a result since the search space is being explored more\cite{SALongestCommon,VariousCoolingSA}. More exploration is favourable for SA as it lets the algorithm be less susceptible to local minimum. 

If the initial temperature is set to a very low temperature, the algorithm might converge too quickly and thus produce a result which may be the local minimum\cite{SALongestCommon,VariousCoolingSA,AutoConfigSA}.

The initial temperature together with the cooling factor allows the algorithm designer to define the time window for the algorithm to escape local minima, as well as the rate of convergence to an optimum solution\cite{SALongestCommon,VariousCoolingSA}.

A low initial temperature together with a low cooling factor makes the time window for the algorithm to leave a local optimum very small\cite{SALongestCommon}. With a high initial temperature and cooling factor value that is almost 1, the time window for the algorithm to leave the local optimum is much larger \cite{SALongestCommon}. 

When the algorithm is near a global optimum, a low initial temperature and low cooling factor will allow the algorithm to reach the optimum faster in the solution space\cite{SALongestCommon}. In contrast, if a high temperature and a very low cooling factor are used, the algorithm will take longer to reach the optimum even though it is near the global optimum\cite{SALongestCommon}.

\subsubsection{Move Generation}
Most of the research done on the SA algorithm focuses on the annealing schedule and not so much on the move/solution/neighbourhood generation. Typically an initial solution is generated and then small changes are made to the solution to represent a new solution. The solution is said to be perturbed to the next solution.

Move generation is the phase where neighbouring solutions to the current solution are generated. It is the ideal section for an algorithm designer to embed domain-specific knowledge which will allow the algorithm to generate better possible solutions.

In research done by Tseung and Lin \cite{CurveFittingSA} an initial solution is not modified, but a move generation technique known as \emph{pattern} search is used. Pattern search has two forms of movement, namely the exploratory move and the patten move. The exploratory move continually changes the certain variables of a solution \cite{CurveFittingSA}. This is done so that the technique can rapidly find and identify a ``downhill'' move. The pattern move uses the information gathered by the exploratory move to move towards the minimum of the function \cite{CurveFittingSA}.
\subsubsection{Algorithm Efficiency}
The algorithm is also efficient with regard to CPU cycles when compared with the genetic algorithm. SA only has to evaluate a certain number of moves each iteration, instead of a whole population each iteration. The genetic algorithm is discussed in section~\ref{sec:geneticalgorithm}.

Unlike TS, the basic SA algorithm does not keep any memory and is therefore memory efficient, but in contrast suffers the risk that the solution will cycle. The more iterations spent at a temperature, the longer the algorithm spends at a certain temperature and therefore the higher the probability that solutions may cycle.

In next section the flow of the SA algorithm is discussed and pseudo code for the SA algortihm is presented.

\subsection{Flow of the Algorithm}
In an attempt to better understand how the SA algorithm operates, a general discussion on the flow of the algorithm will now be given using algorithm~\ref{alg:SA} as a reference point.
\begin{algorithm}[H]
\caption{Basic Simulated Annealing Algorithm\cite{VeryFastSAImageEnchancement,ChaosSA}}
\label{alg:SA}
	\begin{algorithmic}[1]
		\State Initialize parameters
		\State Set starting temperature $T(0)$
		\State $x_0 \leftarrow$ Generate initial starting solution
		\While{Stopping criterion not met}
			\State $y_i \leftarrow$ Generate neighbouring solutions to $x_i$
			\State Evaluate $y_i$ neighbours with fitness function $f(y_i)$
			\State Calculate probability $p_i$ of $y_i$ neighbours with equation~\ref{eq:saprobability}
			\State $x_i \leftarrow$ Select $y_i$ neighbour based on probability $p_i$
			\State Reduce temperature $T(i)$ based on cooling schedule
		\EndWhile
		\State Return best solution $x_i$
	\end{algorithmic}
\end{algorithm}

From lines 1 -- 3, the SA algorithm is initialised. The most important step here is setting the starting temperature for the annealing process to start. As mentioned in the introduction, the temperature of the annealing process plays a critical role in the potential solution selection process.

After the algorithm has been initialised the search phase of the algorithm starts which ranges from lines 4 -- 10. Like the TS algorithm, the SA algorithm starts the search phase by generating a number of neighbours to the current solution held by the algorithm as can be observed in line 5.

Before selecting a neighbour the algorithm first needs to evaluate the generated neighbours. It evaluates each neighbour by applying a fitness function $f(y_i)$ in order to determine its fitness.
Once the fitness of all the generated neighbours has been determined, the algorithm uses equation~\ref{eq:saprobability} to calculate the probability of selecting a particular neighbour for all the generated neighbours as well. The probability calculation can be observed to occur in line 7. The algorithm then selects the neighbour with the highest probability to be the current solution, as observed in line 9. 

Before the algorithm advances to the next iteration the temperature needs to be lowered. As discussed, the temperature is lowered according to a particular cooling schedule. In the algorithm the process of lowering the temperature occurs in line 10.

In the section that follows a discussion is given on when SA is applied to the FAP.
\subsection{Simulated Annealing on the FAP}
The SA algorithm, as with the TS algorithm, has achieved good results in other optimisation problems, as mentioned in section \ref{sec:SAIntroduction}. Due to its success on other NP-Complete optimisation problems, the SA algorithm has also been applied to the FAP.

In literature by Carlo Mannino and Gianpaolo Oriolo\cite{SolvingSuperIntervalGraphs} the SA algorithm is applied to the FAP. The resulting SA algorithm was benchmarked on the COST 259 Siemens benchmark instances. The results obtained by the authors is presented in table~\ref{tab:SA}. Values represent the total interference generated by the frequency assignment. 
\begin{table}[H]
\centering
	\begin{tabular}{| c | c | c | c |}
	\hline
	Problem instance & SA & COST 259 (old) & COST 259 (new) \\ \hline
	Siemens1 & 22.96 & 23.00 & 2.200\\ \hline
	Siemens2 & 14.72 & 14.75 & 14.280\\ \hline
	Siemens3 & 52.43 & 52.55 & 5.19\\ \hline
	Siemens4 & 80.96 & 80.80 & 81.89\\ \hline
	\end{tabular}
\caption{SA on COST 259 Benchmark}
\label{tab:SA}
\end{table}
Note that at the time the above results where obtained the best results obtained for the siemens problem were different from the latest results.

An overview of the SA algorithm along with its unique characteristics have now been given. Utilising the knowledge that was gained from understanding how the SA algorithm operates a critical evaluation can be presented. 

In the following paragraphs, a theoretical critical evaluation is presented if the SA algorithm would be applied to the FAP.

\paragraph{Cooling Schedule}
--- Depending on the cooling schedule selected, the algorithm might converge too quickly. As discussed previously the cooling schedule reduces the temperature. The temperature plays a large part in the determination of whether a particular solution will be moved to or not in an iteration. Thus early on the algorithm will explore a lot more (diversification) and later on will exploit more (intensification). In the FAP, the algorithm must not only be able to explore and exploit, but also be able to return to an exploration phase if need be.
As the temperature becomes colder the SA algorithm exploits more and therefore will not easily move to a worse off solution. In the FAP, it might be desirable to rather move a worse off solution later on, as the particular current solution is a local minimum and yields bad neighbours as potential next solutions. With the cooling schedule this is simply not possible, unless the temperature and schedule are reset. Resetting the temperature and schedule is not ideal, since the algorithm keeps no history and might risk making the same faults as before the reset.
\paragraph{Neighbourhood generation}
--- The SA algorithm, as with the TS algorithm, has no set process that defines how neighbours should be generated. As discussed, neighbourhood generation is the primary means by which the SA algorithm moves about the search space in search of an optimal solution. Therefore applying the SA algorithm would require a custom neighbourhood generation scheme. A desireble neighbourhood generation scheme would be one that keeps track of where the algorithm has been previously. By keeping history, the algorithm will be able to avoid previously explored areas in the search space.
\paragraph{Single solution based search}
--- The SA algorithm is similar to the TS algorithm in the sense that it only searches from one solution per iteration. It searches by generating neighbours around the current solution of the algorithm. The FAP search space is huge; hence it would be more efficient to have multiple current solutions from which neighbours are generated. This enables the algorithm to explore the search space much more efficiently at the expense of more computational resources.

\section{Genetic Algorithm}
\label{sec:geneticalgorithm}
\subsection{Introduction}
The genetic algorithm (GA) is a stochastic search method that is based on the natural process of genetic evolution and the Darwinian concept of ``survival of the fittest'' \cite{DistributedHierarchicalGA,AcceleratingGA,AdaptiveSAGA,FamilyGA}. The GA was initially developed for adaptive systems by Holland, but has been widely used in the optimisation field of study due to its effective exploration of the solution space as well as its relative success in multidimensional problems\cite{ParallelGASA,DistributedHierarchicalGA,FamilyGA}. 

The GA is generic since it defines no specific crossover, selector and mutation operator. It defines that these operators exist, but exactly how they perform their respective operations is up to the algorithm designer.

The wide use of the GA can also be attributed to its generic algorithm structure as well as the ease of implementation \cite{FamilyGA,AdaptiveSAGA}. GA is \emph{problem dependent} and thus the operators used needs to be tailored to fit within the problem defined search space \cite{AcceleratingGA}.

The GA search procedure involves searching the solution space through artificial evolution and natural selection\cite{FamilyGA,MultiPopGA,HybridIntelliGA}. An individual or point in the solution space is known as a \emph{chromosome} \cite{HumanPassiveGA}. An initial set of chromosomes (referred to in the research as the \emph{population}) is randomly generated to form the starting population\cite{FamilyGA,HybridIntelliGA,AcceleratingGA,MultiPopGA}. 

A chromosome consists of a sequence of smaller parts called \emph{genes}\cite{CompuIntelligenceIntro}. The sequence upon which these genes appear in the chromosome determines the characteristics of an individual\cite{CompuIntelligenceIntro}. In the GA a single gene represents a single variable of a solution\cite{FamilyGA,AcceleratingGA}. A whole chromosome therefore represents a solution\cite{FamilyGA,AcceleratingGA}. 

According to the evolution theory proposed by Darwin individuals of a population with the best chromosome have the best chance to survive and to reproduce\cite{CompuIntelligenceIntro}. These individuals are reffered to as the fittest individuals of the population. In a GA population each individual is ``rated'' to determine how good the solution its chromosome represents is\cite{CompuIntelligenceIntro}. The rating of a chromosome is referred to as its fitness value\cite{CompuIntelligenceIntro}.

Determining the fitness value of a chromosome is achieved by means of a \emph{fitness function}. The fitness function is a mathematical function which maps the chromosome representation to a scalar value\cite{CompuIntelligenceIntro}. An optimisation problem has a objective function which calculates how good a solution is, therefore in the GA the fitness function represents the objective function\cite{CompuIntelligenceIntro}.

When the fitness of individuals in a population have been determined the GA probablisticly selects individuals for the next generation\cite{CompuIntelligenceIntro}. The selection probability is referred to as the \emph{selective pressure}\cite{CompuIntelligenceIntro}. The selected individuals enter the reproduction phase of the GA where offspring is created.

Offspring is created by combining parts of two or more chromosomes to form a new chromosome this procedure is called the \emph{crossover}\cite{CompuIntelligenceIntro}. The chromomsomes used in the production of the offspring is referred to as the parent chromosomes\cite{CompuIntelligenceIntro}.

With regard to how offspring and parents are handled, there are two forms of the GA \cite{FamilyGA}. One form is called the \emph{generational} GA  and the other form is known as the \emph{steady-state} GA \cite{GeostatisticalGA,FamilyGA}.

With the generational GA the offspring are not immediately used in the next generation; instead they are kept in a pool until the pool reaches a required size \cite{FamilyGA}. The offspring are then used to replace the parents entirely in the next generation \cite{FamilyGA}. In the steady-state GA the offspring are continually integrated with the population; thus offspring and parents occupy the same population pool every generation \cite{GeostatisticalGA,FamilyGA}.

The GA search process moves around in the search space using probabilistic rules rather than deterministic rules \cite{FamilyGA}. The probabilistic transition rules aid the algorithm to avoid local optima regions in the solution space \cite{HybridIntelliGA}. 

The GA makes no assumptions about the solution space and primarily works on the information provided by the objective function \cite{ConstrainedGA,HybridIntelliGA}. 

Unlike the SA and TS algorithms, the GA does not concentrate on one point when searching the solution space, but rather on a wide range of points represented by the population \cite{DistributedHierarchicalGA,FamilyGA,HybridIntelliGA}\label{GASearchPoints}. The population is artificially evolved each generation by using a set of operators\cite{SelfAdaptiveGA}. 

The set of operators consists of a selection operator, crossover operator and mutation operator\cite{SelfAdaptiveGA,MultiPopGA}. In the section that follows each operator will be discussed.

\subsection{Important Genetic Algorithm Characteristics}
The various operators used by the GA makes the procedure unique and is one of its defining characteristics. The first is the characteristic to be discussed is the selection operator.

\subsubsection{Selection Operator}
The selection operator is the first operator to be applied to the population after each generation. The operator determines which individuals will survive from the current population for the next generation based on their fitness values \cite{CoactiveFuzzyGA,CombinedBranchBoundGA,ConstrainedGA}.

The individuals who survive and who thus have the highest fitness are moved to a ``mating pool'' \cite{HumanPassiveGA}. Individuals from this mating pool are used by the crossover and mutation operators (in the reproduction phase), to generate offspring that is used as the new population \cite{AdaptiveSAGA,AcceleratingGA}.

By favouring high fitness individuals above low fitness individuals the operator guides the search towards better high quality solutions \cite{ConstrainedGA}. Care must be taken if the operator is too eager regarding high quality individuals since this may eliminate diversity in the population and thus result in premature convergence\cite{ConstrainedGA}. A selection operator that is too eager is said to have high selective pressure.

If the search space is known to have only one optimum, then a high selection pressure is feasible\cite{ConstrainedGA}. The selection operator directs the search into a gradient-based direction that converges on a single optimum solution \cite{ConstrainedGA}. 

In contrast, with a search space that is known to have multiple optima a low selection pressure is more benificial to the GA\cite{ConstrainedGA}. Low selection pressure allows the population of the GA to explore the search space more\cite{ConstrainedGA}.

The most widely adopted selection scheme is known as the \emph{roulette wheel} selection scheme \cite{ConstrainedGA,GeostatisticalGA,HybridBaldwinGA,CoactiveFuzzyGA}. With this scheme an individual is selected based on a probability defined by the fitness of the individual divided by the collective fitness of the population \cite{GeostatisticalGA}.

Depending on how complicated the objective function is and how large the population is, the selection phase may be the most computationally expensive as well as time consuming \cite{AcceleratingGA}.

The interested reader is directed to the survey by Engelbrecht\cite{CompuIntelligenceIntro} for a more wide-ranging discussion on selection operators.

\subsubsection{Crossover Operator}
\label{sec:crossover}
The crossover operator is the first operator applied to the population in the reproduction phase. The crossover operates exclusively on the chromosomes in the mating pool. This operator is the main process by which by the GA is able to diversify as well as exploit certain optimal regions \cite{CombinedBranchBoundGA,CoactiveFuzzyGA}. 

Crossover works by interchanging genes from one or more parent chromosomes. The parent chromosomes are selected with a probability from the mating pool to produce a single chromosome known as the offspring \cite{FamilyGA,HumanPassiveGA,CoactiveFuzzyGA}. The probability is known as the \emph{crossover probability} and is problem dependent\cite{CompuIntelligenceIntro}.

By defining a high crossver probability, good genes form the current population are retained in the next population\cite{CompuIntelligenceIntro}. With good genes being retained more historical information is passed onto the next generations population\cite{FamilyGA}.

There are a variety of ways in which genes are interchanged between chromosomes in the crossover operation i.e. fixed point crossover, two point crossover, uniform crossover and gaussian crossover\cite{CompuIntelligenceIntro}. Fixed point crossover operates on binary parents whereby a point is selected in one parent and then all other bits are replaced by the other parents' bits \cite{HumanPassiveGA}. 

Two point crossover generates two random indices which dictate a certain segment in the one parent to be interchanged with the other parent \cite{ConstrainedGA}. Where a segment consists of a subset of genes from the chromosome. 

With a uniform crossover a chromosome is represented by a byte\cite{CompuIntelligenceIntro}. Each byte is made up of sequence of bits. As discussed each chromosome is made out of genes and therefore in the uniform crossover each bit is a gene\cite{CompuIntelligenceIntro}. The uniform crossover is the most basic of all crossovers since it randomly selects bits from one parent to be replaced by another parent's bits and is usually used when a large solution space must be search\cite{ParallelGASA,GeostatisticalGA}. 

Finally, the Gaussian crossover also uses the byte representation. The Gaussian crossover interchanges bits between parents based on a Gaussian distribution \cite{ParallelGASA,GeostatisticalGA}. Depending on the state of the algorithm, crossover operators can also be interchanged or even paired if the algorithm needs better search performance for large or small solution spaces \cite{HetergeneousGA,ParallelGASA}.

Caution should be exercised by the crossover operator when selecting chromosomes for reproduction. It is possible for the operator to select the same chromosome twice to be the parent of a single offspring\cite{CompuIntelligenceIntro}. The operator is also at risk of selecting the same chromosome multiple times for reproduction\cite{CompuIntelligenceIntro}. The opertator must therefore incorporate a test to detect unenecessary repeated usage of a chromosome\cite{CompuIntelligenceIntro}.

\subsubsection{Mutation Operator}
The mutation operator is a probabilistic operator and thus it is applied to an existing individual with a probability referred to as the mutation rate\cite{CompuIntelligenceIntro}. 

The purpose of the mutation operator is to increase the diversity of the genes of an individual chromosomes\cite{CompuIntelligenceIntro}. By introducing new genes into an individual the diversity of the populations characteristics are increased\cite{CoactiveFuzzyGA,AcceleratingGA,ConstrainedGA}.

The mutation operator has no previous information on the chromosome it is mutating; thus it is entirely possible that the mutation may modify the chromosome for the worse \cite{AcceleratingGA}. A worse solution might lead the algorithm out of local optima or lead it down a new path to find the global optima, but this is not always the case \cite{AdaptiveSAGA,FamilyGA,ConstrainedGA}. It is for this reason that it is recommend that the mutation rate be set to a low value to ensure good solutions are not distored too much\cite{CompuIntelligenceIntro}.

In a survey done by Engelbrecht\cite{CompuIntelligenceIntro} another mutation operator is discussed. Instead of mutating a small part of randomly selected chromosomes, this operator generates new offspring to be inserted back into the population. The operator randomly generates a new chromosome and then uses any of the previously discussed crossover operators (see page~\pageref{sec:crossover}).

The mutation operator is not always a basic random replacement of genes operation. In research done by Il-kwon Jeong and Ju-jang Lee\cite{AdaptiveSAGA}, a mutation operator is presented that uses the SA algorithm to determine the genes that need to be replaced. The SA mutation operator generates a new chromosome from which genes are used to replace in the chromosome being mutated \cite{AdaptiveSAGA}.

Addition mutation operators are discussed in Engelbrecht\cite{CompuIntelligenceIntro}
\subsubsection{Initial Population Generation}
Initial population generation is the very first activity that the GA performs. Out of this population potential mating candidates are selected based on their fitness, which indicates desirability. Generally the initial population is generated by means of randomisation \cite{SelfAdaptiveGA}. Since the algorithm searches multiple points simultaneously in the solution space, it is desirable that the initial population have a wide diversity with regard to the solution it represents \cite{CombinedBranchBoundGA,DistributedHierarchicalGA}. By controlling the initial population generation we can control, to a small degree, the amount of exploration the algorithm does initially as well as avoid premature convergence \cite{CombinedBranchBoundGA}. Therefore care must be taken in the selection of the particular randomisation scheme that will be used to generate solutions.

In a survey done by Andrea Reese\cite{RandomNumberGA}, two randomisation schemes which defined, namely pseudo-random number generators (PRNGs) and quasi-random number generators (QRNGs). PRNGs were found to be heavily problem dependent, improving the search efficiency in some instances and in other instances having no considerable impact. QRNGs, on the other hand, were shown to significantly improve the final solution produced by the GA as well as lower the number of generations for the solution to be obtained \cite{RandomNumberGA}.

\subsubsection{Algorithm Efficiency}
The GA is a powerful, yet simple algorithm and tends to find good solutions given enough time, it does have its disadvantages. One of the major disadvantages occurs when the GA is applied to problems that have very large solution spaces. In these problems, the population size is a very sensitive parameter\cite{AdaptiveSAGA,HetergeneousGA,SelfAdaptiveDataMiningGA,PatternDetectionGA}. If the population is too small the algorithm will not have enough diversity to search and will tend to converge prematurely. 

A large population is preferred in large problem spaces, but then the algorithm is very computationally expensive since more time is spent evaluating than evolving new populations and the speed of the algorithm convergence decreases drastically. Hence, the population size must be fine-tuned to achieve optimal performance in large problem spaces \cite{AdaptiveSAGA,CompuIntelligenceIntro}

In conclusion, the algorithm has some sort of hill-climbing ability through the mutation operator, but the probability of the mutation operator is far too low for the algorithm to be considered to have a real hill-climbing capability.

In the next section the pseudo code for the GA algorithm is presented and the flow of the GA algorithm is discussed
\subsection{Flow of the Algorithm}
The core concepts of the genetic algorithm were introduced in the previous section. To better understand the algorithm, a general overview of the algorithm be presented in this section using algorithm~\ref{alg:GA} as a reference point.
\begin{algorithm}[H]
\caption{Basic Genetic Algorithm Algorithm\cite{FamilyGA,AdaptiveSAGA,DistributedHierarchicalGA,SelfAdaptiveGA}}
\label{alg:GA}
	\begin{algorithmic}[1]
		\State $pop_n\leftarrow$Initialize population
		\State Evaluate population with fitness function $f(i)$
		\While{Stopping criteria is not met}
		\State $y_k \leftarrow$ Select best performing genes from population
		\State Remove other genes from population
		\Repeat
			\For{Each gene $g_i$ in $y_{k-1}$}
				\State $z_i \leftarrow$ Perform crossover with $g_i$ and $g_{i+1}$
				\State $m_i\leftarrow$ Calculate Mutation probability for $z_i$
				\If{$m_i \geq$ Mutation threshold}
					\State Perform mutation on gene $z_i$
				\EndIf
				\State Add gene $z_i$ to $new_pop$
			\EndFor
		\Until{$size(new_pop) = size(pop_n)$}
		\State $pop_n \leftarrow new_pop$
		\EndWhile
		\State $x_i \leftarrow$ Determine best gene in $pop_n$
		\State Return best solution $x_i$
	\end{algorithmic}
\end{algorithm}
The GA algorithm is a population-based algorithm and therefore needs to initialise its population. Each individual of the population represents a potential solution. Population initialisation occurs in line 1 in algorithm~\ref{alg:GA}. 

The amount of individuals in a population to be generated is predefined and is known as the population size. In algorithm~\ref{alg:GA} the population size is represented by the subscript $n$ in $pop_n$, where $n > 0$.

Before the algorithm can start \emph{evolving} its population, it first needs to determine each individual in the population's fitness. The fitness of an individual is calculated using a fitness function $f(i)$.

Since the initial population has been evaluated, the algorithm is now able to start its searching process, which starts at line 3 and ends at line 17 of algorithm~\ref{alg:GA}.

The first part of the search process that is performed by the algorithm is to select a certain subset of the population. This subset selection can be observed to occur in line 4.

All the individuals that are not in the subset are removed from the population. Once the subset selection has been completed, the algorithm is ready to enter the reproduction phase, which ranges from lines 6 -- 15 of algorithm~\ref{alg:GA}.

The first step in the reproduction phase is where various individuals of the subset are mated together to produce a new offspring individual. Mating occurs through the use of the crossover operator. Generation of new individuals with the crossover operator can be observed from lines 7 -- 8 of algorithm~\ref{alg:GA}.

The second step of the reproduction phase is where mutation occurs. For each of the offspring generated by the crossover a mutation probability is calculated. If the calculated probability for a particular offspring is high enough, the algorithm enters the mutation phase. In the mutation phasean individual is selected and the mutation operator is applied. Application of the mutation operator can be seen to occur in lines 9 -- 10 of algorithm~\ref{alg:GA}.

Regardless of whether the offspring has been mutated or not, the resulting offspring is added to the new population. The reproduction phase continually loops, until the new population equals the size of the initial starting population. Once the amount of individuals in the new population has reached the population size $n$, the algorithm moves on to its next iteration.

The algorithm continually generates a new population for each generation until a predetermined stopping criterion has been met. Once the criterion has been met, the algorithm selects the individual with the best fitness in the current population as its most optimal solution.

The following section discusses literature where the GA has been applied to the FAP.

\subsection{Genetic Algorithm on the FAP}
Continuing the trend of the SA and TS algorithms, the GA has also been applied to a wide variety of optimisation problems and has on average obtained good results as can be observed in the following research \cite{GANonConvex,SelfAdaptiveDataMiningGA,AutoComplexMeta}. Due to the success of the GA on other difficult problems, it has therefore also been applied to the FAP.

Colombo and Allen\cite{ProblemDecompMIFAP} have developed a GA to be applied on the FAP. The authors decomposed the FAP into smaller subproblems. On average the solution quality is improved by using the technique but at the expense of more complex and taxing evaluations that have to be performed\cite{ProblemDecompMIFAP}. 

In table~\ref{tab:GA} the results obtained by authors on the COST259 benchmarks are comapred. The listed values are scalars representing the total interference generated by the frequency plan.
\begin{table}[H]
\centering
	\begin{tabular}{| c | c | c |}
	\hline
	Problem instance & GA & COST 259 \\ \hline
	Siemens 1 & 2.60 & 2.20 \\ \hline
	Siemens 2 & 16.34 & 14.280 \\ \hline
	Siemens 3 & 6.37 & 5.19 \\ \hline
	Siemens 4 & 84.08 & 81.89 \\ \hline
	\end{tabular}
\caption{GA on COST 259 Benchmark}
\label{tab:GA}
\end{table}

As per the results in table~\ref{tab:GA} the GA produces good results coming close to the best obtained results in the benchmark. By critically reasoning about the GA if it were applied to the FAP in theory, the following disadvantages can be identified:

\paragraph{Diversity}
--- The GA continually operates on a set population that was randomly initialised at the beginning of the algorithm. It therefore only has this set of generated genes in the initialised population to therefore evolve successive populations.
If one disregards mutation, the GA is a process by which the optimal combinations of the starting genes are found. Thus, the GA purifies the starting population genes in an attempt to find those individual genes, which if combined into a single individual, will produce an optimal individual, i.e. solution. Therefore the GA is very reliant on the quality of the random generator used. The probability of the algorithm finding a particular desirable gene that is exhibited by the starting population is directly related to the random number generator used to initialise the population. 
\paragraph{Crossover}
--- The crossover operation in the GA is the only means by which successive populations are generated and can therefore be regarded as the primary means by which the algorithm performs its search. As the crossover is defined in the standard algorithm, certain parts of both parents are copied and combined to form a new individual. With regard to the FAP, if each individual represents a frequency plan, the crossover operation would copy certain cells from the two parent plans. This is not desirable, since a single channel within a cell can generate major interference which overshadows the rest of the channels that generate low interference in the cell. Thus, for the GA to generate high quality solutions on the FAP, the algorithm would be better off utilising a crossover operation which works on individual channels assigned rather than cells. Crossover operation is also a memory and computationally expensive operation since individuals need to be constantly created and values need to be copied to these new individuals from the respective parents.
\paragraph{Mutation}
--- Mutation is a means by which more diversity is introduced into the chromosomes of the invididuals.  As discussed, the mutation operator introduces new genes to existing chromosome which can lead to an excellent solution being distorted and becoming one of the worst solutions. With regard to the FAP, the low probability of mutation is not desirable, as the FAP search space is huge and therefore requires constant diversity to be introduced to accurately explore it. A possible good mutation would be one that is slightly more intelligent than the standard mutation operator, which just randomly modifies a selected individual. An intelligent mutation would be one that takes into account the recent history of the individual as well as the history of the population and, based on the collective knowledge alters or \emph{mutates} a particular individual. Another means of applying the mutation operator is to set the mutation rate proprtional to an individuals fitness. By modifying the mutation rate in this manner, mutating good individuals can be avoided.
\section {Summary}
In this chapter a description was given of metaheuristic algorithms. What it means for an algorithm to be classified as being of a metaheuristic nature was explained and the characteristics these algorithms must exhibit were identified.

Three metaheuristic algorithms were discussed in this chapter. For every algorithm discussed an explanation was given of how the algorithm works as well as the various characteristics that make the algorithm unique.

For each algorithm, a brief overview of studies using the particular algorithm was given as well as some of the disadvantages or challenges that would be faced when applying the particular algorithm to the FAP.

The first algorithm discussed was the tabu search algorithm and the second was the simulated annealing algorithm. The chapter concluded with the genetic algorithm. In the next chapter the class of algorithm that is relatively new to optimisation research, namely the swarm intelligence class of algorithms, will be discussed.

In the table~\ref{tab:summary1}, the algorithms discussed in this chapter and their performance on the COST 259 set of benchmarks have been summarised.
\begin{table}[H]
\label{tbl:summaryMetaTable}
\begin{center}
	\begin{tabular}{| c | c | c | c | c |}
	\hline
	Problem instance & TS & SA & GA & COST 259 \\ \hline
	Siemens1 & 2.7692 & 23.00 & 2.60 & 2.20 \\ \hline
	Siemens2 & 14.9360 & 14.75 & 16.34 & 14.280 \\ \hline
	Siemens3 & 6.6496 & 52.55 & 6.37 & 5.19 \\ \hline
	Siemens4 & 110.9725 & 80.80 & 84.08 & 81.89 \\ \hline
	\end{tabular}
\caption{Summary of algorithm performance on the COST 259 benchmark}
\label{tab:summary1}
\end{center}
\end{table}
