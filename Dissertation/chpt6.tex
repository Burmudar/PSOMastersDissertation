\chapter{PSO on benchmark functions}
\label{chpt:benchmark}
\section{Introduction}
In this section a discussion will be presented on a series of optimisation benchmark functions. First a formulation of all the benchmark functions will be given. In the second section all the benchmark functions will be plotted on a 3d graph. Finally this chapter will conclude with a presentation of the results obtained by the PSO algorithm.

The functions vary from being relatively easy to optimize, to functions that contain lots of local minima and a slightly concealed global minima. In total fourteen benchmark functions will be formulated and benchmarked againts.

If one observed the following research \cite{devparallelgasa,CompuIntelligenceIntro,FundamentalSwarm}. Various optimisation algorithms are benchmarked such as Genetic Algorithm, Artificial Bee Colony algorithm, Tabu Search and Simulated Annealing. In the presented research the algorithms are benchmarked using numerical optimization functions.

Numerical optimization functions are good candidates to test optimisation algorithms as with a few slight changes the function operates in more dimensions or can have more or less optima\cite{devparallelgasa,CompuIntelligenceIntro,FundamentalSwarm}. Being able to alter these functions is a desirable trait as it enables one to accurately benchmark an algorithm, not only with regard to how the algorithm coupes with increased dimensionality but also in the algorithms accuracy in locating optima\cite{devparallelgasa,CompuIntelligenceIntro,FundamentalSwarm}.

The reason why these functions have variable amount of local and global optima, it to test various factors on how good the algorithm is that is being applied on the function. The factors that are tested are\cite{CompuIntelligenceIntro,FundamentalSwarm}:
\begin{itemize}
\item Rate of convergence
\item Exploration
\item Exploitation
\item Diversity
\item Breaking out of local minima
\item Information sharing
\end{itemize}

As discussed previously the numerical functions have predetermined optima, which means researchers are able to produce statistical information on how the algorithm performs. For instance, researchers will not be able to measure accurately the performance of the algorithm on a NP-Complete problem\cite{CompuIntelligenceIntro,FundamentalSwarm}. Yes, they can compare results with what other algorithms have produced, but cannot with absolute certain say or measure the algorithm convergence, diversity etc. on NP-Complete problems as their problem spaces are huge\cite{evalevoalgo}. 

With these benchmark functions, the optima have been mathematically calculated and their position is known within the problem space\cite{evalevoalgo}. Thus, researchers can now with surety measure the convergence rate, diversity and compare it with other algorithms, since the domain the algorithms operate it is not as specific as a NP-Complete problem\cite{evalevoalgo}. Rather, the domain is mathematical and deterministic and therefore allows easy comparison\cite{evalevoalgo}.

In this dissertation, two PSO algorithm were developed specifically to measure the performance of the PSO algorithm and also to better understand the various underlying dynamics of the algorithm.

The first PSO that was developed was just the standard PSO algorithm with no constriction coefficient or inertia weight. The second PSO algorithm differed to the standard algorithm in the sense that it utilizes the notion of inertia to move particles. 

Both of these algorithms have been applied to all fourteen benchmarks that are presented in the chapter and will be compared (where applicable) to other optimisation algorithms that have also been applied to the same benchmark problems. The comparison between these algorithms will be presented in section~\ref{sec:benchResults}.

In the next section, all the benchmarks that will be used for testing the PSO algorithms will be formulated. For the interested reader, 3D graphs along with the python code that generated the graphs are presented in the appendix. This chapter will conclude with a section on the results of the PSO algorithms that were developed being applied to the presented benchmarked as well as compared with other results that have been obtained by other algorithms and presented in the literature.
\section{Test Functions}
In this section all the test functions on which the two developed PSO algorithms will be benchmarked against will be presented. For each test function that is presented a mathematical formulation will be given and the global optimum will be explicitly stated. In addition to the formulation and global optimum, each function will also be classified whether it is a unimodal or modal function as well as whether it is separable or non-separable. Each of these classifications will now be explained.

\begin{description}
\item{\textbf{Unimodal}} --- A particular problem is classified as being unimodal when there is only clear solution. With only one clear solution, it means there is only one global optimum point in the solution space\cite{evalevoalgo,numericalABC,FundamentalSwarm,CompuIntelligenceIntro}.
\item{\textbf{Multimodal}} --- A problem is multimodal when it has more than one defined solution. Thus, the particular problem space contains multiple global optima\cite{evalevoalgo,numericalABC,FundamentalSwarm,CompuIntelligenceIntro}.
\item{\textbf{Separable}} --- Functions that are classified as being separable have the characteristic when the function can be written as a series of summations of just one variable\cite{numericalABC}. This quality makes the function easier to solve as the algorithm has only one variable to be concerned about\cite{evalevoalgo,numericalABC}. Separable functions also have the inherent quality of being scalable, meaning they can be easily be adapted to higher dimensions\cite{evalevoalgo,numericalABC}.
\item{\textbf{Non-separable}} --- Functions classifiied as non-separable cannot be rewritten into a series of summation functions as the variables used in the functions have the characteristic of being interrelated\cite{evalevoalgo,numericalABC}. The interrelation of the variables makes non-separable functions more difficult than separable functions to solve since the algorithm has more interdependent variables to be concerned with\cite{evalevoalgo,numericalABC}.
\end{description}

The De Jong test functions (F1, Shekel's Foxhole) are not considered to be the gold standard of testing optimisation algorithms\cite{evalevoalgo}. The only reason for their extensive use in the literature is due to the functions being the first to be developed and applied to test an optimisation algorithm i.e. Genetic Algorithm\cite{devparallelgasa,evalevoalgo}.

Since the inception of the De Jong test functions, additional functions have been developed which make it more difficult for an optimisation algorithm to locate the optimum\cite{evalevoalgo}. These functions are more difficult in the sense, as they have multiple local optima, which does in actual fact leads the algorithm astray which is to say the problem space is deceptive\cite{CompuIntelligenceIntro,FundamentalSwarm,evalevoalgo}. 

Problems that have deceptive search spaces tests how good the algorithm is resistant to Hill-climbing\footnote{continously selecting what seems to be better moves, but in reality its moving towards a local optima peak} and hence how efficient the algorithm is in exploring the entire search space\cite{evalevoalgo}.
%\textbf{Development of a parallel optimization method based on genetic simulated annealing algorithm}\\
%\textbf{Adaptive Diversity in PSO}\\
%\textbf{A hybrid intelligent genetic algorithm}\\
%\textbf{A Diversity-Guided Particle Swarm Optimizer â€“ the ARPSO}\\
%\textbf{A distributed hierarchical genetic algorithm for efficient optimization and pattern matching}\\
%\textbf{Tabu search for global optimization of continuous functions with application to phase equilibrium calculations}\\
%\textbf{Tabu Search applied to global optimization}\\
%\textbf{On the performance of artificial bee colony (ABC) algorithm}\\
%\textbf{Improving solution characteristics of particle swarm optimization using digital pheromones}\\
%\textbf{Continuous ant colony system and tabu search algorithms hybridized for global minimization of continuous multi-minima functions}\\
%\textbf{Chaotic bee colony algorithms for global numerical optimization}\\
%\textbf{A powerful and efficient algorithm for numerical function optimization: artificial bee colony (ABC) algorithm}\\
%\textbf{A New Quantum Behaved Particle Swarm Optimization}\\
%\textbf{A comparative study of Artificial Bee Colony algorithm}\\
\subsection{DeJong F1 Function}
\begin{equation}
\label{eq:DeJongF1}
	f(x) = \sum_{i=1}^n x^2_i, -5.12 \leq x_i \geq 5.12, i \in \mathbb{N}
\end{equation}
The DeJong F1 Function has the following global minimum when $f(x) = 0, x(i) = 0, i:n$ where $n$ is the amount of dimensions\cite{numericalABC,ABCCompareStudy,ARPSO,PerfABC,ContinACSTS,TestFunctions}. In the literature the function is classified as being unimodal and separable\cite{ABCCompareStudy,TestFunctions}. 
\subsection{Shekel's Foxhole}
\begin{align}
\label{eq:shekel}
	f(x_1,x_2) &= \{0.002 + \sum^{25}_{j=1} [j + (x_1 - a_{1j})^6 + (x_2 - a_{2j})^6]^{-1}\}^{-1}\\
\intertext{where}
	a &= \begin{pmatrix} \nonumber
			-32 & -16 & 0 & 16 & 32 & -32 & ... & 0 & 16 & 32 \\
			-32 & -32 & -32 & -32 & -32 & -16 & ... & 32 & 32 & 32 \\
		 \end{pmatrix}
\end{align}
the variables $x_1$ and $x_2$ are usually restricted to the square represented by $-65.356 \leq x_1 \leq 65.357, -65.357 \leq x_2 \leq 65.356$\cite{ABCCompareStudy,TSGlobalOptimization,ContinACSTS,TestFunctions}. The global optimum is when $f(x_1,x_2) = 0, \{x_1,x_2\} = \{-32,-32\}$\cite{ABCCompareStudy,TSGlobalOptimization,ContinACSTS,TestFunctions}.

The matrix controls the holes that appear in the search space. The interested reader is directed to the 3D graph rendering of this function presented in the appendix on page~\pageref{fig:ShekelGraph} for a visual representation. In the literature the function is classified being multimodal and separable\cite{adaptPSO,ABCCompareStudy,TestFunctions}.
\subsection{Rastrigin}
\begin{equation}
	f(x) = 10n + \sum_{i=1}^n [x_i^2 - 10\cos(2 \pi x_i)],\, i \in \mathbb{N}
\end{equation}
The values of the variable $x_i$ is bounded by the hypercube $-5.12 \leq x_i \leq 5.12$\cite{adaptPSO,ABCCompareStudy,numericalABC,ARPSO,PerfABC,HybridIntelliGA,TestFunctions}. The global optimum for the function is when $f(x_i) = 0,\, x_i = 0, \, i = 1,\dots,n$\cite{adaptPSO,ABCCompareStudy,numericalABC,HybridIntelliGA,TestFunctions}.

Rastrigins function is based on DeJong's first function equation~\ref{eq:DeJongF1} adding a cosine term, which in turns alters the problem space by introducing many local minima\cite{numericalABC,PerfABC,HybridIntelliGA,TestFunctions}. In the literature the function is classified being multimodal and separable\cite{adaptPSO,ABCCompareStudy,numericalABC,ARPSO,ChaoticABC,PerfABC,HybridIntelliGA,TestFunctions}.
\subsection{Schwefel}
\begin{equation}
	f(x) = 418.9829n - \sum^n_{i=1} [x_i\sin{\sqrt{|x_i|}}], \qquad i \in \mathbb{N}
\end{equation}
The variable $x_i$ is restricted to be in the hypercube $-500 \leq x_i \leq 500, i = 1,\ldots,n$\cite{ABCCompareStudy,numericalABC,HybridIntelliGA,DistributedHierarchicalGA,TestFunctions}. The global optimum for the function is $f(x) = 0$ when $x_i = 420.9687$\cite{ABCCompareStudy,numericalABC,HybridIntelliGA,DistributedHierarchicalGA,TestFunctions}. 

If one observes the 3D rendering of the schwefel function problem space on page~\pageref{fig:SchwefelGraph}, one can see that the search space contains a great number of peaks which might be local optima. The function also has the characteristic of having a second best optima far from the global optima which many algorithms get trapped in\cite{ABCCompareStudy,numericalABC,HybridIntelliGA,DistributedHierarchicalGA,TestFunctions}. In the literature the function is classified as being multimodal and separable\cite{ABCCompareStudy,numericalABC,HybridIntelliGA,TestFunctions}.
\subsection{Griewank}
\begin{equation}
	f(x) = \sum^n_{i=1} \frac{x^2_i}{4000} - \prod^n_{i=1}\cos{(\frac{x_i}{\sqrt{i}})} + 1, \qquad i \in \mathbb{N}
\end{equation}
The variable $x_i$ is bounded to within the hypercube $ -600 \leq x_i \leq 600 $\cite{numericalABC,ABCCompareStudy,ARPSO,PerfABC,ContinACSTS,TestFunctions}. The global optimum of the function is when $f(x) =0$ which occurs when $ x_i = 0, i = 1, \dots, n $\cite{numericalABC,ABCCompareStudy,ARPSO,PerfABC,ContinACSTS,TestFunctions}.

As with the schwefel function, the Griewank function also has a great number of peaks and valleys which many algorithm get trapped in. A particular quality of the griewank function is at low dimensions, the function is quite difficult to solve, whereas it has been shown that at higher dimensions the function becomes much easier due to there being less peaks and valleys to navigate in the search space\cite{evalevoalgo,ABCCompareStudy,numericalABC,PerfABC,TestFunctions}. In the literature the function is classified as being multimodal and non-separable\cite{adaptPSO,ABCCompareStudy,numericalABC,ChaoticABC,PerfABC,TestFunctions}.
\subsection{Salomon}
\begin{equation}
	f(x) = -\cos{(2\pi\sum_{i=1}^n\sqrt{x_i^2})} + 0.1 \sqrt{\sum_{i=1}^n x_i^2} + 1, \quad i \in \mathbb{N}
\end{equation}
Unlike the previous functions discussed in this section, the Salomon function imposes no constraint on the $x_i$ variable. The global optimum is when $f(x) = 0$ and $x_i = 0$ where $i = 1,\ldots,n$. This particular function seems to have been applied as benchmarking function yet, as no literature can be found. Nonetheless, the function is indeed a numerical optimisation function that is classified as being multimodal and non-separable\cite{http://www.it.lut.fi/ip/evo/functions/node12.html}.
\subsection{Ackley}
\begin{equation}
	f(x) = -20e^{-0.2\sqrt{\frac{1}{2}\sum_{i=1}^n x_i^2}} - e^{\frac{1}{2}\sum_{i=1}^n\cos{2\pi x_i}} + 20 + e^1, \qquad i \in \mathbb{N}
\end{equation}
The variable $x_i$ is restricted to the hypercube represented by $-32.768 \leq x_i \leq 32.768$\cite{numericalABC,ABCCompareStudy,ARPSO,TestFunctions}. The global minimum is when $f(x) = 0$ and is obtainable for $x_i = 0, i = 1,\ldots,n$\cite{numericalABC,ABCCompareStudy,ARPSO,TestFunctions}.

As can be observed from the mathematical formulation of the function, the function utilizes a exponential term. By using the exponential term the problem space contains numerous local optima which requires an algorithm to search much wider to avoid getting trapped. The literature classifies this function as being multimodal and non-separable\cite{adaptPSO,ABCCompareStudy,numericalABC,TestFunctions}.
\subsection{Six-Hump Camel Back}
\begin{equation}
	f(x_1,x_2) = (4 - 2.1x_1^2 + x_1^{\frac{4}{3}})x_1^2 + (x_1x_2) + (-4 + 4x_2^2)x_2^2
\end{equation}
The variables $x_1$ and $x_2$ are subject to the following boundary constraints $-3 \leq x_1 \leq 3$ and $-2 \leq x_2 \leq 2$\cite{DistributedHierarchicalGA,TestFunctions}. The global minimum is when $f(x_1,x_2) = -1.0316$ and is obtained when $x_1 = -0.0898$ and $x_2 = 0.7126$ or when  $x_1 = 0.0898$ and $x_2 = 0.7126$\cite{DistributedHierarchicalGA,TestFunctions}. 

True to its name the function has six peaks, four of which are local minima and two that are global minima as has already been defined. The literature classifies this function as being multimodal and non-separable\cite{ABCCompareStudy,TestFunctions}.
\subsection{Shubert}
\begin{equation}
	f(x_1,x_2) = -\sum_{i = 1}^5 (i\cos{(i +1)x_1 + 1})\sum_{i=1}^5 (i\cos{(i+1)x_2 + 1})
\end{equation}
The search domain is constrained to $-10 \leq x_i \leq 10, i = 1,2, \ldots, n$\cite{ABCCompareStudy,TSGlobalOptimization,ContinACSTS,TestFunctions}. The global optimum which is when $f(x_i) = -186.7309$\cite{ABCCompareStudy,TSGlobalOptimization,ContinACSTS,TestFunctions}. 

As can be observed from the 3D graph presented in the appendix on page~\pageref{fig:ShubertGraph} the landscape of the Shubert function contains various peaks and slopes. Thus, the function is classified as being multimodal and non-separable\cite{ABCCompareStudy,TestFunctions}.
\subsection{Himmelblau}
\begin{equation}
	f(x_1,x_2) = (x_1^2 + x_2 - 11)^2 + (x_1 + x_2^2 - 7)^2
\end{equation}
The variables $x_1,x_2$ are constraint to be within the hypercube represented by $-6 \leq x_1 \leq 6, -6 \leq x_2 \leq 6$\cite{TestFunctions,ABCCompareStudy}. The Himmelblau function contains no local optima, but on the contrary, it has 4 global optima when $f(x_i) = 0$ which can be obtained at the following points\cite{TestFunctions,ABCCompareStudy}.
\begin{itemize}
\item $(x_1,x_2) = (-3.779310,-3.283185)$
\item $(x_1,x_2) = (-2.805118,3.131312)$
\item $(x_1,x_2) = (3,2)$
\item $(x_1,x_2) = (3.584428,-1.848126)$
\end{itemize}
The literature classifies the function as being multimodal and non-separable\cite{TestFunctions,ABCCompareStudy}.
\subsection{Rosenbrock Valley}
\begin{equation}
	f(x) = \sum_{i=1}^{n-1}[100(x_{i+1} - x_i^2)^2 + (1-x_i)^2
\end{equation}
The variable $x_i$ is bounded to with the following constraint $ -2.048 \leq x_i \leq 2.048 $\cite{numericalABC,ABCCompareStudy,ARPSO,PerfABC,TSGlobalOptContinFunc,HybridIntelliGA}. The global optimum is when $f(x) = 0$ and is obtained when $x_i = 1, i = 1,\ldots,n$\cite{numericalABC,ABCCompareStudy,ARPSO,TSGlobalOptContinFunc,HybridIntelliGA}.

The rosenbrock search space has a curving valley which leads forces algorithms to coupe with the changing direction of the landscape\cite{numericalABC,ABCCompareStudy,ChaoticABC,PerfABC,HybridIntelliGA}. If the algorithm does not adapt to the changing direction it will fail in locating the global optimum. The function is classified as being multimodal and non-separable\cite{numericalABC,ABCCompareStudy,ChaoticABC,PerfABC,HybridIntelliGA}.
\subsection{Dropwave}
\begin{equation}
	f(x) = -\frac{1 + \cos{(12\sqrt{x_1^2 + x_2^2})}}{\frac{1}{2}(x_1^2 + x_2^2) + 2}
\end{equation}
The variables $x_1$ and $x_2$ are restricted to be within the following bounds $-5.12 \leq x_i \leq 5.12$\cite{TestFunctions}. The landscape of this function resembles that of a droplet falling into a pool of water, as can be observed from the 3D graph presented in the appendix on page~\pageref{fig:DropwaveGraph}. Due to its ``wave'' nature, the function has various local minima and only a single optima which is when $f(x_1,x_2) = 0$. The function is classified being multimodal and non-separable\cite{TestFunctions}.
\subsection{Easom}
\begin{equation}
	f(x_1,x_2) = -\cos(x_1)\cos(x_2)e^{(-(x_1 - \pi)^2 - (x_2 - \pi)^2)}
\end{equation}
The variables $x_1$ and $x_2$ are restricted to be within the hypercube represented by $-100 \leq x_1 \leq 100, -100 \leq x_2 \leq 100$\cite{TSGlobalOptContinFunc,ContinACSTS,TestFunctions}. The global minimum is when $f(x_1,x_2) = -1$ and is obtainable if $(x_1,x_2) = (\pi,\pi)$\cite{TSGlobalOptContinFunc,ContinACSTS,TestFunctions}. 

The easom function has a deceptive global minimum as it is very close to other local minima\cite{ABCCompareStudy,TSGlobalOptimization}. As can be observed from the 3D graph of the function on page~\pageref{fig:EasomGraph}, the search space is a flat with the global minima clearly visible. Flat search spaces are difficult to navigate by algorithms as the surrounding area gives no indication whether the algorithm is on the right track of not. The function is classied as being multimodal and non-separable in the literature \cite{ABCCompareStudy,TSGlobalOptimization,TSGlobalOptContinFunc}.
\subsection{Branins}
\begin{align}
	f(x_1,x_2) &= a(x_2 - bx_1^2 + cx_1 - d)^2 + e(1-f)\cos{x_1} + e \\
\intertext{where}
	a &= 1\nonumber\\
	b &= \frac{5.1}{4\pi^2}\nonumber\\
	c &= \frac{5}{\pi}\nonumber\\
	d &= 6\nonumber\\
	e &= 10\nonumber\\
	f &= \frac{1}{8\pi}\nonumber
\end{align}
The variables $x_1$ and $x_2$ are subject to the following boundary constraints $-5\leq x_1 \leq 10, 0 \leq x_2 \leq 10$\cite{ABCCompareStudy,TSGlobalOptimization,ContinACSTS,TestFunctions}. The global optimum is when $f(x_1,x_2) = 0.397887$ and is obtainable when $x_1$ and $x_2$ have the following values\cite{ABCCompareStudy,TSGlobalOptimization,ContinACSTS,TestFunctions}:
\begin{enumerate}
\item $x_1 = -\pi,\:x_2=12.275$
\item $x_1 = \pi,\:x_2=2.275$
\item $x_1 = 9.42478,\:x_2=2.475$
\end{enumerate}
The literature classifies this function as being multimodal and separable\cite{ABCCompareStudy,TSGlobalOptimization,ContinACSTS,TestFunctions}.
\subsection{Michalewicz}
\begin{equation}
	f(x) = -\sum_{i=1}^n\sin{(x_i)}[\sin{(\frac{(1 - x_i^2)}{\pi})}]^{2m}, \qquad i,m \in \mathbb{N}
\end{equation}
The variable $x_i$ is usually constricted to the following defined boundary $0 \leq x_i \leq \pi, i = 1,\ldots,n$\cite{ABCCompareStudy,TestFunctions}. The parameter $m$ defines the steepness of the valleys in the function. The function has two approximated global minima's that are difficult to locate since their size in comparison to the rest of the search space is relatively small\cite{ABCCompareStudy,TestFunctions}.
%http://www.geatbx.com/docu/fcnindex-01.html#P216_11735
\begin{enumerate}
\item $f(x) = -4.687,\: n = 5$
\item $f(x) = -9.66,\: n = 10$
\end{enumerate}
In the literature the function is classified as being multimodal and separable\cite{ABCCompareStudy}.
\subsection{Goldstein}
\begin{align}
	f(x_1,x_2) &= (1 + (x_1 + x_2 + 1)^2)\nonumber\\
			   &=*(19-14x_1+3x_1^2 -14x_2 + 6x_1x_2 + 3x_2^2)\nonumber\\
			   &=*(30 + (2x_1 -3x_2)^2\nonumber\\
			   &=*(18 - 32x_1 + 12x_1^2 +48x_2 -36x_1x_2 + 27x_2)\nonumber
\end{align}
The variables $x_1$ and $x_2$ are subject to the following boundary constraints $-2 \leq x_1 \leq 2, -2 \leq x_2 \leq 2$\cite{ABCCompareStudy,TSGlobalOptimization,TSGlobalOptContinFunc,ContinACSTS,TestFunctions}. The function has only one global minimum four local optima\cite{ABCCompareStudy,TSGlobalOptimization}. The global optimum is when $f(x_1,x_2) = 3$ and is obtainable when $x_1 = 0$ and $x_2 = -1$\cite{ABCCompareStudy,TSGlobalOptimization,TSGlobalOptContinFunc,ContinACSTS,TestFunctions}.

The literature classifies this function as being multimodal and non-separable\cite{ABCCompareStudy}.

\section{Results}
\label{sec:benchResults}
In the previous section sixteen benchmark functions were mathematically defined, where applicable comments were given on the search space. For each function the following was explicitly stated:
\begin{itemize}
\item The global optimum and where is located.
\item Whether the function is unimodal or multimodal.
\item Whether the function is non-separable or separable.
\end{itemize}
In this section the results will be presented of how the two PSO algorithms that were developed performed on each of the presented benchmark functions. 

The algorithms will be compared with the following criteria on each benchmark.
\begin{itemize}
\item The number of iterations the algorithm took to find the global optimum
\item The accuracy of the algorithm, which means if the algorithm has indeed located the optimum or if not, how far off is the algorithm.
\item The PSO is a population-based algorithm, thus diversity is important. Diversity will be measured and compared on each benchmark function.
\end{itemize}
For each of the above criteria a subsection will be presented. Where possible the results obtained will be compared to other algorithms that have been applied to similar test functions.

\subsection{Fina values}
\begin{center}
	\begin{tabular}{| c | c | c | c | c | c |}
	\hline
	Function & PSO & PSO* & GA & TS & ABC\\  \hline
	DeJong F1 & 0 & -- & -- & -- & --\\ \hline
	Shekel's Foxhole & 0 & -- & -- & -- & --\\ \hline
	Rastrigin & 0.74622 & -- & N/A & N/A & N/A\\ \hline
	Schwefel & -1.19069e+43 & -- & N/A & N/A & N/A\\ \hline
	Griewank & 1.00003 & -- & N/A & N/A & N/A\\ \hline
	Salomon & -0.460561 & -- & -- & -- & --\\ \hline
	Ackley & -4.5901 & -- & N/A & N/A & N/A\\ \hline
	Six-Hump camelback & -1.03163 & -- & -- & -- & --\\ \hline
	Shubert & -186.731 & -- & -- & -- & --\\ \hline
	Himmelblau & 1.02103 & -- & -- & -- & --\\ \hline
	RosenbrockValley & 1.81101e-08 & -- & -- & -- & --\\ \hline
	Dropwave & 4.59843e-15 & -- & -- & -- & --\\ \hline
	Easom & -1 & -- & -- & -- & --\\ \hline
	Branins & 1.22875 & -- & N/A & N/A & N/A\\ \hline
	Michalewicz & -2.40391 & -- & -- & -- & --\\ \hline
	Goldstein & infinity & -- & -- & -- & --\\ \hline
	\end{tabular}
\end{center}
\subsection{Diversity}
\begin{center}
	\begin{tabular}{| c | c | c | c | c | c |}
	\hline
	Function & PSO & PSO* & GA & TS & ABC\\  \hline
	DeJong F1 & -- & -- & -- & -- & --\\ \hline
	Shekel's Foxhole & -- & -- & -- & -- & --\\ \hline
	Rastrigin & -- & -- & -- & -- & --\\ \hline
	Schwefel & -- & -- & -- & -- & --\\ \hline
	Griewank & -- & -- & -- & -- & --\\ \hline
	Salomon & -- & -- & -- & -- & --\\ \hline
	Ackley & -- & -- & -- & -- & --\\ \hline
	Six-Hump camelback & -- & -- & -- & -- & --\\ \hline
	Shubert & -- & -- & -- & -- & --\\ \hline
	Himmelblau & -- & -- & -- & -- & --\\ \hline
	RosenbrockValley & -- & -- & -- & -- & --\\ \hline
	Dropwave & -- & -- & -- & -- & --\\ \hline
	Easom & -- & -- & -- & -- & --\\ \hline
	Branins & -- & -- & -- & -- & --\\ \hline
	Michalewicz & -- & -- & -- & -- & --\\ \hline
	Goldstein & -- & -- & -- & -- & --\\ \hline
	\end{tabular}
\end{center}
\subsection{Accuracy}
\begin{center}
	\begin{tabular}{| c | c | c | c | c | c |}
	\hline
	Function & PSO & PSO* & GA & TS & ABC\\  \hline
	DeJong F1 & -- & -- & -- & -- & --\\ \hline
	Shekel's Foxhole & -- & -- & -- & -- & --\\ \hline
	Rastrigin & -- & -- & -- & -- & --\\ \hline
	Schwefel & -- & -- & -- & -- & --\\ \hline
	Griewank & -- & -- & -- & -- & --\\ \hline
	Salomon & -- & -- & -- & -- & --\\ \hline
	Ackley & -- & -- & -- & -- & --\\ \hline
	Six-Hump camelback & -- & -- & -- & -- & --\\ \hline
	Shubert & -- & -- & -- & -- & --\\ \hline
	Himmelblau & -- & -- & -- & -- & --\\ \hline
	RosenbrockValley & -- & -- & -- & -- & --\\ \hline
	Dropwave & -- & -- & -- & -- & --\\ \hline
	Easom & -- & -- & -- & -- & --\\ \hline
	Branins & -- & -- & -- & -- & --\\ \hline
	Michalewicz & -- & -- & -- & -- & --\\ \hline
	Goldstein & -- & -- & -- & -- & --\\ \hline
	\end{tabular}
\end{center}

\section{Summary}
In this chapter a series of mathimatical optimisation problems were presented. Each problem was mathematically formulated as well as categorised as to whether the particular problem is multi-modal and seperable.

To get a better idea on how the Particle Swarm Optimisation (PSO) algorithm operates and performs two PSO algortihms were developed. The first PSO algorithm uses the standard velocity equation with no alterations. The second PSO algorithm uses the notion of inertia with regard to velocity calculation.

The algorithms that were developed were tested on all the mathematical problems outlined at the start of this chapter. The chapter concluded with a presentation of the results obtained by the algorithms along with a short discussion on the obtained results.
