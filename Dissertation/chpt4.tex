\chapter{Metaheuristic Algorithms}
\label{chpt:heuristic}
\section{Introduction}
Metaheuristics is a subdomain of the \gls{AI} domain\cite{AIModernApproach}. It evolved out of a need for more efficient search techniques with regard to hard problems. 

Metaheuristics forms part of a collective body of algorithms that use heuristics to search a particular domain's search space for the most optimal solution\cite{AIModernApproach,NatureInspiredMetaHeuristic}. Some problems explicitly define constraints to which the produced solution must adhere to. Problems that define such constraints are referred to as constraint optimisation problems\cite{FundamentalSwarm}. Problems where there are either no constraints or just a boundary constraint  defined are referred to as unconstrained problems\cite{FundamentalSwarm}.

Constraints can be either hard or soft. A hard constraint is defined as a certain condition an algorithm or potential solution is not allowed to violate\cite{AIModernApproach,NatureInspiredMetaHeuristic,Karen2004,Eisenblatter}. A soft constraint is allowed to be violated but there is some sort of penalty or cost involved which is imposed on the potential solution, which lowers its desirability\cite{AIModernApproach,NatureInspiredMetaHeuristic,Karen2004,Eisenblatter}. 

An optimal solution would therefore be any solution that violates no hard constraints and violates no or a minimum number of soft constraints\cite{AIModernApproach,NatureInspiredMetaHeuristic,Karen2004,Eisenblatter}. Algorithms that are classified as being part of the collective body of algorithms known as metaheuristic algorithms are Tabu Search\cite{TabuVechicleRoutingWithTimeWindows,TabuCSP}, Simulated Annealing \cite{SASingleMultiObj,CurveFittingSA} and Genetic Algorithm\cite{GATSP, GeostatisticalGA}.

The above-mentioned algorithms are not the only algorithms to form part of this subdomain, but they are the algorithms that have received the most attention in the literature and produce good results\cite{SweepMeta}.
The main focus of this chapter is on each of the above algorithms. Before each of the algorithms is discussed, a brief overview is given of the various characteristics that metaheuristic algorithms exhibit. 

\gls{AI} search algorithms operate in search spaces, where they occupy various states while they are searching for a solution or goal state in a search space. Before these algorithms can be properly introduced, the concept of search spaces and states needs to be introduced.

\section{Search Spaces and States}
A search space is defined as a set of candidate goal states, which may or may not be a solution to a problem\cite{AIModernApproach}. The candidate states are defined by the problem definition\cite{AIModernApproach}. The possible states can be given by a successor function, which generates new states adhering to the problem-defined constraints if any\cite{AIModernApproach}. However, this is not true for all problem domains. In some problem domains the genereted states violate the problem constraints. The aim of the search in these problem domains is to then identify a state which satisfies all problem constraints.

The search space can also be explicitly defined with constraints to which a solution must adhere to\cite{AIModernApproach}. When constraints are defined it is up to the algorithm how it moves from one state to another in the search space\cite{AIModernApproach}. Each state the algorithm moves to is then checked whether it adheres to the constraints\cite{AIModernApproach}.

A state is a position in the search space\cite{AIModernApproach}. This position represents a configuration that describes a possible candidate solution to the defined problem\cite{AIModernApproach}. When an algorithm is said to be moving to a new state, the algorithm moves to a new position in the search space\cite{AIModernApproach}. This new position represents a different candidate solution than the previous position that was occupied\cite{AIModernApproach}.

When dealing with an optimisation problem there can be more than one candidate solution\cite{AIModernApproach}. Which is why with optimisation problems not only does an algorithm have to find a solution, but also the most optimal solution among the candidate solutions\cite{AIModernApproach}. This subset of solutions in the search space is referred to as the \emph{solution} space.

Both concepts of search spaces and states have now been defined. All search algorithms operate in search spaces and occupy states. The domain of search contains a wide variety of algorithms. The following section discusses metaheuristics algorithms as this body of algorithms are used to solve optimisation problems.

\section{Metaheuristics Algorithms}
NP-Complete\footnote{A discussion of NP-Complete problems appears in section \ref{sec:NPComplete}} problems have been proven not to be solvable in polynomial time by traditional uninformed search algorithms such as Breath-first Search and Depth-first Search\cite{AIModernApproach}. 
Uninformed algorithms are not able to distinguish whether a particular goal state is more ``correct'' than any other goal state\cite{AIModernApproach}. Uninformed search algorithms search spaces are represented by a tree structure\cite{AIModernApproach}. 

A search is performed in the search space by starting at a root node, the algorithm expands successive nodes based on a strategy\cite{AIModernApproach}. Bread-first search continuously expands all successive nodes and Depth-first search expands the deepest node first\cite{AIModernApproach}.

The path taken to a final  goal state node represents a candidate solution\cite{AIModernApproach}. In a worst-case scenario, an uninformed search will expand every single node in its search space, thus each and every single possible solution is evaluated\cite{AIModernApproach}.

It is not always viable to test every possible candidate solution in a given problem search space, especially in NP-Complete problems, since their search spaces are huge or infinite. This is why traditional algorithms are not able to produce optimal solutions in polynomial time\cite{AIModernApproach}.

Metaheuristic algorithms are considered to be \emph{general-purpose} algorithms and can thus be applied to a wide variety of optimisation problems with only small modifications that need to be made to the algorithm model\cite{MetaGraph}.
Metaheuristic algorithms do not dictate all aspects of the search procedure but define guidelines and aspects which directs the search\cite{HandbookofMH}. Therefore a metaheuristic can be defined as follows: \emph{the algorithm selects candidate solutions from a neighbourhood with respect to one or more current solutions of which the candidates are either rejected or accepted}\cite{HandbookofMH}.

As can be gathered from the name, a metaheuristic algorithm uses some sort of heuristic. A heuristic is a decision rule. Algorithms use heuristics to make decisions by applying the heuristic to the data the algorithm is currently working on\cite{AIModernApproach,NatureInspiredMetaHeuristic}.

When an algorithm utilises a heuristic its forms part of a group known as informed search algorithms. As the name implies the algorithm is more ``informed'' using the heuristic the algorithm gains additional information about its search space. With this additional information the algorithm is able to perform a better search for a possible candidate solution\cite{AIModernApproach}.

Heuristics are strategies that direct the search based on the information available, to move towards areas where there is a higher probability of obtaining high quality candidate solutions \cite{AIModernApproach}. A heuristic is able to dictate what an algorithm must do for its next iteration by evaluating the current internal state of the algorithm, i.e. whether it should move to a different point in the search space, generate new data, or select the current data as the most optimal solution\cite{AIModernApproach,NatureInspiredMetaHeuristic}. 

In general ``meta'' means \emph{beyond} or \emph{higher level}\cite{AIModernApproach,NatureInspiredMetaHeuristic}. A metaheuristic therefore refers to a heuristic that is more complex with regard to the decisions it is able to make compared with a standard heuristic\cite{AIModernApproach,NatureInspiredMetaHeuristic}. With a standard heuristic only the current state is considered\cite{AIModernApproach}. A metaheuristic takes additional information into account when considering a current state\cite{NatureInspiredMetaHeuristic}. This additional information can be previous states, or states that are considered to be close in the search space\cite{AIModernApproach, NatureInspiredMetaHeuristic}.

Metaheuristic search algorithms are not guaranteed to find the most optimal solutions in the search space; instead these algorithms are used to find near-optimal solutions. Thus most algorithmic development in the metaheuristic domain focuses on developing new techniques that will increase the probability that a good solution will be obtained in difficult combinatorial problems \cite{MetaAgricultural}.

Similarly, metaheuristics are not guaranteed to find suitable candidate solutions or perform well in each problem domain it is applied to. The quality of the solution and performance of the metaheuristic is very much dependent upon on the expertise of the algorithm designer \cite{AutoComplexMeta}. Besides the algorithm designer modifying the algorithm; metaheuristic algorithms that are inherently population-based, hybrid and/or distributed, use the concept of social and self-organisation to better exploit the solution space\cite{Self-AdaptiveMeta}.

In this section the characteristics of metaheuristics that set these algorithms apart from the conventional algorithms used on difficult problems was introduced. The next section of this chapter presents the tabu search algorithm.
\section{Tabu Search}
\label{sec:tabusearch}
\subsection{Introduction}
\label{sec:TSIntroduction}
\glsreset{TS}\Gls{TS} was first proposed by Glover\cite{Glover89} as a new searching technique to help algorithms avoid getting trapped in local optima in combinatorial and optimisation problems \cite{TabuRCAProblem}. Since Glover introduced the algorithm in the 1980s, tabu search has been applied to a wide range of problems such as the vehicle routing problem\cite{TabuVechicleRoutingWithTimeWindows}, \gls{FAP}\cite{TabuMontemanniSmith}, capacitated-lot sizing problem\cite{TabuCarryOver}, Nurse Scheduling\cite{TabuNurse} and the Resource Constrained Assignment Problem\cite{TabuRCAProblem}. 

Even though the problems mentioned differ by a large margin, the algorithm has been successful in most optimisation problems it has been applied to. If one observes the results obtained in research \cite{TabuMontemanniSmith,tabuglobalplanning3g}, it can be inferred that tabu search has on average obtained the best results compared with previous attempts with other algorithms. 

\Gls{TS} resembles in its most basic form the hill-climbing search algorithm\cite{TabuBiddingStrats}. The hill-climbing search algorithm starts from an initial candidate solution and then iteratively moves from the current solution to a neighbouring solution\cite{AIModernApproach}. Each neighbour is rated based on its attractiveness as a possible optimal solution that the algorithm is being applied to\cite{AIModernApproach}. 

The hill-climbing algorithm moves to the neighbour with the highest rating without considering whether the neighbour might lead the algorithm astray, to a position where the neighbours are in fact \emph{worse} than previously encountered possible solutions\cite{AIModernApproach}. 
The \gls{TS} algorithm addresses this shortcoming by introducing the concept of memory\cite{TabuBiddingStrats}. The memory of the algorithm is actually a history of previous candidate solutions that the algorithm has moved to in its search for the search space for a candidate solution\cite{TabuBiddingStrats}. The memory used by the \gls{TS} algorithm is known as the \emph{tabu list}.

General search algorithms like hill-climbing, random-restart\footnote{Random-restart is a search algorithm where once a certain trend of repeated moves is noticed, the algorithm restarts by generating a new initial candidate solution to start from and then continues its search process from that generated solution\cite{AIModernApproach}.} or local beam search\footnote{Local beam search keeps track of $k$ states. The algorithm generates all successor states for each $k$ state. If the goal is among the generated states. The algorithm stops. Otherwise the best successor amongst the generated successors is selected and the algorithm repeats\cite{AIModernApproach}.} tend to get trapped in local optima \cite{AIModernApproach}. The local optima might be a very attractive candidate solution and thus general search algorithms will not move to better solutions since, according to the algorithm's built-in strategy, it has found the best candidate solution. 

In actual fact the candidate solution that was found is the best solution in the \emph{local} search space, but not in the \emph{global} search space\cite{CompuIntelligenceIntro,AIModernApproach}. Therefore an important characteristic of algorithms being applied to optimisation problems is breaking out of local optima\cite{CompuIntelligenceIntro,AIModernApproach}.
The next section discusses some of the characteristics that make the \gls{TS} algorithm unique.

\subsection{Important Tabu Search Characteristics}
\label{sec:TScharacteristics}
Various characteristics are important to the \gls{TS} algorithm. The first characteristic is exactly how the \gls{TS} algorithm iteratively improves upon the initial start solution.

\subsubsection{Initial Solution Generation}
The core feature of the \gls{TS} algorithm is to sequentially improve an initial candidate solution \cite{TSHazardous}. An initial possible solution is a point in the search space where the \gls{TS} algorithm will \emph{start} exploring in search of a more optimal candidate solution \cite{AIModernApproach,TSHazardous}.
An important consideration one has to make is how initial candidate solutions are generated for the \gls{TS} algorithm to start on\cite{AIModernApproach,TSHazardous}.

Random initial solutions might seem to be a good starting point, but by introducing randomisation it becomes hard to control the quality of the end candidate solution\cite{TSHazardous}. Hence the generation of starting solutions must be controlled to limit the infeasibility of potential solutions \cite{TSHazardous}. 

Control of the randomly generation solutions can be achieved by simply constraining the random solution generator to only generate initial starting points in a bounded subset of the entire search space. For example: instead of letting the random initial starting point be any number between positive infinity and negative infinity, the random number generator is constrained to only generate numbers between 5 and -5.

\subsubsection{Neighbourhood Search}
The following discussion on neighbourhood search is not meant to be an exhaustive survey on the different methods and how they differ under different problems. Instead the discussion is meant as a general overview to get an idea of neighbourhood generation in the \gls{TS} algorithm context. The following neighbourhood discussion will be based on the assumption that the underlying problem the \gls{TS} is applied to, has a search space with defined boundaries that is suitable for neighbourhood generation.

TS uses a neighbourhood local search process to explore the solution space. There is no set process of how neighbourhood candidate solutions are selected as it is problem dependant. The overall quality of the solution produced by \gls{TS} is also dependent on the neighbourhood search strategy used \cite{TSHazardous}. 

The neighbourhood search phase is the first operation performed after the algorithm has been initialised, which is to say the algorithm has generated an initial starting candidate solution from which the exploration process can start. The neighbourhood search phase is the primary means for the \gls{TS} algorithm to search the solution space for an optimal solution. It is within this phase that new possible candidate solutions must be presented for the \gls{TS} heuristic to allow the algorithm to decide to which solution it must move next.

The new possible candidate solutions that are generated are called neighbouring solutions; hence the \gls{TS} algorithm always moves to a neighbouring solution. When the \gls{TS} algorithm moves to a neighbouring solution, the current solution is replaced by the neighbouring solution. Therefore, in the next iteration, neighbours for the new solution need to be generated. Generation of new neighbours can range from a simple increment option to a complex operation that incorporates additional intelligence by means of a more heuristic approach to generate new neighbours. This different means of generating neighbours is referred as a \emph{neighbourhood search strategy}.

The \gls{TS} algorithm is not limited to just one neighbourhood search strategy. In the research by Gopalakrishnan \emph{et al.}\cite{TabuCarryOver} five-neighbourhood move strategies are developed and are used interchangeably; in some cases a strategy is used three times in a row due to stagnation in the search space. Stagnation occurs when the algorithm does not move to a better solution; \gls{TS} opts to stay on the current solution, as no neighbouring solution is better than the current one. 

Another neighbourhoud strategy that \gls{TS} can use is the node exchange strategy. In research conducted by Wasan the node exchange strategy is utilised on the vechile routing problem. Each candidate solution represents a vehicle route and each node is destination on the vechiles route. A series of connected nodes represents a route. Node exchange actually ocnsists of two methods called \emph{1-exchange} and \emph{2-exchange}\cite{ReactiveTabuVHR}.
\begin{itemize}
\item{1-exchange} --- Moves a node on one route and places it on another route and then swapping out two nodes between two given routes\cite{ReactiveTabuVHR}
\item{2-exchange} --- Extends 1-exchange by moving two consecutive nodes from one route and places it on another route. Additionally it also swaps out four nodes between two given routes by taking two consecutive nodes from each route\cite{ReactiveTabuVHR}.
\end{itemize}

Depending on how nodes are connected many different unique routes can be created. A single node being connected to a different node represents a different candidate solution. Therefore, node exchange enables the \gls{TS} algorithm to search much more broadly due to the constant supply of different solutions. Since candidate solutions are constantly modified, it enables the \gls{TS} procedure to be a very fined-grained process, because often a small change in a potential candidate solution can have a big impact on the overall proposed solution by the \gls{TS} algorithm.

In the research done by Zhang \emph{et al.}\cite{TSHazardous} a neighbourhood selection scheme called \emph{dynamic penalty} is developed. When the algorithm moves to an infeasible solution a penalty is imposed. By dynamically changing the penalty that is imposed the ``feasibility'' of solutions produced is influenced. 

Therefore, if and when the algorithm continually produces infeasible solutions, the penalty imposed is increased to guide the algorithm to produce more feasible solutions. Finally, when the algorithm becomes trapped at local optima, the penalty is reduced, which allows the algorithm to consider moving onto infeasible solutions thus escaping local optima.

TS is an iterative algorithm, executing a set of operations sequentially until a stopping criterion is met\cite{EvoParallelTabu,TabuVechicleRoutingWithTimeWindows}. At each iteration the algorithm has to determine feasibility of the immediate neighbourhood candidate solutions \cite{EvoParallelTabu,TabuVechicleRoutingWithTimeWindows}. 

Therefore each candidate must be evaluated by some function, which may be a costly operation in terms of computational cycles as well as in terms of time\cite{EvoParallelTabu,TabuVechicleRoutingWithTimeWindows}. This constant evaluation can drastically reduce the overall performance of the algorithm, since it is spending more time calculating feasibility than actually searching the solution space \cite{EvoParallelTabu,TabuVechicleRoutingWithTimeWindows}. 

\subsubsection{Memory Structures of Tabu Search}
The hill-climbing and random-restart algorithms are able to break out of local minima, but there is nothing stopping these algorithms from avoiding the local optima with their second or n-pass in the search space. \gls{TS} addresses the shortcoming of these algorithms by incorporating an important concept: the notion of memory.

In its most basic form \gls{TS} keeps a local memory of all its recent best moves, and puts them into a \emph{tabu list} that has a predefined size. In the literature the Tabu list is also referred to as the \emph{tabu tenure} \cite{TSHazardous,TabuCarryOver}. The algorithm is not allowed to move to any solution that is in the tabu list unless a solution that is \emph{tabu} is better than any current moves available in the immediate search neighbourhood \cite{TabuCarryOver,ReactiveTabuVHR}. The process of overriding a solution's tabu status in the tabu tenure is called the \emph{aspiration criterion} \cite{TSHazardous,TabuCarryOver}. With the use of the tabu tenure and the aspiration criterion, the algorithm is able to avoid cycling, local optima as well as searching in a too narrow region \cite{TabuSingleMachineScheduling,CircuitTabu}.

Research done by Sureka and  Wurman makes an important distinction with regard to the memory scheme that is used in the \gls{TS} algorithm. Two memory schemes are discussed: \emph{explicit memory} and \emph{attribute-based memory} \cite{TabuBiddingStrats,TabuFormGames}. Of the two memory schemes the explicit memory scheme is the most used in the literature \cite{TabuVechicleRoutingWithTimeWindows}.

With explicit memory the algorithm stores a complete candidate solution in the tabu tenure; hence the algorithm is prohibited from moving to that position in the search for as long as the solution is in the tabu tenure\cite{TabuBiddingStrats,TabuFormGames}. With attribute-based memory the algorithm stores the \emph{operation} used to move from the previous solution to the current solution\cite{TabuBiddingStrats,TabuFormGames}. Therefore with attribute-based memory the tabu tenure intended function is changed from prohibiting certain solutions already encountered to rather prohibiting making changes to the current solution that would lead to solutions already present in the tabu tenure \cite{TabuBiddingStrats,TabuFormGames}.

In research conducted by Clarkson \emph{et\ al.}\cite{MultiObjTabu}, the authors add two additional memory structures called \gls{MTM} and \gls{LTM} besides the standard \gls{STM}, referred to as the tabu list \cite{MultiObjTabu}. Each additional structure remembers a different set of candidate solutions for use by the diversification and intensification phases in the algorithm. Both of these phases are discussed in depth in the next section.

STM is similar to the traditional tabu list: to store the most recent candidate solutions produced by the algorithm. \gls{MTM} is designed to remember optimal or near-optimal candidate solutions. These solutions are therefore used later in the intensification phase. Finally, the \gls{LTM} structure stores all the solutions that the algorithm has already explored and is thus used in the diversification phase of the algorithm \cite{MultiObjTabu}.

\subsubsection{Search Phases}
\label{TSSearchPhases}
As \gls{TS} searches through the search space, it goes through two cycles of search phases called \emph{diversification} and \emph{intensification} \cite{TabuParameterization,TabuCrewSchedulingProblem,NonlinearGlobalTabu,SelfControllingReactiveTabu}. The diversification phase in the \gls{TS} algorithm is the phase where the algorithm is directed to areas in the search space that has not yet been explored\cite{ReactiveTabuVHR,SelfControllingReactiveTabu}.

Research by Fescioglu-Unver and Kokar \cite{SelfControllingReactiveTabu} provides a strategy that consists of two components namely the \emph{observer} and the \emph{diversifier}. The goal of the observer is to continually monitor the best solution obtained by the algorithm as to whether it violates the \emph{stagnation period}. The stagnation period is defined as the number of iterations where the current best-obtained solution has not changed and the algorithm has not moved to a new solution\cite{SelfControllingReactiveTabu}. 

As soon as the current solution exceeds the stagnation period the observer component activates and transfers the necessary information needed by the diversifier component. The diversifier component dynamically changes the size of the tabu tenure based on the information the observer gathered. The diversifier mainly targets older moves to diversify, but for short bursts of time it decreases the tabu list size to a very small value in an attempt to combine new and old moves \cite{SelfControllingReactiveTabu}.

The specific mechanism used to define a new position where the algorithm can continue to search, should ideally select areas in the search space that have not been explored yet\cite{NonlinearGlobalTabu,SelfControllingReactiveTabu}. Therefore, the diversification phase makes extensive use of the knowledge present in the long-term memory structures as an indication of what areas of the search space have been previously explored and which areas have not \cite{NonlinearGlobalTabu,SelfControllingReactiveTabu}.

Intensification is the first phase of the \gls{TS} algorithm, since it is responsible for building up a history in memory on which the diversification phase can act. Fescioglu-Unver and Kokar also present an intensification strategy based on control theory in their research \cite{SelfControllingReactiveTabu}. The authors identify repetition length as a critical value for their intensification strategy to be based upon. The repetition length is a control measure that defines how many times the algorithm can occupy the same solution within a span of iterations. The following section presents an overview of the flow the \gls{TS} algorithm along with pseudo code describing the \gls{TS} algorithm.
\subsection{Flow of the algorithm}
In this section the general flow of the \gls{TS} algorithm is described using algorithm~\ref{alg:TS} as a reference point.
\begin{algorithm}[H]
\caption{Basic Tabu Search Algorithm\cite{TabuRCAProblem,TabuMontemanniSmith}}
\label{alg:TS}
	\begin{algorithmic}[1]
		\State Initialize parameters
    \State $\hat{x_0} \leftarrow$ Initialize starting solution
		\While{stopping criteria not met}
    \State $\hat{y_i} \leftarrow$ Determine $\hat{x_i}$ neighbourhood solutions 
    \State Evaluate neighbouring solutions with fitness function $f(\hat{y_i})$
    \State $\hat{z_i} \leftarrow$Select best neighbour from $\hat{y_i}$
    \If{Move to $\hat{z_i}$ is Tabu}
    \If{$\hat{z_i}$ meets Aspiration Criterion}
    \State $\hat{x_i} \leftarrow \hat{z_i}$
				\EndIf
			\Else
      \State Add $\hat{x_i}$ to Tabu List
      \State $\hat{x_i} \leftarrow \hat{z_i}$
      \If{$\hat{x_i}$ repeated $\ge$ max repeats}
					\State diversify()
				\Else
					\State intensify()
				\EndIf
			\EndIf
		\EndWhile
    \State Return $\hat{x_i}$ as best found solution
	\end{algorithmic}
\end{algorithm}

Before the algorithm can actually start searching, it first needs to initialise various parameters. These parameters include, but are not limited to, the tabu list size, the aspiration criterion and the starting solution. The initialisation can be observed to occur from lines 1 - 2. Once all the various parameters that are needed by the algorithm have been initialised, the algorithm is ready to enter the actual search phase, which ranges from lines 3 -- 21. 

The search phase starts off by first generating possible solutions that neighbour the current solution $x_i$ as can be observed in line 4. Generating neighbouring solutions are a critical process in the \gls{TS} algorithm as they are the means by which the algorithm is able to move from one possible solution to the next in the search space.

After all the solutions that are in the neighbourhood of the current possible solution have been generated, the algorithm needs to decide which of the possible neighbours is the most rewarding. The algorithm therefore determines the fitness of each neighbour $y_i$ by applying a fitness function $f(y_i)$. 

Once all the neighbours have been evaluated, the algorithm selects the best neighbour that not only has the best fitness out of all the generated neighbours, but also has a better fitness than the current solution held by the algorithm. The best neighbour selection can be seen to occur in line 6.

The algorithm has now determined a possible neighbour $z_i$ to move towards. Before moving on to the next iteration, it first needs to perform a series of checks that will aid it in the search process. The first check that needs to be performed is whether the neighbour $z_i$ is in the tabu list and this occurs in line 7. 

If the neighbour $z_i$ is in the tabu list, then another check is performed. The next check that is performed determines the probability that the algorithm can move to a neighbourhood solution even though it is tabu. The higher the probability the more likely the algorithm will move to a neighbourhood solution even though it is tabu. This probablity is known as the aspiration criterion and is calculated on lines 8 -- 10. If the aspiration criterion has been met, the algorithm makes neighbour $z_i$ its current solution $x_i$.

In the algorithm, if a neighbour $z_i$ is found not to be in the tabu list, the algorithm then adds the currently held solution $x_i$ to the tabu list. The current solution is added to prohibit future movements to the same solution in an attempt to avoid cycling of solutions. After $x_i$ has been added to the tabu list, the algorithm makes $z_i$ the current solution $x_i$. This process can be observed from lines 12 -- 14.

Before the algorithm continues to the next iteration, it performs one last final check. The purpose of this check is to determine whether the algorithm is repeating solutions. As can be observed from lines 15 -- 19, the algorithm calculates whether the new selected solution has been repeated for a certain number of iterations. If the solution has indeed been repeated for a predetermined number of iterations, the algorithm activates its diversification strategy or intensifies its search.
The section that follows presents an overview of the \gls{TS} applied to the \gls{FAP}.
\subsection{Tabu Search on the \gls{FAP}}
In a study conducted by Montemanni and Smith \cite{TabuMontemanniSmith} the \gls{TS} algorithm is used on the \gls{FS-FAP}. The authors had to make some alterations to the algorithm to suit their needs as well as to make the algorithm more efficient in exploring in the \gls{FAP} solution space. The \gls{TS} algorithm uses the multistart \gls{TS} algorithm, which randomly starts on different initial solutions \cite{TabuMontemanniSmith}.

\glsreset{HMT}
The authors developed a technique called \gls{HMT}. \Gls{HMT} first monitors an underlying heuristic algorithm being used on the problem, in the case of the research presented the \gls{TS} algorithm was used\cite{TabuMontemanniSmith}. It then identifies characteristics that favourable solutions exhibit which is for instance frequencies that get assigned to transmitters which results in an overall lower interference value\cite{TabuMontemanniSmith}.

The \gls{HMT} then uses the identified characteristics to add \emph{additional} constraints to the problem\cite{TabuMontemanniSmith}. By adding constraints, the search space is reduced. However, by reducing the search space, other near-optimal solutions, which might be far better are excluded\cite{TabuMontemanniSmith}. It is for this reason that that Montemanni and Smith opted not to add the constraints permanently.

Montemanni and Smith applied their \gls{TS} algorithm together with \gls{HMT} to the \gls{COST} 259 family of benchmarks, specifically the Siemens1, Siemens2, Siemens3 and Siemens4 problems. The results are presented in table~\ref{TSCOST259}. The values presented are scalar and indicate the total interference of the frequency plan. The lower the interference value is, the better the frequency plan.
\begin{table}[H]
\centering
	\begin{tabular}{| c | c | c |}
		\hline
		Problem instance & \gls{TS} with HMT & Best COST 259 \\ \hline
		Siemens1 & 2.7692 & 2.20 \\ \hline
		Siemens2 & 14.9360 & 14.28 \\ \hline
		Siemens3 & 6.6496 & 5.19 \\ \hline
		Siemens4 & 110.9725 & 81.89 \\ \hline
	\end{tabular}
\caption{Results of applying \gls{TS} with HMT on COST 259}
\label{TSCOST259}
\end{table}
As can be observed from the results obtained by the authors, the \gls{TS} algorithm with HMT produces results that rank very favourably against other algorithms also applied to the \gls{COST} 259. 

Bouja et al. \cite{} presents a TS algorithm that is applied to the MI-FAP. In the algorithm the fitness function counts the number of violations. The algorithm restricts the neighborhood by only selecting $n$ cells that have large local constraint violations. $n$ is gradually increased by the algorithm as the search progresses. Once all the instances are selected the algorithm performs arc consistency on them. Arc consistency is best explained with a formal example. Consider a cell $c$ with neighbors given by the function $N(c)$. As frequencies get assigned to the neighbours of $c$ the assignement will cause some frequencies on $c$ to be blocked(violating constraints). If there are still free frequencies left to assign to $c$ after the maximum number of blocked frequencies has been reached, then $c$ can be removed from the selected instances.

In research by Montemanni et al.\ discuss a \gls{TS} algorithm which utilizes dynamic tabu\cite{MontiTS}. By using dynamic tabu the algorithm gradually reduces the length of the tabu list with each iteration. After a pre-defined number of iterations the algorithm applies cell reoptimisation. Cell reoptimisation fixes the assignement of frequencies to all cells except one. The frequencies for this one cell can then be changed and optimised to provide the lowest interference\cite{MontiTS}. The algorithm is applied to the COST 259 family of benchmarks and the results obtained is denoted by MontiTS.

Adjakpl\'{e} and Jaumard present a TS algorithm that utilizes block assignment\cite{AdJa97}. Block assignment refers to a small set of frequencies which are assigned to the cell as a whole and not to a TRX individually. This set or block of frequencies is generated to minimise interference, maximise variance and to not use forbidden channels or violate constraints with respect to the particular cell. Frequecies are then assigned to TRXs individually from this block of frequencies, instead of the entire spectrum. The algorithm presented by the authors use block assignment for the initial solution generation. Neighbourhood moves are performed by changing the block assigned to a cell with another available block. The neighbourhood is restricted by only allowing blocks for selection where the number of local constraint violations they'll impose on a cell is minimal. The algorithm was tested on real life instances provided by Bell Mobility.

Finally, in work done by Galinier and Hao\cite{TSCSP} a general TS algorithm is applied to various CSP problems with the FAP being one of them. The authors model the co-channel and adj-channel constraints into proprietry model readable by their algorithm. The algorithm performs moves by using the 1-exchange move strategy, where the nodes that are exhcanged orginated from different frequency plans\cite{TSCSP}.

When critically reasoning about the \gls{TS} algorithm with regard to applying it to the \gls{FAP}, the following disadvantages in theory can be identified:
\paragraph{Search based on a single solution}
--- The \gls{TS} algorithm at any moment in time only searches in the vicinity of \emph{one} current solution for possible neighbours that might be the current solution for the next iteration. \gls{FAP}s have huge search spaces due to their NP-Complete nature. Therefore, only searching for possible rewarding neighbours from only potential solutions seems to be terribly inefficient. A better strategy would be to use the notion of population-based algorithms and have multiple solutions from which more rewarding neighbours are searched.
\paragraph{Neighbourhood generation}
--- The \gls{TS} algorithm defines no set process for generating a neighbouring solution given a starting solution. Generating neighbours from a solution is a critical process in the \gls{TS} algorithm, for it is the only means by which the algorithm considers other solutions, i.e. it is the mechanism by which the algorithm searches. Generating a new neighbour can be as simple as changing only one value from the current solution or it can be very complex and incorporate other algorithms together with mathematics formulae. Regardless of the complexity of the neighbour generation that is used, care must be taken to ensure that the algorithm is able to produce a wide diversity of neighbours and is also able to intensify on the most optimal solution.
\paragraph{Tabu lifetime}
--- The \gls{TS} algorithm only operates on a single solution at a time and at most only considers one potential neighbour as its next possible current solution. Therefore a difficult choice needs to be made as to how long a solution stays tabu. In the \gls{FAP} a solution might be entered into the tabu list early in the search process of the algorithm. A large majority of the neighbours of this solution are vastly superior solutions compared with any of the current solutions produced by the algorithm. Due to the solution with these neighbours being in the tabu list, these neighbours will not be reconsidered until much later when the solution is removed from the list. A possible option to allow the \gls{TS} to reconsider the tabu solutions is to increase the aspiration criterion. Increasing the aspiration criterion does have its risks. A high aspiration criterion and the algorithm might be too eager to select just any solution even though it is tabu. A low aspiration criterion and the algorithm will be too strict in selecting a tabu solution. The following section discusses the simulated annealing algorithm.
\section{Simulated Annealing}
\label{sec:simulatedannealing}

\subsection{Introduction}
\label{sec:SAIntroduction}
\Gls{SA} is a metaheuristic search technique proposed in the 1980s by Kirkpatrick to solve combinatorial optimisation problems. The technique is based on a natural process, which is known in metallurgy as annealing \cite{SASingleMultiObj,TempCyclingSA}. Kirkpatrick was the first to use \gls{SA} to solve optimisation problems but Metropolis \emph{et al.}\ defined the basic algorithm structure in 1953 \cite{CurveFittingSA,VeryFastSAImageEnchancement}.

Annealing is the natural process of crystallisation when a solid is heated to a high temperature and then systematically cooled to a lower temperature to reach a crystallised form \cite{NewSAs,ConstantTempSA}. The crystallised form of the solid is known to be the global minimum of the solids internal energy state. 

When the solid is rapidly cooled from a high temperature, the molecules have no time to reach a thermodynamic equilibrium stage \cite{MobileRobotSA,ConstantTempSA}. Therefore the molecules of the solid have high energy and the resultant structure has no real crystalline form; thus the solid energy is at a local minimum\cite{CurveFittingSA,NewSAs,MobileRobotSA}. When the solid is slowly cooled in a controlled manner, the molecules are able to reach a thermal equilibrium at each temperature \cite{ChaosSA,CurveFittingSA,NewSAs}.
In the algorithm the energy state is the \emph{cost function} that needs to be minimised, and the molecules are the \emph{variables}, which represent the solutions, and thus their state needs to be optimised to reach the desired energy state.

The \gls{SA} algorithm is able to purposely move to a worse solution in the search space\cite{EcoEquilSA}. In the research when an algorithm moves to a worse solution it is classified as an \emph{uphill} move\cite{AIModernApproach}. Similarly, when a move is made by the algorithm, which results in a better solution the move is classified as a \emph{downhill} move\cite{AIModernApproach}.

The following equation is the standard probability function that is used to determine when an uphill move is performed by the algorithm. This function is known in the literature as the \emph{metropolis criterion}. 
\begin{equation}
\label{eq:saprobability}
	M_{AC} =
	\begin{cases}
	1, &\text{if $f(y) \leq f(x)$}\\
	e^{-\frac{\Delta E}{T_k}} , &\text{otherwise}\\
	\end{cases}
\end{equation}
The function $f$ is the objective function or a function that determines the state of a given position in solution space\cite{EcoEquilSA}. The parameter $T_k$ is the temperature of the algorithm at iteration $k$ \cite{EcoEquilSA}. Finally, $\Delta E$ is the change in ``energy'' between two solutions $x$ and $y$ \cite{EcoEquilSA}.

The main purpose of the \gls{SA} algorithm (like most optimisation algorithms) is to minimise or maximise the cost function \cite{SASingleMultiObj}. This cost function evaluates a solution desirability compared with other solutions in the immediate \emph{neighbourhood} of the algorithm's current position \cite{TheoPraticalSA}. 

The immediate neighbourhood of solutions is generated based on a heuristic implemented by the algorithm designer\cite{AIModernApproach}. This heuristic, as with the \gls{TS} algorithm, can be simple or complex.

A neighbouring solution is only selected as the new best state if its desirability ranks higher than the current solution.

The best state is not always selected; in some cases the algorithm is also able to move to solutions that are worse than the current solution. A worse solution is only selected based on a probability, which is controlled by the \emph{annealing temperature} of the algorithm \cite{TheoPraticalSA}. 

At a high annealing temperature the probability that the algorithm will select a bad solution is very good. As the annealing temperature decreases so does the probability that a bad solution will be selected \cite{CurveFittingSA}.  Uphill moves allow the algorithm to break out of local minima and can lead the algorithm down a different path, which may ultimately result in obtaining the global optimum \cite{SASingleMultiObj}. 

As with the \gls{TS} algorithm, the standard \gls{SA} algorithm does not define a set neighbourhood generation mechanism; instead it is up to the algorithm designer to implement a suitable generation mechanism that will allow the algorithm to adequately explore the search space\cite{VariousCoolingSA}. 
The following section presents a discussion on various characteristics of the \gls{SA} algorithm.
\subsection{Important Simulated Annealing Characteristics}
There are four characteristics of the \gls{SA} algorithm that make the algorithm unique. One of the most important is the cooling schedule. 

\subsubsection{Cooling Schedule}
The cooling schedule/annealing schedule is the most defining characteristic of the \gls{SA} algorithm. It is the procedure where the natural annealing process is mimicked. The temperature of the \gls{SA} algorithm is a control parameter that defines how much the algorithm moves around in the search space.

After each iteration, whether the algorithm has selected a new best solution or not, the temperature is reduced by a certain amount. This amount is determined by the \emph{cooling schedule}.

In general, when the \gls{SA} algorithm temperature has a very high value most solutions that are produced from the neighbourhood are accepted \cite{ClusterSA}. Thus the algorithm moves freely in the search space with little constraint. As the temperature decreases, the probability that the algorithm will select a bad or just any solution decreases\cite{ClusterSA}. When the temperature is very low, the \gls{SA} algorithm is similar to a greedy algorithm in the sense that it only accepts downhill movements\cite{ClusterSA}.

The cooling schedule provides the \gls{SA} algorithm with the critical ability to control the rate the algorithm transitions from the diversification phase (high temperature) to the intensification phase (low temperature)\cite{ClusterSA}. By controlling this rate, one is able to direct the algorithm to explore more early on to locate the more promising areas for possible solutions. These promising areas can then be used by the algorithm in its intensification phase to find more promising solutions.

In the literature there are three annealing schedules in common use, namely \emph{the logarithmic schedule}, the \emph{geometric schedule} and the \emph{Cauchy schedule}\cite{VeryFastSAImageEnchancement,SASingleMultiObj}. 
The standard and most commonly used schedule is known as the logarithmic schedule and is based on Boltzmann annealing \cite{VeryFastSAImageEnchancement}. The main disadvantage of this schedule is that it is slow due to its logarithmic nature \cite{VeryFastSAImageEnchancement}. It also requires moves to be generated from a Gaussian distribution for it to be able to reach the global minimum\cite{SASingleMultiObj}. The logarithmic annealing function has the following form:
\begin{equation}
\label{eq:logcooling}
	T_k = \frac{T_0}{ln(k)},\text{where k is the iteration value and } k \neq 0\\
\end{equation}
Where $T_k$ is the temperature at iteration $k$. The next section describes how $T_0$, which is the initial temperature, is determined.
The Cauchy schedule is faster than the logarithmic schedule. Similar to the logarithmic, this schedule also has a movement requirement. Moves must be generated from a Cauchy distribution for the algorithm to be able to reach the global minimum \cite{SASingleMultiObj,VeryFastSAImageEnchancement}. The Cauchy schedule is also referred to as fast annealing\cite{VeryFastSAImageEnchancement}. The schedule has the following form:
\begin{equation}
\label{eq:cauchycooling}
	T_k = \frac{T_0}{k} ~, k \neq 0
\end{equation}

Finally, the fastest annealing schedule is known as the geometric or exponential annealing schedule \cite{SASingleMultiObj}. By using the geometric schedule the \gls{SA} temperatures are rescaled which is called \emph{re-annealing}\cite{VeryFastSAImageEnchancement}.
The geometric schedule has the following form:
\begin{equation}
\label{eq:geocooling}
	T(k)=T_0e^{-C_k},\text{where C is a constant}
\end{equation}

Move generation which is used by the annealing schedules is discussed in more detail in the section that follows the initial temperature discussion.

\subsubsection{Initial Temperature}
The initial temperature is a very important parameter to define in the \gls{SA} algorithm, since it defines a point from which the cooling schedule will start\cite{VariousCoolingSA}. Therefore, depending on what the initial value of the temperature is, the final result that the algorithm will produce can be influenced\cite{SALongestCommon,AutoConfigSA}.

When the initial temperature is set to a very high value, the algorithm takes a long time to reach a result since the search space is being explored more\cite{SALongestCommon,VariousCoolingSA}. More exploration is favourable for \gls{SA} as it lets the algorithm be less susceptible to local minimum. 

If the initial temperature is set to a very low temperature, the algorithm might converge too quickly and thus produce a result, which may be the local minimum\cite{SALongestCommon,VariousCoolingSA,AutoConfigSA}.
The initial temperature together with the cooling factor allows the algorithm designer to define the time window for the algorithm to escape local minima, as well as the rate of convergence to an optimum solution\cite{SALongestCommon,VariousCoolingSA}.

A low initial temperature together with a low cooling factor makes the time window for the algorithm to leave a local optimum very small\cite{SALongestCommon}. With a high initial temperature and cooling factor value that is almost 1, the time window for the algorithm to leave the local optimum is much larger \cite{SALongestCommon}. 

When the algorithm is near a global optimum, a low initial temperature and low cooling factor will allow the algorithm to reach the optimum in a fewer amount of iterations in the search space\cite{SALongestCommon}. In contrast, if a high temperature and a very low cooling factor are used, the algorithm will take longer to reach the optimum even though it is near the global optimum\cite{SALongestCommon}.

Research by Suman and Kumar\cite{SASingleMultiObj} present equation~\ref{eq:sainittemp} which can be used to determine the best suited initial temperature after experimentation:
\begin{equation}
    \label{eq:sainittemp}
    T_0=-\frac{\delta f_0}{ln(\xi_0)}
\end{equation}
where $\delta f_0$ is the average increase in the objective function. $\xi_0$ is the number of accepted bad moves divided by the number of attempted bad moves\cite{SASingleMultiObj}.

\subsubsection{Move Generation}
Most of the research done on the \gls{SA} algorithm focuses on the annealing schedule and not so much on the move/solution/neighbourhood generation. Typically an initial solution is generated and then small changes are made to the solution to represent a new solution. The solution is said to be perturbed to the next solution.

Move generation is the phase where neighbouring solutions to the current solution are generated. It is the ideal section for an algorithm designer to embed domain-specific knowledge that will allow the algorithm to generate better possible solutions.

In research done by Tseung and Lin \cite{CurveFittingSA} an initial solution is not modified, but a move generation technique known as \emph{pattern} search is used. Pattern search has two forms of movement, namely the exploratory move and the pattern move. The exploratory move continually changes the certain variables of a solution \cite{CurveFittingSA}. This is done so that the technique can rapidly find and identify a ``downhill'' move. The pattern move uses the information gathered by the exploratory move to move towards the minimum of the function \cite{CurveFittingSA}.
\subsubsection{Algorithm Efficiency}
The algorithm is also efficient with regard to CPU cycles when compared with the genetic algorithm. \gls{SA} only has to evaluate a certain number of moves each iteration, instead of a whole population of individuals each iteration. The genetic algorithm is discussed in section~\ref{sec:geneticalgorithm}.

Unlike \gls{TS}, the basic \gls{SA} algorithm does not keep any memory and is therefore memory efficient, but in contrast suffers the risk that the solution may cycle. The more iterations spent at a temperature, the longer the algorithm spends at a certain temperature and therefore the higher the probability that solutions may cycle. In the next section the flow of the \gls{SA} algorithm is discussed and pseudo code for the \gls{SA} algorithm is presented.

\subsection{Flow of the Algorithm}
In an attempt to better understand how the \gls{SA} algorithm operates, a general discussion on the flow of the algorithm will now be given using algorithm~\ref{alg:SA} as a reference point.
\begin{algorithm}[H]
\caption{Basic Simulated Annealing Algorithm\cite{VeryFastSAImageEnchancement,ChaosSA}}
\label{alg:SA}
	\begin{algorithmic}[1]
		\State Initialize parameters
		\State Set starting temperature $T(0)$
    \State $\hat{x_0} \leftarrow$ Generate initial starting solution
		\While{Stopping criterion not met}
    \State $\hat{y_i} \leftarrow$ Generate neighbouring solutions to $\hat{x_i}$
    \State Evaluate $\hat{y_i}$ neighbours with fitness function $f(\hat{y_i})$
    \State Calculate probability $\hat{p_i}$ of $\hat{y_i}$ neighbours with equation~\ref{eq:saprobability}
    \State $\hat{x_i} \leftarrow$ Select $\hat{y_i}$ neighbour based on probability $p_i$
			\State Reduce temperature $T(i)$ based on cooling schedule
		\EndWhile
    \State Return best solution $\hat{x_i}$
	\end{algorithmic}
\end{algorithm}

From lines 1 -- 3, the \gls{SA} algorithm is initialised. The most important step here is setting the starting temperature for the annealing process to start. As mentioned in the introduction, the temperature of the annealing process plays a critical role in the potential solution selection process. After the algorithm has been initialised the search phase of the algorithm starts, which ranges from lines 4 -- 10. Like the \gls{TS} algorithm, the \gls{SA} algorithm starts the search phase by generating a number of neighbours to the current solution held by the algorithm as can be observed in line 5.

Before selecting a neighbour the algorithm first needs to evaluate the generated neighbours. It evaluates each neighbour by applying a fitness function $f(y_i)$ in order to determine its fitness.
Once the fitness of all the generated neighbours has been determined, the algorithm uses equation~\ref{eq:saprobability} to calculate the probability of selecting a particular neighbour for all the generated neighbours as well. The probability calculation can be observed to occur in line 7. The algorithm then selects the neighbour with the highest probability to be the current solution, as observed in line 9. 

Before the algorithm advances to the next iteration the temperature needs to be lowered. As discussed, the temperature is lowered according to a particular cooling schedule. In the algorithm the process of lowering the temperature occurs in line 10. 

This concludes the discussion on the flow of the SA algorithm. The section that follows discusses the theoretical implications if the \gls{SA} is applied to the \gls{FAP}.
\subsection{Simulated Annealing on the \gls{FAP}}
The \gls{SA} algorithm, as with the \gls{TS} algorithm, has achieved good results in other optimisation problems, as mentioned in section \ref{sec:SAIntroduction}. Due to its success on other NP-Complete optimisation problems, the \gls{SA} algorithm has also been applied to the \gls{FAP}.

Research conducted by Duque-Anton et al.\cite{SAChanAss} utilise the \gls{SA} on the FAP. The authors introduce the concept of a \emph{dummy} frequency. The frequency is used to indicate (partial) unsatisfied demand. Where demand is defined as the amount of transceivers installed at a cell to handle expected traffic. By swapping out a real frequency with the dummy frequency or vice versa the algorithm is able to increase of decrease violation of the traffic demand. The particular cell and frequency that is used for this swapping that occurs is chosen at random. The authors opted to use a custom cooling schedule. In their schedule the cooling rate is calculated based on the average cost taken over two L-loops at temperature $t1$ and $t2$ is less than the standard deviation of solutions at temperature $t1$. Where L-loop refers to the \gls{SA} algorithm inner loop where the temperature does not change.

A unconventional \gls{SA} algorithm  is presented by Zerovnik\cite{Zerovnik97}. The defining characteristic that sets this algorithm apart from conventional \gls{SA} algorithms is that the initial temperature $T$ is never changed. The algorithm selects a cell based on the number of constraints it has violated. Using this cell frequencies are assigned to it based on a probability. This probability is formulated as follows. For every new frequency $f$ the probability that is will be assigned to the cell is given by $e^{S_f/T}$, where $S_f$ represents the number of violated constraints if $f$ is assigned to the particular TRX at the cell.

Hellebrandt and Heller \cite{Hehe00} present an \gls{SA} algorithm which is applied to the COST 259 set of benchmark instances. In their algorithm, the initial solution is generated in such a manner to ensure it fulfills all hard constraints. The authors opted to set the initial temperature acceptance rate to between 0.8 and 0.9. Movement occurs through the 1-exchange movement strategy. The algorithm restricts the neighbourhood by only selecting potential moves which will not violate any constraints. The defining attribute of the algorithm presented by the authors is due to their \emph{one-cell optimisation} method that is applied at the end of each iteration. With this method a cell is randomly selected and by using a simple dynamic program the authors show the cell can be efficiently optimised. The optimisation occurs by allowing that for the particular cell all frequencies can be changed simultaneously\cite{Hehe00}. 

In literature by Beckmann and Killat\cite{BeKi99a} the \gls{SA} algorithm is applied to the \gls{FAP}. The resulting \gls{SA} algorithm was benchmarked on the \gls{COST} 259 Siemens benchmark instances. The results obtained by the authors are presented in table~\ref{tab:SA} in column ``(BK) \gls{SA}''. Values represent the total interference generated by the frequency assignment. 

Research presented by Mannino et al.\cite{MaOrRi02} discusses an \gls{SA} algorithm that utilizes \emph{interval graphs}. A graph $G(V,E)$ is an interval graph if the graph contains a ordering of vertices ${v_1,\dots,v_n}$ such that it can be shown that for any triple $(r,s,t)$ that $r < s < t$, if $v_1v_2 \in E$, then $v_1v_s \in E$. If such an ordering exists within the graph, then a maximum weighted stable set can be found by a dynamic programming algorithm in $O(|V|)$-time. The authors generalised the graph interval to define when a graph is k-thin. Namely a graph $G(V,E)$ is k-thin when one can define a ordering of vertices ${v_1,\dots,v_n}$ for $V$ and partition the ordering into $k$ classes $V^1,\dots,V^k$, such that for any triple $(r,s,t)$ that $r < s < t$, if $v_r,v_s \in E$ belong to the same $k$ class and $v_t,v_r \in E$ then $v_t,v_s \in E$. If a graph is found to satisfy these conditions defined by the authors then the graph is an interval graph if and only if it is 1-thin. When the ordering and partition is given from the beginning, then the authors are able to find a maximum stable set $S^*$ on a k-thin graph in $O(\frac{|V|}{k})^k$. Using this k-thin graph approach together with \gls{SA} algorithm allowed the authors to out perform all the other results presented in the \gls{COST} 259 benchmark. 

In table\ref{tab:SA} results are presented from research presented by two authors. As mentioned previously, the results in column ``(BK) \gls{SA}'' are from the research presented by Beckmann and Killat\cite{BeKi99a}. The results depicted in column ``Best \gls{COST} 259'' are from the research presented by Mannino et al.\cite{MaOrRi02}.
\begin{table}[H]
\centering
	\begin{tabular}{| c | c | c | c |}
	\hline
    Problem instance & (BK)\gls{SA} & Best \gls{COST} 259 \\ \hline
	Siemens1 & 2.78 & 2.20\\ \hline
	Siemens2 & 15.46 & 14.28\\ \hline
	Siemens3 & 6.75 & 5.19\\ \hline
	Siemens4 & 89.15 & 81.89\\ \hline
	\end{tabular}
\caption{SA on \gls{COST} 259 Benchmark}
\label{tab:SA}
\end{table}


An overview of the \gls{SA} algorithm along with its unique characteristics have now been given. Utilising the knowledge that was gained from understanding how the \gls{SA} algorithm operates a critical evaluation can be presented. In the following paragraphs, a theoretical critical evaluation is presented that lists the various characteristics, which would be problematic if the \gls{SA} algorithm would be applied to the \gls{FAP}.

\paragraph{Cooling Schedule}
--- Depending on the cooling schedule selected, the algorithm might converge too quickly. As discussed previously the cooling schedule reduces the temperature. The temperature plays a large part in the determination of whether a particular solution will be moved to or not in an iteration. Thus early on the algorithm will explore a lot more (diversification) and later on will exploit more (intensification). In the \gls{FAP}, the algorithm must not only be able to explore and exploit, but also be able to return to an exploration phase if need be.
As the temperature becomes colder the \gls{SA} algorithm exploits more and therefore will not easily move to a worse off solution. In the \gls{FAP}, it might be desirable to rather move a worse off solution later on, as the particular current solution is a local minimum and yields bad neighbours as potential next solutions. With the cooling schedule this is simply not possible, unless the temperature and schedule are reset. Resetting the temperature and schedule is not ideal, since the algorithm keeps no history and might risk making the same faults as before the reset.
\paragraph{Neighbourhood generation}
--- The \gls{SA} algorithm, as with the \gls{TS} algorithm, has no set process that defines how neighbours should be generated. As discussed, neighbourhood generation is the primary means by which the \gls{SA} algorithm moves about the search space in search of an optimal solution. Therefore applying the \gls{SA} algorithm would require a custom neighbourhood generation scheme. A desirable neighbourhood generation scheme would be one that keeps track of where the algorithm has been previously. By keeping history, the algorithm will be able to avoid previously explored areas in the search space.
\paragraph{Single solution based search}
--- The \gls{SA} algorithm is similar to the \gls{TS} algorithm in the sense that it only searches from one solution per iteration. It searches by generating neighbours around the current solution of the algorithm. The \gls{FAP} search space is huge; hence it would be more efficient to have multiple current solutions from which neighbours are generated. This enables the algorithm to explore the search space much more efficiently at the expense of more computational resources.

\section{Genetic Algorithm}
\label{sec:geneticalgorithm}
\subsection{Introduction}
The genetic algorithm (GA) is a stochastic search method that is based on the natural process of genetic evolution and the Darwinian concept of ``survival of the fittest'' \cite{DistributedHierarchicalGA,AcceleratingGA,AdaptiveSAGA,FamilyGA}. The \gls{GA} was first proposed by Fraser, but it was not till the research presented by Holland that \gls{GA}'s became popular\cite{CompuIntelligenceIntro}. Holland initially applied the algorithm to adaptive systems but the algorithm has been widely used in optimisation due to its success on multidimensional problems\cite{ParallelGASA,DistributedHierarchicalGA,FamilyGA}. The \gls{GA} developed by Holland is referred to in the literature as the Canonical \gls{GA} (CGA)\cite{CompuIntelligenceIntro}.

The wide use of the \gls{GA} can also be attributed to its generic algorithm structure as well as the ease of implementation \cite{FamilyGA,AdaptiveSAGA}. The \gls{GA} consists of three main operations. An initial population is created upon which the GA operates. Using a selection method (dicussed in section\ref{sec:}) parents are chosen. The \gls{GA} uses genetic operators like crossover and mutation (both discussed in section\ref{sec:}) on the selected parents to create offspring\cite{GAGoldberg}. It should be noted that initially the mutation operator was not required to be part of the \gls{GA}. Only after successive implementations of the \gls{GA} did it showcase the explorative power that the mutation operator brings to the search capability of the \gls{GA}. Hence, the importance of the mutation operator was recognised\cite{CompuIntelligenceIntro}. 


The \gls{GA} search procedure involves searching the solution space through artificial evolution and natural selection\cite{FamilyGA,MultiPopGA,HybridIntelliGA}. An individual or point in the search space is known as a \emph{chromosome} \cite{HumanPassiveGA}. An initial set of chromosomes (referred to in the research as the \emph{population}) is randomly generated to form the initial population\cite{FamilyGA,HybridIntelliGA,AcceleratingGA,MultiPopGA}. 

A chromosome consists out of a sequence of smaller parts called \emph{genes}\cite{CompuIntelligenceIntro}. The sequence upon which these genes appear in the chromosome determines the characteristics of an individual\cite{CompuIntelligenceIntro}. In the \gls{GA} a single gene represents a single variable of a candidate solution. Depending on the problem domain a variable can represent a different characteristic, for instance in the Traveling Salesman Problem  variable that forms part of a solution could be a node on a route\cite{FamilyGA,AcceleratingGA}. A whole chromosome therefore represents a candidate solution\cite{FamilyGA,AcceleratingGA}. Exactly how best to represent a chromosome as a solution is problem dependent\cite{CompuIntelligenceIntro}.

According to the evolution theory proposed by Darwin individuals of a population with the best genes have the best chance to survive and to reproduce\cite{CompuIntelligenceIntro}. These individuals are referred to as the fittest individuals of the population. In a \gls{GA} population each individual is ``rated'' to determine how good the solution its genes represents \cite{CompuIntelligenceIntro}. The rating of an individual is referred to as its fitness value and is also problem dependent\cite{CompuIntelligenceIntro}.

Determining the fitness value of a chromosome is achieved by means of a \emph{fitness function}. The fitness function is a mathematical function, which maps the chromosome representation to a scalar value\cite{CompuIntelligenceIntro}. An optimisation problem has an objective function, which calculates how good a candidate solution is, therefore in the \gls{GA} the fitness function represents the objective function\cite{CompuIntelligenceIntro}.

When the fitness of individuals in a population has been determined the \gls{GA} probabilistically selects individuals for the next generation\cite{CompuIntelligenceIntro}. The selection probability is referred to as the \emph{selective pressure}\cite{CompuIntelligenceIntro}. Individuals are selected using a selection method (discussed in section\ref{sec:}) for the reproduction phase of the \gls{GA} where offspring are created\cite{CompuIntelligenceIntro}.

Offspring are created by combining parts of one or more parent chromosomes to form a new chromosome. This procedure is called the \emph{crossover}\cite{CompuIntelligenceIntro}. The chromosomes used in the production of the offspring are referred to as the parent chromosomes\cite{CompuIntelligenceIntro}.

With regard to how offspring and parents are handled, there are two forms of the \gls{GA} \cite{FamilyGA}. One form is called the \emph{generational} \gls{GA}  and the other form is known as the \emph{steady-state} \gls{GA} \cite{GeostatisticalGA,FamilyGA}.

With the generational \gls{GA} offspring is created from the current generations parents. The newly generated offspring replaces the old population and becomes the new population for the next generation\cite{FamilyGA, SpringerIntroToGAs, IntroToGAs}. In the steady-state \gls{GA} only a certian portion of the population is replaced each generation. Depending on the selection pressure the least fit individuals are chosen for replacement\cite{GeostatisticalGA,FamilyGA, SpringerIntroToGAs, IntroToGAs}. Steady-state \gls{GA} is more favourable for systems where incremental learning is key and knowledge needs to be retained more agressively\cite{SpringerIntroToGAs, IntroToGAs}. Due to individuals not being replaced at every generational step the knowledged of each individual ``lives'' longer\cite{SpringerIntroToGAs, IntroToGAs}.

The \gls{GA} search process moves around in the search space using genetic recombination operators i.e., crossover and mutation which are applied probabilistically instead of deterministically\cite{FamilyGA}. The operators aid the algorithm to avoid local optima regions in the search space \cite{HybridIntelliGA}. Note that by using the operators there is no guarantee that the \gls{GA} will completely avoid local optima\cite{CompuIntelligenceIntro}.

The \gls{GA} makes no assumptions about the search space and primarily works on the information provided by the chromosomes and the representation of solutions used\cite{CompuIntelligenceIntro,ConstrainedGA,HybridIntelliGA}. Unlike the \gls{SA} and \gls{TS} algorithms, the \gls{GA} if properly initialized, starts with a number of search points that cover a wide area of the search space. Depending on the operators used, diversity can be quickly lost and the population can become homogeneous very quickly\cite{DistributedHierarchicalGA,FamilyGA,HybridIntelliGA}\label{GASearchPoints}.
 
The different operators used by the GA along with their probabilities are not specific. Each operator and probability can be changed to suit the problem domain the GA is being applied to. Therefore the particular probability and operators that the GA uses is problem dependent.

As discussed, the algorithm consists of two key parts. The first part which is the selection method that is used to select individuals upon which the operators function and the second part which is where the operators get applied. In the next section, the selection method, crossover and mutation operators are each individually disccused.

\subsection{Important Genetic Algorithm Characteristics}
The various operators used by the \gls{GA} makes the procedure unique and is one of \gls{GA}'s defining characteristics. One of the \gls{GA}'s defining characteristic is discussed below, namely the selection operator.

\subsubsection{Selection Method}
The selection method is applied at first chance to the population after each generation. The method determines which individuals will be used to for the next generation and if need be create offspring \cite{CoactiveFuzzyGA,CombinedBranchBoundGA,ConstrainedGA}.

Individuals that are selected by the selection method are used by the crossover and mutation operators (in the reproduction phase), to generate offspring which will form the next popultion\cite{AdaptiveSAGA,AcceleratingGA}.

By favouring high fitness individuals above low fitness individuals the selection method is said to have a high selective pressure\cite{CompuIntelligenceIntro}. Care must be taken if the method uses a high selective pressure. Since with hight selective pressure high fitness individuals are preferred, the algorithm effectively explores less and diversity among the individuals in the population will deteriorate and thus result in premature convergence\cite{ConstrainedGA, CompuIntelligenceIntro}.

There are a variety of selection methods available for use by the \gls{GA}. Each with their respective strengths in certain problem domains. The following list is only a brief summary of the methods available.
\begin{itemize}
    \item{Fitness-Proportionate Selection} --- For each individual a calculation is made to determine the amount of times the individual will be used to reproduce. This ``value'' of an individual is determined by taking the individuals fitness and dividing it by the average fitness of the population. Roulette wheel is a commonly used in this selection method. Each individual is assigned a section of the wheel. The size of the section is proportionate to the calculated ``value''. At each generation, the wheel is spun $N$ times where $N$ is the size of the population. The individual under the wheels marker after each spin is selected to be in the pool parents for the next generation\cite{IntroToGAs}.
    \item{Sigma Scaling} --- The selection pressure in sigma scaling is aimed to be more constant over a full algorithm run and is based on a individuals expected value. High expected values means that the individual will be used more in the production of offspring. The expected value of a individual is a function of its fitness, the population mean, and the populations standard deviation\cite{IntroToGAs}.
    \item{Elitism} --- Selects a few of the best individuals to be retained each generation, which would ordinairly be lost through crossovers and/or mutation.
    \item{Boltzmann Selection} --- Similar to simulated annealing Boltzmann selection is based on a temperature that gets gradually changed. A High temperature means there is a low selection temperature and therefore all individuals a reasonable probability of reproducing. As the temperature gets gradually lowered according to schedule the selection pressure increases. As the selection pressure increases, the less likely it becomes that less fit individuals will be selected to produce offspring\cite{IntroToGAs}.
    \item{Rank Selection} --- Individuals are ranked based on their fitness. Individuals are then selected based on their rank in the population rather than their fitness.  Ranked based selection reduces dominance of high fit individuals\cite{IntroToGAs}.
    \item{Tournament selection} --- Two individuals are randomly chosen from the population. The fittest individual between the two is selected with a defined probability to be a parent, if not the less individual is chosen\cite{IntroToGAs}.
    \item{Steady-State selection} --- As discussed in the previous section, only a certain portion of the population is replaced each iteration.
\end{itemize}

With respect to numerical optimisation problems, some problems might be classified as \emph{unimodal} or \emph{multimodal}\cite{FirstMathModel, CompuIntelligenceIntro}. The mathematical analysis on the search spaces of these numerical optimisation problems, the amount of optimums a particular problem contains can be determined\cite{FirstMathModel, CompuIntelligenceIntro}. When a numerical problem is known to have only one global optimum, the problem is referred to as being unimodal\cite{FirstMathModel, CompuIntelligenceIntro}. Problems with more than one global optimum are known as multimodal problems\cite{FirstMathModel, CompuIntelligenceIntro}.

With regard to unimodal problems and \gls{GA}, a high selection pressure is benificial\cite{ConstrainedGA}. With high selection pressure the selection method directs the search into a gradient-based direction that converges on a single optimum solution \cite{ConstrainedGA}. The high selection pressure forces the algorithm to explore less and exploit more, therefore converging in less generations to a single optimum. When the \gls{GA} is applied to multimodal problems a low selection pressure is more beneficial to the \gls{GA}\cite{ConstrainedGA}. Low selection pressure allows the population of the \gls{GA} to explore the search space vigorously\cite{ConstrainedGA}.

Depending on the complexity of the objective function and the size of the population, the selection phase may be the most computationally expensive as well as time consuming phase\cite{AcceleratingGA}. 

These replacement policies define which individuals of the current population should be replaced by the newly generated offspring\cite{CompuIntelligenceIntro}. For instance, the policy can define that the entire current population should be replaced by the offspring\cite{CompuIntelligenceIntro}. Using this policy is not ideal as some generated offspring could have a much worse fitness than their parents but due to the replacement the solutions represented by the good parents are lost\cite{CompuIntelligenceIntro}. 

Other replacement policies include replacing the worst individual in the current population with offspring on the premise that the offspring has a better fitness\cite{CompuIntelligenceIntro}. By replacing older chromosomes with better performing chromosomes in the population replacement strategy, the \gls{GA} achieves a hill-climbing ability. The reader that is interested in more information on different selection operators is directed to the survey by Engelbrecht\cite{CompuIntelligenceIntro}.
\subsubsection{Crossover Operator}
\label{sec:crossover}
The crossover operator is the first operator applied to the population in the reproduction phase. The crossover operates exclusively on the chromosomes in the mating pool. Crossover works by interchanging genes from two or more parent chromosomes. The parent chromosomes are selected from the current population using one of the selection methods discussed previously to form the mating pool from which offspring is produced\cite{FamilyGA,HumanPassiveGA,CoactiveFuzzyGA}. 

Offspring is formed by applying the crossover operator with a defined probability to two or more parents. The probability is known as the \emph{crossover probability} and is problem dependent\cite{CompuIntelligenceIntro}. By defining a high crossover probability that decreases over time, the good genes that form the current population are retained in the next population\cite{CompuIntelligenceIntro}. With good genes being retained more historical information is passed onto the next generation's population\cite{FamilyGA}.

There are a variety of ways in which genes are interchanged between chromosomes in the crossover operation i.e. fixed point crossover, two point crossover, uniform crossover and Gaussian crossover\cite{CompuIntelligenceIntro}. All of these crossovers operate on the premise that a byte representation is used for the chromosomes. 

With the byte representation, each chromosome is a byte. Each byte is made up of sequence of bits. As discussed previously each chromosome is made out of genes and therefore each bit is a gene\cite{CompuIntelligenceIntro}.

Fixed-point crossover operates on binary parents whereby a point is selected in one parent and then all other bits are replaced by the other parents' bits \cite{HumanPassiveGA}. Two-point crossover generates two random indices, which dictate a certain segment in the one parent to be interchanged with the other parent \cite{ConstrainedGA}. Where a segment consists of a subset of genes from the chromosome. 

The uniform crossover is the most basic of all crossovers since it randomly selects bits from one parent to be replaced by another parent's bits\cite{ParallelGASA,GeostatisticalGA}. Finally, the Gaussian crossover also uses the byte representation. The Gaussian crossover interchanges bits between parents based on a Gaussian distribution \cite{ParallelGASA,GeostatisticalGA}.

Caution should be exercised by the crossover operator when selecting chromosomes for reproduction. It is possible for the operator to select the same chromosome twice to be the parent of a single offspring\cite{CompuIntelligenceIntro}. The operator is also at risk of selecting the same chromosome multiple times for reproduction\cite{CompuIntelligenceIntro}. The operator must therefore incorporate a test to detect unnecessary repeated usage of a chromosome\cite{CompuIntelligenceIntro}. Finally, it is important to note that there exist strategies for both Gaussian crossover and mutation based operators for use with continuous-valued representations\cite{FundamentalSwarm}.

\subsubsection{Mutation Operator}
The mutation operator is a probabilistic operator and is applied to individuals in the offspring population with a probability referred to as the mutation rate\cite{CompuIntelligenceIntro}. The purpose of the mutation operator is to increase the diversity of the genes of an individual's chromosome\cite{CompuIntelligenceIntro}. By introducing new genes into an individual the diversity of the populations characteristics are increased\cite{CoactiveFuzzyGA,AcceleratingGA,ConstrainedGA}.

The mutation operator has no previous information on the chromosome it is mutating; thus it is entirely possible that the mutation may modify the chromosome for the worse \cite{AcceleratingGA}. A worse solution might lead the algorithm out of local optima or lead it down a new path to find the global optima, but this is not always the case \cite{AdaptiveSAGA,FamilyGA,ConstrainedGA}. It is for this reason that it is recommended that the mutation rate be set to a low value to ensure good solutions are not distorted too much\cite{CompuIntelligenceIntro}.

In a survey done by Engelbrecht\cite{CompuIntelligenceIntro} another mutation operator is discussed. Instead of mutating a small part of randomly selected chromosomes, this operator generates new offspring to be inserted back into the population. The operator randomly generates a new chromosome and then uses any of the previously discussed crossover operators (see page~\pageref{sec:crossover}).

The mutation operator is not always a basic random replacement of genes operation. In research done by Jeong and Lee\cite{AdaptiveSAGA}, a mutation operator is presented that uses the \gls{SA} algorithm to determine the genes that need to be replaced. The \gls{SA} mutation operator generates a new chromosome from which genes are used to replace in the chromosome being mutated \cite{AdaptiveSAGA}. Addition mutation operators are discussed in Engelbrecht\cite{CompuIntelligenceIntro}.
\subsubsection{Initial Population Generation}
Initial population generation is the very first activity that the \gls{GA} performs. Out of this population potential mating candidates are selected based on their fitness, which indicates desirability. The initial population is generated by means of randomisation \cite{SelfAdaptiveGA}. Since the algorithm searches multiple points simultaneously in the search space, it is desirable that the initial population have a wide diversity with regard to the problem search space\cite{CombinedBranchBoundGA,DistributedHierarchicalGA}. By controlling the initial population generation the amount of exploration the algorithm does initially can be controlled to a small degree\cite{CombinedBranchBoundGA}. Therefore care must be taken in the selection of the particular randomisation scheme that will be used to generate chromosomes for individuals.

In research by E. Cantu-Paz, the \gls{MT} random number generator (considered to be the best pseudo random number generator) is compared against a severly limited \gls{MT} random number generator\cite{OnRandomGA}. The second \gls{MT} generator was limited by the authors through reseeding the generator every 1000 calls with the original seed, which leads to the numbers being generated being a lot less random and repeating after every thousand calls\cite{OnRandomGA, RLRandomGA}. By comparing these two random number generators with respect to the performance of the \gls{GA}, the authors came to the following conclusion that the initial population generation is the most sensitive to the quality of the random number generator used\cite{OnRandomGA}. Other parts of the \gls{GA} like crossover and mutation were found to not be impacted by the quality of the random number generator used\cite{OnRandomGA, RLRandomGA}.

\subsubsection{Algorithm Efficiency}
The \gls{GA} is a powerful, yet simple algorithm and tends to find good solutions given enough time, it does have its disadvantages. One of the major disadvantages occurs when the \gls{GA} is applied to problems that have very large solution spaces. In these problems, the population size is a very sensitive parameter\cite{AdaptiveSAGA,HetergeneousGA,SelfAdaptiveDataMiningGA,PatternDetectionGA}. If the population is too small the algorithm will not have enough diversity to search and will tend to converge prematurely. 

A large population is preferred in large search spaces in order to get good chromosome diversity among individuals. Hence, the population size must be fine-tuned to achieve optimal performance in large search spaces \cite{AdaptiveSAGA,CompuIntelligenceIntro}. Note, an increase in population does not guarantee good chromosome diversity among the population. As discussed in initial population generation it is also dependant on the random generation scheme used as well. 


The following section presents the pseudo code for the \gls{GA} algorithm and also the flow of the \gls{GA} algorithm is discussed
\subsection{Flow of the Algorithm}
The core concepts of the genetic algorithm were introduced in the previous section. To better understand the algorithm, a general overview of the algorithm will be presented in this section using algorithm~\ref{alg:GA} as a reference point.
\begin{algorithm}[H]
\caption{Basic Genetic Algorithm Algorithm\cite{FamilyGA,AdaptiveSAGA,DistributedHierarchicalGA,SelfAdaptiveGA}}
\label{alg:GA}
	\begin{algorithmic}[1]
		\State $pop_n\leftarrow$Initialize population
		\While{Stopping criteria is not met}
    \State Evaluate individuals of population with fitness function $f(\hat{x_i})$
    \State $\hat{y_k} \leftarrow$ Select parent individuals from population using selection operator
		\Repeat
    \For{Each chromosome $\hat{g_i}$ in $\hat{y_{k-1}}$}
    \State $\hat{c_i} \leftarrow$ calculate crossover probability for $\hat{g_i}$
    \If{$\hat{c_i} \geq$ Crossover threshold}
    \State $\hat{g_i} \leftarrow$ Apply crossover operator to $\hat{g_i}$
				\EndIf
        \State $\hat{o_i} \leftarrow \hat{g_i}$
        \State $\hat{m_i}\leftarrow$ Calculate Mutation probability for $\hat{o_i}$
        \If{$\hat{m_i} \geq$ Mutation threshold}
        \State Apply mutation operator to $\hat{o_i}$
				\EndIf
        \State Add offspring chromosome $\hat{o_i}$ to $new_{pop}$
			\EndFor
		\Until{$size(new_{pop}) = size(pop_n)$}
		\State $pop_n \leftarrow$ select new population from $pop_n$ and $new_{pop}$
		\EndWhile
    \State $\hat{x_i} \leftarrow$ Determine best chromosome in $pop_n$
    \State Return best solution $\hat{x_i}$
	\end{algorithmic}
\end{algorithm}
The \gls{GA} algorithm is a population-based algorithm and therefore needs to initialise its population. Each individual of the population represents a potential solution. Population initialisation occurs in line 1 of algorithm~\ref{alg:GA} on page~\pageref{alg:GA}. 

The amount of individuals in a population to be generated is predefined and is known as the population size. In algorithm~\ref{alg:GA} on page~\pageref{alg:GA} the population size is represented by the subscript $n$ in $pop_n$, where $n > 0$.

Before the algorithm can start \emph{evolving} its population, it first needs to determine each individual in the population's fitness. The fitness of an individual is calculated using a fitness function $f(\hat{x_i})$. After each individual within the initial population has been evaluated, the algorithm is able to start its searching process, which starts at line 3 and ends at line 17 of algorithm~\ref{alg:GA} on page~\pageref{alg:GA}.

Since each individual has a fitness value after being evaluated, the selection operator is applied. The selection operator used on line 4 determines which individuals will form part of the parents used to generate offspring for the next population. Once the selection operator has selected the parent individuals needed for the next population, the algorithm is ready to enter the reproduction phase, which ranges from lines 5 -- 17 of algorithm~\ref{alg:GA} on page~\pageref{alg:GA}.

In the reproduction phase the crossover and mutation operators are applied probabilistically. For the crossover operator a crossover probability is calculated on line 7. Depending if the calculated probability satisfies the crossover probability the crossover operator is applied on line 9. Depending on the crossover used, offspring are generated using one or more individuals as parents. The resulting offspring from the crossover operation is assigned to $\hat{g_i}$.

On line 11 the value of $\hat{g_i}$ is assigned to the variable $\hat{o_i}$, which represents the offspring. From lines 7 -- 11 it can be observed that a particular individual does not have to take part in a crossover operation to be carried over to the new population. Thus knowledge gained from the current population is persisted based on the crossover probability.

The second step of the reproduction phase is where the mutation operator is applied. For each of the offspring a mutation probability is calculated. If the calculated probability for a particular offspring is high enough, the algorithm enters the mutation phase. In the mutation phase an individual is selected and the mutation operator is applied. Application of the mutation operator can be seen to occur in lines 13 -- 15 of algorithm~\ref{alg:GA} on page~\pageref{alg:GA}.

Regardless of whether the offspring has been mutated or not, the resulting offspring are added to the new population. The reproduction phase continually loops, until the new population equals the size of the initial starting population. Once the amount of individuals in the new population has reached the population size $n$, the algorithm moves on to its next iteration.

After the algorithm has completed the reproduction phase the algorithm selects the new population to be used in the next generation. The new population is selected from the current population and the created offspring.

The algorithm continually generates a new population for each generation until a predetermined stopping criterion has been met. Once the criterion has been met, the algorithm selects the individual with the best fitness in the current population as its most optimal solution. The following section discusses literature where the \gls{GA} has been applied to the \gls{FAP}.

\subsection{Genetic Algorithm on the \gls{FAP}}
Continuing the trend of the \gls{SA} and \gls{TS} algorithms, the \gls{GA} has also been applied to a wide variety of problems. These problems include: solving nonconvex nonlinear programming problems\cite{GANonConvex}, data mining \cite{SelfAdaptiveDataMiningGA} and auto configuring metaheuristic algorithms for complex combinatorial problems \cite{AutoComplexMeta}.

In research by Cuppini\cite{}a \gls{GA}is presented where the fitness function is the sum of two terms. The first term is the global interference and the second the amount of different frequencies used in the candidate solution. Within each chromosome $C$, a gene represents a subset of tranceivers which are assigned a particular frequency $f$, formally stated $C = {f_1,f_2, ..., f_{max}}$ and $f_n = {T_1, T_2, ... T_k}$ where $n$ denotes the frequency and $T_k$ denotes the particular tranceiver assigned. To generate a new generation the algorithm uses asexual crossover which operaters on a single chromosome. The algorithm performs asexual crossover by choosing two genes. For each chosen gene two crossover points are selected. The selected points are the same for both genes. By breaking the genes at the crossover points new genes are formed. The genes are formed by taking the first part of the one gene and completing it with the second part of the second gene and vice versa. The chromosome used for crossover is selected with a probability that is proportional to its fitness value.

Ngo and Li\cite{} is the same chromosome representation as Cuppini\cite{}. In their algorithm, two point crossover is used, where two genes would be selected. Due to each gene containing a group of transceivers the crossover only swaps out a subset from each gene between the two genes. The algorithm also applies mutation to the generated offspring by randomly selecting a single trx and assigning it a different frequency. The algorithm aims to diversify its search by randomly applying a local search procedure the current best solution. The procedure inspects the solution and searches for the trxs whose assigned frequencies caused the most interference. Once these trxs have been located they are randomly assigned new frequencies.

A different chromosome representation scheme is used by Crisan and Mhlenbein\cite{}. The authors represent each chromosone as a $n$-dimensional vector $v$ where each element in the vector represents a trx and $n$ the amount of trxs that need to be assigned frequencies. A frequency $f_i \in [f_{min}, f_{max}]$ is assigned to a element (trx) $v_j, j \le n$ where $[f_{min}, f_{max}]$ represents the frequency spectrum. In their application of the \gls{GA} on the FAP the mutation operator randomly randomly chooses a vector element $v_j$ and assigns a new frequecy $f_i$ which originates from a candidate set and adheres to co-cell constraints. Crossover in their algorithm consists of first selecting a cell $c$ where $c = {v_1,v_2,\dots,v_n}$ thus a cell is a subset of v. The selected cell has no local constraint violations. The algorithm then determines all the cells that are involved with interference with this cell. All the involved cells are then moved as is into the the new offspring chromosome. The rest of the cells are then taken from the other parent and moved to the offspring chromosome to complete the crossover.

Jaimes-Romero et al.\cite{} uses a two phase genetic algorithm approach to find candidate solutions for the \gls{FAP}. During the first phase the main concern of the algorithm is to find a candidate solution where the probability that calls will be blocked is 0. Once the algorithm has found such a candidate solution it enters the second phase. In the second phase the algorithm tries to minimise the interference. 

Colombo and Allen\cite{ProblemDecompMIFAP} have developed a \gls{GA} to be applied to \gls{FAP}. The authors decomposed the \gls{FAP} into smaller sub problems. On average the solution quality is improved by using the technique but at the expense of more complex and taxing evaluations that have to be performed\cite{ProblemDecompMIFAP}. 

In table~\ref{tab:GA} the results obtained by authors on the \gls{COST} 259 benchmarks are compared. The listed values are scalars representing the total interference generated by the frequency plan.
\begin{table}[H]
\centering
	\begin{tabular}{| c | c | c |}
	\hline
	Problem instance & \gls{GA} & Best \gls{COST} 259 \\ \hline
	Siemens1 & 2.60 & 2.200 \\ \hline
	Siemens2 & 16.34 & 14.280 \\ \hline
	Siemens3 & 6.37 & 5.19 \\ \hline
	Siemens4 & 84.08 & 81.89 \\ \hline
	\end{tabular}
\caption{GA on \gls{COST} 259 Benchmark}
\label{tab:GA}
\end{table}

As per the results in table~\ref{tab:GA} the \gls{GA} produces good results coming close to the best-obtained results in the benchmark. By critically reasoning about the \gls{GA} if it were applied to the \gls{FAP} in theory, the following disadvantages can be identified:

\paragraph{Diversity}
--- The \gls{GA} continually operates on a set population that is randomly initialised at the beginning of the algorithm. Each individual in the population is a chromosome which is composed of genes. It therefore only has this set of generated genes per individual in the initialised population to evolve successive populations.
If one disregards mutation, the \gls{GA} is a process by which the optimal combinations of the starting genes are found. Thus, the \gls{GA} purifies the starting population genes in an attempt to find those individual genes, which if combined into a single individual, will produce an optimal individual, i.e. solution. Therefore the \gls{GA} is very reliant on the quality of the random generator used. The probability of the algorithm finding a particular desirable gene that is exhibited by the starting population is directly related to the random number generator used to initialise the population. 
\paragraph{Crossover}
--- The crossover operation in the \gls{GA} is the only means by which successive populations are generated and can therefore be regarded as the primary means by which the algorithm performs its search. As the crossover is defined in the standard algorithm, certain parts of both parents are copied and combined to form a new individual. With regard to the \gls{FAP}, if each individual represents a frequency plan, the crossover operation would copy certain cells from the two parent plans. This is not desirable, since a single channel within a cell can generate major interference, which overshadows the rest of the channels that generate low interference in the cell. Thus, for the \gls{GA} to generate high quality solutions on the \gls{FAP}, the algorithm would be better off utilising a crossover operation which works on individual channels assigned rather than cells. Crossover operation is also a memory and computationally expensive operation since individuals need to be constantly created and values need to be copied to these new individuals from the respective parents.
\paragraph{Mutation}
--- Mutation is a means by which more diversity is introduced into the chromosomes of the individuals.  As discussed, the mutation operator introduces new genes to existing chromosome that can lead to an excellent solution being distorted and becoming one of the worst solutions. With regard to the \gls{FAP}, the low probability of mutation is not desirable, as the \gls{FAP} search space is huge and therefore requires constant diversity to be introduced to accurately explore it. A possible good mutation would be one that is slightly more intelligent than the standard mutation operator, which just randomly modifies a selected individual. An intelligent mutation would be one that takes into account the recent history of the individual as well as the history of the population and, based on the collective knowledge alters or \emph{mutates} a particular individual. Each chromosome would therefore be required to keep history of changes made to it, to allow the mutation operator to take it into account. Another means of applying the mutation operator is to set the mutation rate proportional to an individual's fitness.
\section {Summary}
In this chapter a description was given of metaheuristic algorithms. What it means for an algorithm to be classified as being of a metaheuristic nature was explained as well as the characteristics these algorithms exhibit was identified.

Three metaheuristic algorithms were discussed in this chapter. For every algorithm discussed an explanation was given of how the algorithm works as well as the various characteristics that make the algorithm unique.

For each algorithm, a brief overview of studies using the particular algorithm was given as well as some of the disadvantages or challenges that would be faced when applying the particular algorithm to the \gls{FAP}.
The first algorithm discussed was the tabu search algorithm and the second was the simulated annealing algorithm. The chapter concluded with the genetic algorithm. 
In table~\ref{tab:summary1}, the algorithms discussed in this chapter and their performance on the \gls{COST} 259 set of benchmarks have been summarised.
The next chapter presents a discussion on a class of algorithm that is new to optimisation research, namely swarm intelligence.
\begin{table}[H]
\label{tbl:summaryMetaTable}
\begin{center}
	\begin{tabular}{| c | c | c | c | c |}
	\hline
	Problem instance & \gls{TS} & \gls{SA} & \gls{GA} & Best \gls{COST} 259 \\ \hline
	Siemens1 & 2.7692 & 2.78 & 2.60 & 2.20 \\ \hline
	Siemens2 & 14.9360 & 15.46 & 16.34 & 14.28 \\ \hline
	Siemens3 & 6.6496 & 6.75 & 6.37 & 5.19 \\ \hline
	Siemens4 & 110.9725 & 89.15 & 84.08 & 81.89 \\ \hline
	\end{tabular}
\caption{Summary of algorithm performance on the \gls{COST} 259 benchmark}
\label{tab:summary1}
\end{center}
\end{table}

