<tex2html_file>#./chpt4.tex#
\chapter<<2418>>Metaheuristic Algorithms<<2418>>
\label<<2419>>chpt:heuristic<<2419>>
\section<<2420>>Introduction<<2420>>
Metaheuristics is a subdomain of the \gls<<2421>>AI<<2421>> domain\cite<<2422>>AIModernApproach<<2422>>. It evolved out of a need for more efficient search techniques with regard to hard problems. 
\begin<<3296>>tex2html_deferred<<3296>>\par\end<<3297>>tex2html_deferred<<3297>>
Metaheuristics forms part of a collective body of algorithms that use heuristics to search a particular domain's search space for the most optimal solution\cite<<2423>>AIModernApproach,NatureInspiredMetaHeuristic<<2423>>. Some problems explicitly define constraints to which the produced solution must adhere to. Problems that define such constraints are referred to as constraint optimisation problems\cite<<2424>>FundamentalSwarm<<2424>>. Problems where there are either no constraints or just a boundary constraint  defined are referred to as unconstraint problems\cite<<2425>>FundamentalSwarm<<2425>>.
\begin<<3298>>tex2html_deferred<<3298>>\par\end<<3299>>tex2html_deferred<<3299>>
Constraints can be either hard or soft. A hard constraint is defined as a certain condition an algorithm or potential solution is not allowed to violate\cite<<2426>>AIModernApproach,NatureInspiredMetaHeuristic,Karen2004,Eisenblatter<<2426>>. A soft constraint is allowed to be violated but there is some sort of penalty or cost involved which is imposed onto the potential solution, which lowers its desirability\cite<<2427>>AIModernApproach,NatureInspiredMetaHeuristic,Karen2004,Eisenblatter<<2427>>. 
\begin<<3300>>tex2html_deferred<<3300>>\par\end<<3301>>tex2html_deferred<<3301>>
An optimal solution would therefore be any solution that violates no hard constraints and violates no or a minimum number of soft constraints\cite<<2428>>AIModernApproach,NatureInspiredMetaHeuristic,Karen2004,Eisenblatter<<2428>>. Algorithms that are classified as being part of the collective body of algorithms known as metaheuristic algorithms are Tabu Search\cite<<2429>>TabuVechicleRoutingWithTimeWindows,TabuCSP<<2429>>, Simulated Annealing \cite<<2430>>SASingleMultiObj,CurveFittingSA<<2430>> and Genetic Algorithm\cite<<2431>>GATSP, GeostatisticalGA<<2431>>.
\begin<<3302>>tex2html_deferred<<3302>>\par\end<<3303>>tex2html_deferred<<3303>>
The above-mentioned algorithms are not the only algorithms to form part of this subdomain, but they are the algorithms that have received the most attention in the literature and produce good results\cite<<2432>>SweepMeta<<2432>>.
\begin<<3304>>tex2html_deferred<<3304>>\par\end<<3305>>tex2html_deferred<<3305>>
The main focus of this chapter is on each of the above algorithms. Before each of the algorithms is discussed, a brief overview is given of the various characteristics that metaheuristic algorithms exhibit. 
\begin<<3306>>tex2html_deferred<<3306>>\par\end<<3307>>tex2html_deferred<<3307>>
\gls<<2433>>AI<<2433>> search algorithms operate in search spaces, where they occupy various states while they are searching for a solution or goal state in a search space. Before these algorithms can be properly introduced, the concept of search spaces and states needs to be introduced.
\begin<<3308>>tex2html_deferred<<3308>>\par\end<<3309>>tex2html_deferred<<3309>>
\section<<2434>>Search Spaces and States<<2434>>
A search space is defined as a set of candidate goal states which may or may not be a solution to an problem\cite<<2435>>AIModernApproach<<2435>>. The candidate states are defined by the problem definition\cite<<2436>>AIModernApproach<<2436>>. The possible states can be given by a successor function which generates new states adhering to the problem defined constraints if any\cite<<2437>>AIModernApproach<<2437>>. 
\begin<<3310>>tex2html_deferred<<3310>>\par\end<<3311>>tex2html_deferred<<3311>>
The search space can also be explicitly defined with constraints to which a solution must adhere to\cite<<2438>>AIModernApproach<<2438>>. When constraints are defined it is up to the algorithm how it moves from one state to another in the search space\cite<<2439>>AIModernApproach<<2439>>. Each state the algorithm moves to is then checked whether it adheres to the constraints\cite<<2440>>AIModernApproach<<2440>>.
\begin<<3312>>tex2html_deferred<<3312>>\par\end<<3313>>tex2html_deferred<<3313>>
A state is a position in the search space\cite<<2441>>AIModernApproach<<2441>>. This position represents a configuration that describes a possible solution to the defined problem\cite<<2442>>AIModernApproach<<2442>>. When a algorithm is said to be moving to a new state, the algorithm moves to a new position in the search space\cite<<2443>>AIModernApproach<<2443>>. This new position represents a different solution than the previous position that was occupied\cite<<2444>>AIModernApproach<<2444>>.
\begin<<3314>>tex2html_deferred<<3314>>\par\end<<3315>>tex2html_deferred<<3315>>
When dealing with an optimisation problem there can be more than one solution\cite<<2445>>AIModernApproach<<2445>>. Which is why with optimisation problems not only does an algorithm have to find a solution, but also the most optimal solution among the candidate solutions\cite<<2446>>AIModernApproach<<2446>>. This subset of solutions in the search space is referred to as the \emph<<2447>>solution<<2447>> space.
\begin<<3316>>tex2html_deferred<<3316>>\par\end<<3317>>tex2html_deferred<<3317>>
Both concepts of search spaces and states have now been defined. All search algorithms operate in search spaces and occupy states. The domain of search contains a wide variety of algortihms. In the next section Metaheuristics Algorithms are discussed as this body of algorithms are used to solved optimisation problems.
\begin<<3318>>tex2html_deferred<<3318>>\par\end<<3319>>tex2html_deferred<<3319>>
\section<<2448>>Metaheuristics Algorithms<<2448>>
NP-Complete\begin<<3320>>tex2html_deferred<<3320>>\footnote<<3109>>A discussion of NP-Complete problems is appears in section \ref<<2449>>sec:NPComplete<<2449>><<3109>>\end<<3321>>tex2html_deferred<<3321>> problems have been proven to not be solvable in polynomial time by traditional uninformed search algorithms such as Breath First Search and Depth-First Search\cite<<2450>>AIModernApproach<<2450>>. 
\begin<<3322>>tex2html_deferred<<3322>>\par\end<<3323>>tex2html_deferred<<3323>>
Uninformed algorithms are not able to distinguish whether a particular goal state is more ``correct'' than any other goal state\cite<<2451>>AIModernApproach<<2451>>.
\begin<<3324>>tex2html_deferred<<3324>>\par\end<<3325>>tex2html_deferred<<3325>>
Uninformed search algorithms search spaces are represented by a tree structure\cite<<2452>>AIModernApproach<<2452>>. A search is performed in the search space by starting at a root node, the algorithm expands successive nodes based on a strategy\cite<<2453>>AIModernApproach<<2453>>. Bread-first search continiously expands all successive nodes and Depth-First search expands the deepest node first\cite<<2454>>AIModernApproach<<2454>>.
\begin<<3326>>tex2html_deferred<<3326>>\par\end<<3327>>tex2html_deferred<<3327>>
The path taken to a final  goal state node represents a solution\cite<<2455>>AIModernApproach<<2455>>. In a worst-case scenario, an uninformed search will expand every single node in its search space, thus each and every single possible solution is evaluated\cite<<2456>>AIModernApproach<<2456>>.
\begin<<3328>>tex2html_deferred<<3328>>\par\end<<3329>>tex2html_deferred<<3329>>
It is not always viable to test every possible solution in a given problem search space, especially in NP-Complete problems, since their search spaces are huge or infinite. This is why traditional algorithms are not able to produce optimal solutions in polynomial time\cite<<2457>>AIModernApproach<<2457>>.
\begin<<3330>>tex2html_deferred<<3330>>\par\end<<3331>>tex2html_deferred<<3331>>
Metaheuristic algorithms are considered to be \emph<<2458>>general-purpose<<2458>> algorithms and can thus be applied to a wide variety of optimisation problems with only small modifications that need to made to the algorithm model\cite<<2459>>MetaGraph<<2459>>.
\begin<<3332>>tex2html_deferred<<3332>>\par\end<<3333>>tex2html_deferred<<3333>>
Metaheuristic algorithms do not dictate all aspects of the search procedure but define guidelines and aspects which directs the search\cite<<2460>>HandbookofMH<<2460>>. Therefore a metaheuristic can be defined as follows: \emph<<2461>>the algorithm selects candidate solutions from a neighbourhood with respect to one or more current solutions of which the candidates are either rejected or accepted<<2461>>\cite<<2462>>HandbookofMH<<2462>>.
\begin<<3334>>tex2html_deferred<<3334>>\par\end<<3335>>tex2html_deferred<<3335>>
As can be gathered from the name, a metaheuristic algorithm uses some sort of heuristic. A heuristic is a decision rule. Algorithms use heuristics to make decisions by applying the heuristic to the data the algorithm is currently working on\cite<<2463>>AIModernApproach,NatureInspiredMetaHeuristic<<2463>>.
\begin<<3336>>tex2html_deferred<<3336>>\par\end<<3337>>tex2html_deferred<<3337>>
When an algorithm utilises a heuristic its forms part of a group known as informred search algorithms. As the name implies the algorithm is more ``informed'' using the heuristic the algorithm gains additional information about its search space. With this additional information the algorithm is able to performa a better search for a possible solution\cite<<2464>>AIModernApproach<<2464>>.
\begin<<3338>>tex2html_deferred<<3338>>\par\end<<3339>>tex2html_deferred<<3339>>
Heuristics are strategies that direct the search based on the information available, to move towards areas where there is a higher probability of obtaining high quality candidate solutions \cite<<2465>>AIModernApproach<<2465>>. A heuristic is able to dictate what an algorithm must do for its next iteration by evaluating the current internal state of the algorithm, i.e. whether it should move to a different point in the search space, generate new data, or select the current data as the most optimal solution\cite<<2466>>AIModernApproach,NatureInspiredMetaHeuristic<<2466>>. 
\begin<<3340>>tex2html_deferred<<3340>>\par\end<<3341>>tex2html_deferred<<3341>>
In general ``meta'' means \emph<<2467>>beyond<<2467>> or \emph<<2468>>higher level<<2468>>\cite<<2469>>AIModernApproach,NatureInspiredMetaHeuristic<<2469>>. A metaheuristic therefore refers to a heuristic that is more complex with regard to the decisions it is able to make compared with a standard heuristic\cite<<2470>>AIModernApproach,NatureInspiredMetaHeuristic<<2470>>. With a standard heuristic only the current state is considered\cite<<2471>>AIModernApproach<<2471>>. A metaheuristic takes additional information into account when considering a current state\cite<<2472>>NatureInspiredMetaHeuristic<<2472>>. This additional information can be previous states, or states that are considering to be close in the search space\cite<<2473>>AIModernApproach, NatureInspiredMetaHeuristic<<2473>>.
\begin<<3342>>tex2html_deferred<<3342>>\par\end<<3343>>tex2html_deferred<<3343>>
Metaheuristic-based search algorithms are not guaranteed to find the most optimal solutions in the search space; instead these algorithms are used to find near-optimal solutions. Thus most algorithmic development in the metaheuristic domain focuses on developing new techniques that will increase the probability that a good solution will be obtained in difficult combinatorial problems \cite<<2474>>MetaAgricultural<<2474>>.
\begin<<3344>>tex2html_deferred<<3344>>\par\end<<3345>>tex2html_deferred<<3345>>
Similarly, metaheuristics are not guaranteed to find suitable solutions or perform well in each problem domain it is applied to. The quality of the solution and performance of the metaheuristic is very much dependent upon on the expertise of the algorithm designer \cite<<2475>>AutoComplexMeta<<2475>>. Besides the algoritm designer modifying the algorithm; metaheuristic algorithms that are inherently population-based, hybrid and/or distributed, use the concept of social and self-organisation to better exploit the solution space\cite<<2476>>Self-AdaptiveMeta<<2476>>.
\begin<<3346>>tex2html_deferred<<3346>>\par\end<<3347>>tex2html_deferred<<3347>>
In this section the characteristics of metaheuristics that set these algorithms apart from the conventional algorithms used on difficult problems was introduced. In the next section of this chapter the tabu search algorithm is discussed.
\section<<2477>>Tabu Search<<2477>>
\label<<2478>>sec:tabusearch<<2478>>
\subsection<<2479>>Introduction<<2479>>
\label<<2480>>sec:TSIntroduction<<2480>>
\gls<<2481>>TS<<2481>> was first proposed by Glover\cite<<2482>>Glover89<<2482>> as a new searching technique to help algorithms avoid getting trapped in local optima in combinatorial and optimisation problems \cite<<2483>>TabuRCAProblem<<2483>>. Since Glover introduced the algorithm in the 1980s, tabu search has been applied to a wide range of problems such as the vehicle routing problem\cite<<2484>>TabuVechicleRoutingWithTimeWindows<<2484>>, \gls<<2485>>FAP<<2485>>\cite<<2486>>TabuMontemanniSmith<<2486>>, capacitated-lot sizing problem\cite<<2487>>TabuCarryOver<<2487>>, Nurse Scheduling\cite<<2488>>TabuNurse<<2488>> and the Resource Constrained Assignment Problem\cite<<2489>>TabuRCAProblem<<2489>>. 
\begin<<3348>>tex2html_deferred<<3348>>\par\end<<3349>>tex2html_deferred<<3349>>
Even though the problems mentioned differ by a large margin, the algorithm has been successful in most optimisation problems it has been applied to. If one observes the results obtained in research \cite<<2490>>TabuMontemanniSmith,tabuglobalplanning3g<<2490>>, it can be inferred that tabu search has on average obtained the best results compared with previous attempts with other algorithms. 
\begin<<3350>>tex2html_deferred<<3350>>\par\end<<3351>>tex2html_deferred<<3351>>
\gls<<2491>>TS<<2491>> resembles in its most basic form the hill-climbing search algorithm\cite<<2492>>TabuBiddingStrats<<2492>>. The hill-climbing search algorithm starts from an initial solution and then iteratively moves from the current solution to a neighbouring solution\cite<<2493>>AIModernApproach<<2493>>. Each neighbour is rated based on its attractiveness as a possible optimal solution that the algorithm is being applied to\cite<<2494>>AIModernApproach<<2494>>. 
\begin<<3352>>tex2html_deferred<<3352>>\par\end<<3353>>tex2html_deferred<<3353>>
The hill-climbing algorithm moves to the neighbour with the highest rating without considering whether the neighbour might lead the algorithm astray, to a position where the neighbours are in fact \emph<<2495>>worse<<2495>> than previously encountered possible solutions\cite<<2496>>AIModernApproach<<2496>>. 
\begin<<3354>>tex2html_deferred<<3354>>\par\end<<3355>>tex2html_deferred<<3355>>
The \gls<<2497>>TS<<2497>> algorithm addresses this shortcoming by introducing the concept of memory\cite<<2498>>TabuBiddingStrats<<2498>>. The memory of the algorithm is actually a history of previous solutions that the algorithm has moved to in its search for the search space for a solution\cite<<2499>>TabuBiddingStrats<<2499>>. 
\begin<<3356>>tex2html_deferred<<3356>>\par\end<<3357>>tex2html_deferred<<3357>>
General search algorithms like hill-climbing, random-restart\begin<<3358>>tex2html_deferred<<3358>>\footnote<<3110>>Random-restart is a search algorithm where once a certain trend of repeated moves is noticed, the algorithm restarts by generating a new initial solution to start from and then continues its search process from that generated solution\cite<<2500>>AIModernApproach<<2500>>.<<3110>>\end<<3359>>tex2html_deferred<<3359>> or scatter search tend to get trapped in local optima \cite<<2501>>AIModernApproach<<2501>>. The local optima might be a very attractive solution and thus general search algorithms will not move to better solutions since, according to the algorithm's built-in strategy, it has found the best solution. 
\begin<<3360>>tex2html_deferred<<3360>>\par\end<<3361>>tex2html_deferred<<3361>>
In actual fact the solution that was found is the best solution in the \emph<<2502>>local<<2502>> search space, but not in the \emph<<2503>>global<<2503>> search space\cite<<2504>>CompuIntelligenceIntro,AIModernApproach<<2504>>. Therefore an important characteristic of algorithms being applied to optimisation problems is breaking out of local optima\cite<<2505>>CompuIntelligenceIntro,AIModernApproach<<2505>>.
\begin<<3362>>tex2html_deferred<<3362>>\par\end<<3363>>tex2html_deferred<<3363>>
The next section discussed some of the characteristics that make the \gls<<2506>>TS<<2506>> algorithm unique.
\begin<<3364>>tex2html_deferred<<3364>>\par\end<<3365>>tex2html_deferred<<3365>>
\subsection<<2507>>Important Tabu Search Characteristics<<2507>>
\label<<2508>>sec:TScharacteristics<<2508>>
Various characteristics are important to the \gls<<2509>>TS<<2509>> algorithm. The first characteristic is exactly how the \gls<<2510>>TS<<2510>> algorithm iteratively improves upon the initial start solution.
\begin<<3366>>tex2html_deferred<<3366>>\par\end<<3367>>tex2html_deferred<<3367>>
\subsubsection<<2511>>Initial Solution Generation<<2511>>
The core feature of the \gls<<2512>>TS<<2512>> algorithm is to sequentially improve an initial solution \cite<<2513>>TSHazardous<<2513>>. An initial possible solution is a point in the search space where the \gls<<2514>>TS<<2514>> algorithm will \emph<<2515>>start<<2515>> exploring in search of a more optimal solution \cite<<2516>>AIModernApproach,TSHazardous<<2516>>.
\begin<<3368>>tex2html_deferred<<3368>>\par\end<<3369>>tex2html_deferred<<3369>>
An important consideration one has to make is how initial solutions are generated for the \gls<<2517>>TS<<2517>> algorithm to start on\cite<<2518>>AIModernApproach,TSHazardous<<2518>>.
\begin<<3370>>tex2html_deferred<<3370>>\par\end<<3371>>tex2html_deferred<<3371>>
Random initial solutions might seem to be a good starting point, but by introducing randomisation it becomes hard to control the quality of the end solution\cite<<2519>>TSHazardous<<2519>>. Hence the generation of starting solutions must be controlled to limit the infeasibility of potential solutions \cite<<2520>>TSHazardous<<2520>>. 
\begin<<3372>>tex2html_deferred<<3372>>\par\end<<3373>>tex2html_deferred<<3373>>
Control of the randomly generation solutions can be achieved by simply constraining the random solution generator to only generate initial starting points in a bounded subset of the entire search space. For example: instead of letting the random initial starting point be any number between positive infinity and negative infinity, the random number generator is constrained to only generate numbers between 5 and -5.
\begin<<3374>>tex2html_deferred<<3374>>\par\end<<3375>>tex2html_deferred<<3375>>
\subsubsection<<2521>>Neighbourhood Search<<2521>>
The following discussion on neighbourhood search is not meant to be an exhaustive survey on the different methods and how they differ under different problems. Instead the discussion is ment as a general overview to get an idea of neighborhood generation in the \gls<<2522>>TS<<2522>> algorithm context. 
\begin<<3376>>tex2html_deferred<<3376>>\par\end<<3377>>tex2html_deferred<<3377>>
The following neighbourhood discussion will be based on the assumption that the underlying problem the \gls<<2523>>TS<<2523>> is applied to, has a search space with defined boundaries that is suitable for neighborhood generation.
\begin<<3378>>tex2html_deferred<<3378>>\par\end<<3379>>tex2html_deferred<<3379>>
TS uses a neighbourhood local search process to explore the solution space. There is no set process of how neighbourhood candidate solutions are selected as it is problem dependant. The overall quality of the solution produced by \gls<<2524>>TS<<2524>> is also dependent on the neighbourhood search strategy used \cite<<2525>>TSHazardous<<2525>>. 
\begin<<3380>>tex2html_deferred<<3380>>\par\end<<3381>>tex2html_deferred<<3381>>
The neighbourhood search phase is the first operation performed after the algorithm has been initialised, which is to say the algorithm has generated an initial starting solution from which the exploration process can start.
\begin<<3382>>tex2html_deferred<<3382>>\par\end<<3383>>tex2html_deferred<<3383>>
The neighbourhood search phase is the primary means for the \gls<<2526>>TS<<2526>> algorithm to search the solution space for an optimal solution. It is within this phase that new possible solutions must be presented for the \gls<<2527>>TS<<2527>> heuristic to allow the algorithm to decide to which solution it must move next.
\begin<<3384>>tex2html_deferred<<3384>>\par\end<<3385>>tex2html_deferred<<3385>>
The new possible solutions that are generated are called neighbouring solutions; hence the \gls<<2528>>TS<<2528>> algorithm always moves to a neighbouring solution. When the \gls<<2529>>TS<<2529>> algorithm moves to a neighbouring solution, the current solution is replaced by the neighbouring solution. Therefore, in the next iteration, neighbours for the new solution need to be generated.
\begin<<3386>>tex2html_deferred<<3386>>\par\end<<3387>>tex2html_deferred<<3387>>
Generation of new neighbours can range from a simple increment option to a complex operation that incorporates additional intelligence by means of a more heuristic approach to generate new neighbours.
\begin<<3388>>tex2html_deferred<<3388>>\par\end<<3389>>tex2html_deferred<<3389>>
The \gls<<2530>>TS<<2530>> algorithm is not limited to just one neighbourhood search strategy. In the research by Gopalakrishnan et al.\cite<<2531>>TabuCarryOver<<2531>> five neighbourhood move strategies are developed and are used interchangeably; in some cases a strategy is used three times in a row due to stagnation in the search space. 
\begin<<3390>>tex2html_deferred<<3390>>\par\end<<3391>>tex2html_deferred<<3391>>
Stagnation occurs when the algorithm does not move to a better solution; instead it opts to stay on the current solution, as no neighbouring solution is better than the current one. 
\begin<<3392>>tex2html_deferred<<3392>>\par\end<<3393>>tex2html_deferred<<3393>>
Other neighbourhood strategies developed is that by N. A. Wassan \cite<<2532>>ReactiveTabuVHR<<2532>>. Wassan used a neighbourhood selection strategy that exchanges route nodes from initial vehicle routes for the vehicle routing problem. This route exchange enables the \gls<<2533>>TS<<2533>> algorithm to search much more broadly due to the constant supply of different solutions. 
\begin<<3394>>tex2html_deferred<<3394>>\par\end<<3395>>tex2html_deferred<<3395>>
Since initial solutions are constantly modified, it enables the \gls<<2534>>TS<<2534>> procedure to be a very fined-grained process, because often a small change in a potential solution can have a big impact on the overall proposed solution by the \gls<<2535>>TS<<2535>> algorithm.
\begin<<3396>>tex2html_deferred<<3396>>\par\end<<3397>>tex2html_deferred<<3397>>
In the research done by Zhang et al.\cite<<2536>>TSHazardous<<2536>> a neighbourhood selection scheme called \emph<<2537>>dynamic penalty<<2537>> is developed. When the algorithm moves onto an infeasible solution a penalty is imposed. By dynamically changing the penalty that is imposed the ``feasibility'' of solutions produced is influenced. 
\begin<<3398>>tex2html_deferred<<3398>>\par\end<<3399>>tex2html_deferred<<3399>>
Therefore, if and when the algorithm continually produces infeasible solutions, the penalty imposed is increased to guide the algorithm to produce more feasible solutions. Finally, when the algorithm becomes trapped at local optima, the penalty is reduced, which allows the algorithm to consider moving onto infeasible solutions thus escaping local optima.
\begin<<3400>>tex2html_deferred<<3400>>\par\end<<3401>>tex2html_deferred<<3401>>
TS is an iterative algorithm, executing a set of operations sequentially until a stopping criterion is met\cite<<2538>>EvoParallelTabu,TabuVechicleRoutingWithTimeWindows<<2538>>. At each iteration the algorithm has to determine feasibility of the immediate neighbourhood candidate solutions \cite<<2539>>EvoParallelTabu,TabuVechicleRoutingWithTimeWindows<<2539>>. 
\begin<<3402>>tex2html_deferred<<3402>>\par\end<<3403>>tex2html_deferred<<3403>>
Therefore each candidate must be evaluated by some function, which may be a costly operation in terms of computational cycles as well as in terms of time\cite<<2540>>EvoParallelTabu,TabuVechicleRoutingWithTimeWindows<<2540>>. This constant evaluation can drastically reduce the overall performance of the algorithm, since it is spending more time calculating feasibility than actually searching the solution space \cite<<2541>>EvoParallelTabu,TabuVechicleRoutingWithTimeWindows<<2541>>. 
\begin<<3404>>tex2html_deferred<<3404>>\par\end<<3405>>tex2html_deferred<<3405>>
\subsubsection<<2542>>Memory Structures of Tabu Search<<2542>>
The hill-climbing and random-restart algorithms are able to break out of local minima, but there is nothing stopping these algorithms from avoiding the local optima with their second or n-pass in the search space. \gls<<2543>>TS<<2543>> addresses the shortcoming of these algorithms by incorporating an important concept: the notion of memory.
\begin<<3406>>tex2html_deferred<<3406>>\par\end<<3407>>tex2html_deferred<<3407>>
In its most basic form \gls<<2544>>TS<<2544>> keeps a local memory of all its recent best moves, and puts them into a \emph<<2545>>tabu list<<2545>> that has a predefined size. In the literature the Tabu list is also referred to as the \emph<<2546>>tabu tenure<<2546>> \cite<<2547>>TSHazardous,TabuCarryOver<<2547>>. The algorithm is not allowed to move to any solution that is in the tabu list unless a solution that is \emph<<2548>>tabu<<2548>> is better than any current moves available in the immediate search neighbourhood \cite<<2549>>TabuCarryOver,ReactiveTabuVHR<<2549>>. The process of overriding a solution's tabu status in the tabu tenure is called the \emph<<2550>>aspiration criterion<<2550>> \cite<<2551>>TSHazardous,TabuCarryOver<<2551>>. With the use of the tabu tenure and the aspiration criterion, the algorithm is able to avoid cycling, local optima as well as searching in a too narrow region \cite<<2552>>TabuSingleMachineScheduling,CircuitTabu<<2552>>.
\begin<<3408>>tex2html_deferred<<3408>>\par\end<<3409>>tex2html_deferred<<3409>>
Research done by Ashish Sureka and Peter R. Wurman makes an important distinction with regard to the memory scheme that is used in the \gls<<2553>>TS<<2553>> algorithm. Two memory schemes are discussed: \emph<<2554>>explicit memory<<2554>> and \emph<<2555>>attribute-based memory<<2555>> \cite<<2556>>TabuBiddingStrats,TabuFormGames<<2556>>. Of the two memory schemes the explicit memory scheme is the most used in the literature \cite<<2557>>TabuVechicleRoutingWithTimeWindows<<2557>>.
\begin<<3410>>tex2html_deferred<<3410>>\par\end<<3411>>tex2html_deferred<<3411>>
With explicit memory the algorithm stores a complete solution in the tabu tenure; hence the algorithm is prohibited from moving to that position in the search for as long as the solution is in the Tabu tenure\cite<<2558>>TabuBiddingStrats,TabuFormGames<<2558>>. With attribute-based memory the algorithm stores the \emph<<2559>>operation<<2559>> used to move from the previous solution to the current solution\cite<<2560>>TabuBiddingStrats,TabuFormGames<<2560>>. Therefore with attribute-based memory the tabu tenure intended function is changed from prohibiting certain solutions already encountered to rather prohibiting making changes to the current solution that would lead to solutions already present in the tabu tenure \cite<<2561>>TabuBiddingStrats,TabuFormGames<<2561>>.
\begin<<3412>>tex2html_deferred<<3412>>\par\end<<3413>>tex2html_deferred<<3413>>
In research conducted by Clarkson et.\ al\cite<<2562>>MultiObjTabu<<2562>>, the authors add two additional memory structures called \gls<<2563>>MTM<<2563>> and \gls<<2564>>LTM<<2564>> besides the standard \gls<<2565>>STM<<2565>>, referred to as the tabu List \cite<<2566>>MultiObjTabu<<2566>>. Each additional structure remembers a different set of solutions for use by the diversification and intensification phases in the algorithm.
\begin<<3414>>tex2html_deferred<<3414>>\par\end<<3415>>tex2html_deferred<<3415>>
STM is similar to the traditional tabu list: to store the most recent solutions produced by the algorithm. \gls<<2567>>MTM<<2567>> is designed to remember optimal or near-optimal solutions. These solutions are therefore used later in the intensification phase. Finally, the \gls<<2568>>LTM<<2568>> structure stores all the solutions that the algorithm has already explored and is thus used in the diversification phase of the algorithm \cite<<2569>>MultiObjTabu<<2569>>.
\begin<<3416>>tex2html_deferred<<3416>>\par\end<<3417>>tex2html_deferred<<3417>>
\subsubsection<<2570>>Search Phases<<2570>>
\label<<2571>>TSSearchPhases<<2571>>
As \gls<<2572>>TS<<2572>> searches through the search space, it goes through two cycles of search phases called \emph<<2573>>diversification<<2573>> and \emph<<2574>>intensification<<2574>> \cite<<2575>>TabuParameterization,TabuCrewSchedulingProblem,NonlinearGlobalTabu,SelfControllingReactiveTabu<<2575>>.
\begin<<3418>>tex2html_deferred<<3418>>\par\end<<3419>>tex2html_deferred<<3419>>
The diversification phase in the \gls<<2576>>TS<<2576>> algorithm is the phase where the algorithm is directed to areas in the search space that has not yet been explored. The algorithm applies diversification as mechanisms monitoring the memory; note that solutions being produced are being repeated \cite<<2577>>ReactiveTabuVHR,SelfControllingReactiveTabu<<2577>>. 
\begin<<3420>>tex2html_deferred<<3420>>\par\end<<3421>>tex2html_deferred<<3421>>
Research by Fescioglu-Unver and Kokar \cite<<2578>>SelfControllingReactiveTabu<<2578>> provides a strategy that consists of two components namely the \emph<<2579>>observer<<2579>> and the \emph<<2580>>diversifier<<2580>>. The goal of the observer is to continually monitor the best solution obtained by the algorithm as to whether it violates the \emph<<2581>>stagnation period<<2581>>. The stagnation period is defined as the number of iterations where the current best obtained solution has not changed and the algorithm has not moved to a new solution\cite<<2582>>SelfControllingReactiveTabu<<2582>>. 
\begin<<3422>>tex2html_deferred<<3422>>\par\end<<3423>>tex2html_deferred<<3423>>
As soon as the current solution exceeds the stagnation period the observer component activates and transfers the necessary information needed by the diversifier component. The diversifier component dynamically changes the size of the tabu tenure based on the information the observer gathered. The diversifier mainly targets older moves to diversify, but for short bursts of time it decreases the tabu list size to a very small value in an attempt to combine new and old moves \cite<<2583>>SelfControllingReactiveTabu<<2583>>.
\begin<<3424>>tex2html_deferred<<3424>>\par\end<<3425>>tex2html_deferred<<3425>>
The specific mechanism used to define a new position where the algorithm can continue to search, should ideally select areas in the search space that have not been explored yet\cite<<2584>>NonlinearGlobalTabu,SelfControllingReactiveTabu<<2584>>. Therefore, the diversification phase makes extensive use of the knowledge present in the long-term memory structures as an indication of what areas of the search space have been previously explored and which areas have not \cite<<2585>>NonlinearGlobalTabu,SelfControllingReactiveTabu<<2585>>.
\begin<<3426>>tex2html_deferred<<3426>>\par\end<<3427>>tex2html_deferred<<3427>>
Intensification is the first phase of the \gls<<2586>>TS<<2586>> algorithm, since it is responsible for building up a history in memory on which the diversification phase can act. Fescioglu-Unver and Kokar also present an intensification strategy based on control theory in their research \cite<<2587>>SelfControllingReactiveTabu<<2587>>. The authors identify repetition length as a critical value for their intensification strategy to be based upon. The repetition length is a control measure that defines how many times the algorithm can occupy the same solution within a span of iterations.
\begin<<3428>>tex2html_deferred<<3428>>\par\end<<3429>>tex2html_deferred<<3429>>
In the following section, an overview is presented of the flow the \gls<<2588>>TS<<2588>> algorithm along with pseudo code describing the \gls<<2589>>TS<<2589>> algorithm.
\subsection<<2590>>Flow of the algorithm<<2590>>
In this section the general flow of the \gls<<2591>>TS<<2591>> algorithm is described using algorithm~\ref<<2592>>alg:TS<<2592>> as a reference point.
\begin<<2593>>algorithm<<2593>>[H]
\begin<<3430>>tex2html_deferred<<3430>>\caption<<3111>>Basic Tabu Search Algorithm\cite<<2594>>TabuRCAProblem,TabuMontemanniSmith<<2594>><<3111>>\end<<3431>>tex2html_deferred<<3431>>
\label<<2595>>alg:TS<<2595>>
	\begin<<2596>>algorithmic<<2596>>[1]
		\State Initialize parameters
    \State \begin<<3130>>tex2html_wrap_inline<<3130>>$\hat<<2597>>x_0<<2597>> \leftarrow$\end<<3131>>tex2html_wrap_inline<<3131>>\space Initialize starting solution
		\While<<2598>>stopping criteria not met<<2598>>
    \State \begin<<3132>>tex2html_wrap_inline<<3132>>$\hat<<2599>>y_i<<2599>> \leftarrow$\end<<3133>>tex2html_wrap_inline<<3133>>\space Determine \begin<<3134>>tex2html_wrap_inline<<3134>>$\hat<<2600>>x_i<<2600>>$\end<<3135>>tex2html_wrap_inline<<3135>>\space neighbourhood solutions 
    \State Evaluate neighbouring solutions with fitness function \begin<<3136>>tex2html_wrap_inline<<3136>>$f(\hat<<2601>>y_i<<2601>>)$\end<<3137>>tex2html_wrap_inline<<3137>>
    \State \begin<<3138>>tex2html_wrap_inline<<3138>>$\hat<<2602>>z_i<<2602>> \leftarrow$\end<<3139>>tex2html_wrap_inline<<3139>>Select best neighbour from \begin<<3140>>tex2html_wrap_inline<<3140>>$\hat<<2603>>y_i<<2603>>$\end<<3141>>tex2html_wrap_inline<<3141>>
    \If<<3112>>Move to \begin<<3142>>tex2html_wrap_inline<<3142>>$\hat<<2604>>z_i<<2604>>$\end<<3143>>tex2html_wrap_inline<<3143>>\space is Tabu<<3112>>
    \If<<3113>>\begin<<3144>>tex2html_wrap_inline<<3144>>$\hat<<2605>>z_i<<2605>>$\end<<3145>>tex2html_wrap_inline<<3145>>\space meets Aspiration Criterion<<3113>>
    \State \begin<<3146>>tex2html_wrap_inline<<3146>>$\hat<<2606>>x_i<<2606>> \leftarrow \hat<<2607>>z_i<<2607>>$\end<<3147>>tex2html_wrap_inline<<3147>>
				\EndIf
			\Else
      \State Add \begin<<3148>>tex2html_wrap_inline<<3148>>$\hat<<2608>>x_i<<2608>>$\end<<3149>>tex2html_wrap_inline<<3149>>\space to Tabu List
      \State \begin<<3150>>tex2html_wrap_inline<<3150>>$\hat<<2609>>x_i<<2609>> \leftarrow \hat<<2610>>z_i<<2610>>$\end<<3151>>tex2html_wrap_inline<<3151>>
      \If<<3114>>\begin<<3152>>tex2html_wrap_inline<<3152>>$\hat<<2611>>x_i<<2611>>$\end<<3153>>tex2html_wrap_inline<<3153>>\space repeated \begin<<3154>>tex2html_wrap_inline<<3154>>$\ge$\end<<3155>>tex2html_wrap_inline<<3155>>\space max repeats<<3114>>
					\State diversify()
				\Else
					\State intensify()
				\EndIf
			\EndIf
		\EndWhile
    \State Return \begin<<3156>>tex2html_wrap_inline<<3156>>$\hat<<2612>>x_i<<2612>>$\end<<3157>>tex2html_wrap_inline<<3157>>\space as best found solution
	\end<<2613>>algorithmic<<2613>>
\end<<2614>>algorithm<<2614>>
\begin<<3432>>tex2html_deferred<<3432>>\par\end<<3433>>tex2html_deferred<<3433>>
Before the algorithm can actually start searching, it first needs to initialise various parameters. These parameters include, but are not limited to, the tabu list size, the aspiration criterion and the starting solution. The initialisation can be observed to occur from lines 1 - 2.
\begin<<3434>>tex2html_deferred<<3434>>\par\end<<3435>>tex2html_deferred<<3435>>
Once all the various parameters that are needed by the algorithm have been initialised, the algorithm is ready to enter the actual search phase, which ranges from lines 3 -- 21. 
\begin<<3436>>tex2html_deferred<<3436>>\par\end<<3437>>tex2html_deferred<<3437>>
The search phase starts off by first generating possible solutions that neighbour the current solution \begin<<3158>>tex2html_wrap_inline<<3158>>$x_i$\end<<3159>>tex2html_wrap_inline<<3159>>\space as can be observed in line 4. Generating neighbouring solutions are a critical process in the \gls<<2615>>TS<<2615>> algorithm as tehy are the means by which the algorithm is able to move from one possible solution to the next in the search space.
\begin<<3438>>tex2html_deferred<<3438>>\par\end<<3439>>tex2html_deferred<<3439>>
After all the solutions that neighbour the current possible solution have been generated, the algorithm needs to decide which of the possible neighbours is the most rewarding. The algorithm therefore determines the fitness of each neighbour \begin<<3160>>tex2html_wrap_inline<<3160>>$y_i$\end<<3161>>tex2html_wrap_inline<<3161>>\space by applying a fitness function \begin<<3162>>tex2html_wrap_inline<<3162>>$f(y_i)$\end<<3163>>tex2html_wrap_inline<<3163>>. 
\begin<<3440>>tex2html_deferred<<3440>>\par\end<<3441>>tex2html_deferred<<3441>>
Once all the neighbours have been evaluated, the algorithm selects the best neighbour that not only has the best fitness out of all the generated neighbours, but also has a better fitness than the current solution held by the algorithm. The best neighbour selection can be seen to occur in line 6.
\begin<<3442>>tex2html_deferred<<3442>>\par\end<<3443>>tex2html_deferred<<3443>>
The algorithm has now determined a possible neighbour \begin<<3164>>tex2html_wrap_inline<<3164>>$z_i$\end<<3165>>tex2html_wrap_inline<<3165>>\space to move towards. Before moving on to the next iteration, it first needs to perform a series of checks that will aid it in the search process.
\begin<<3444>>tex2html_deferred<<3444>>\par\end<<3445>>tex2html_deferred<<3445>>
The first check that needs to be performed is whether the neighbour \begin<<3166>>tex2html_wrap_inline<<3166>>$z_i$\end<<3167>>tex2html_wrap_inline<<3167>>\space is in the tabu list and this occurs in line 7. 
\begin<<3446>>tex2html_deferred<<3446>>\par\end<<3447>>tex2html_deferred<<3447>>
If the neighbour \begin<<3168>>tex2html_wrap_inline<<3168>>$z_i$\end<<3169>>tex2html_wrap_inline<<3169>>\space is in the tabu list, then another check is performed where the aspiration criterion is calculated as can be seen in lines 8 -- 10. The aspiration criterion determines whether the algorithm can make neighbour \begin<<3170>>tex2html_wrap_inline<<3170>>$z_i$\end<<3171>>tex2html_wrap_inline<<3171>>\space its current solution once more even though it is tabu. 
\begin<<3448>>tex2html_deferred<<3448>>\par\end<<3449>>tex2html_deferred<<3449>>
If the aspiration criterion has been met, the algorithm makes neighbour \begin<<3172>>tex2html_wrap_inline<<3172>>$z_i$\end<<3173>>tex2html_wrap_inline<<3173>>\space its current solution \begin<<3174>>tex2html_wrap_inline<<3174>>$x_i$\end<<3175>>tex2html_wrap_inline<<3175>>. 
\begin<<3450>>tex2html_deferred<<3450>>\par\end<<3451>>tex2html_deferred<<3451>>
In the algorithm, if a neighbour \begin<<3176>>tex2html_wrap_inline<<3176>>$z_i$\end<<3177>>tex2html_wrap_inline<<3177>>\space is found not to be in the tabu list, the algorithm then adds the currently held solution \begin<<3178>>tex2html_wrap_inline<<3178>>$x_i$\end<<3179>>tex2html_wrap_inline<<3179>>\space to the tabu list. The current solution is added to prohibit future movements to the same solution in an attempt to avoid cycling of solutions. After \begin<<3180>>tex2html_wrap_inline<<3180>>$x_i$\end<<3181>>tex2html_wrap_inline<<3181>>\space has been added to the tabu list, the algorithm makes \begin<<3182>>tex2html_wrap_inline<<3182>>$z_i$\end<<3183>>tex2html_wrap_inline<<3183>>\space the current solution \begin<<3184>>tex2html_wrap_inline<<3184>>$x_i$\end<<3185>>tex2html_wrap_inline<<3185>>. This process can be observed from lines 12 -- 14.
\begin<<3452>>tex2html_deferred<<3452>>\par\end<<3453>>tex2html_deferred<<3453>>
Before the algorithm continues to the next iteration, it performs one last final check. The purpose of this check is to determine whether the algorithm is repeating solutions. As can be observed from lines 15 -- 19, the algorithm calculates whether the new selected solution has been repeated for a certain number of iterations. 
\begin<<3454>>tex2html_deferred<<3454>>\par\end<<3455>>tex2html_deferred<<3455>>
If the solution has indeed been repeated for a predetermined number of iterations, the algorithm activates its diversification strategy or intensifies its search.
\begin<<3456>>tex2html_deferred<<3456>>\par\end<<3457>>tex2html_deferred<<3457>>
The section that follows presents an overview of the \gls<<2616>>TS<<2616>> being applied to the \gls<<2617>>FAP<<2617>>.
\subsection<<3115>>Tabu Search on the \gls<<2618>>FAP<<2618>><<3115>>
In a study conducted by Robert Montemanni and Derek Smith \cite<<2619>>TabuMontemanniSmith<<2619>> the \gls<<2620>>TS<<2620>> algorithm is used on the \gls<<2621>>FS-FAP<<2621>>. The authors had to make some alterations to the algorithm to suit their needs as well as to make the algorithm more efficient in exploring in the \gls<<2622>>FAP<<2622>> solution space.
\begin<<3458>>tex2html_deferred<<3458>>\par\end<<3459>>tex2html_deferred<<3459>>
The \gls<<2623>>TS<<2623>> algorithm used by the authors is the multistart \gls<<2624>>TS<<2624>> algorithm, which randomly starts on different initial solutions \cite<<2625>>TabuMontemanniSmith<<2625>>.
\begin<<3460>>tex2html_deferred<<3460>>\par\end<<3461>>tex2html_deferred<<3461>>
The authors developed a technique called \gls<<2626>>HMT<<2626>>. \gls<<2627>>HMT<<2627>> first monitors an underlying heuristic being used on the problem by the algorithm\cite<<2628>>TabuMontemanniSmith<<2628>>. It then identifies certain characteristics that good solutions exhibit. In the \gls<<2629>>FAP<<2629>>, it is transmitters that are assigned different frequencies which results in an overall lower interference value\cite<<2630>>TabuMontemanniSmith<<2630>>.
\begin<<3462>>tex2html_deferred<<3462>>\par\end<<3463>>tex2html_deferred<<3463>>
The HMT then uses the identified characteristics to add \emph<<2631>>additional<<2631>> constraints to the problem\cite<<2632>>TabuMontemanniSmith<<2632>>. By adding constraints, the search space is reduced. However, by reducing the search space, other near-optimal solutions which might be far better are excluded\cite<<2633>>TabuMontemanniSmith<<2633>>. It is for this reason that that Montemanni and Smith opted not to add the constraints permanently.
\begin<<3464>>tex2html_deferred<<3464>>\par\end<<3465>>tex2html_deferred<<3465>>
Montemanni and Smith applied their \gls<<2634>>TS<<2634>> algorithm together with HMT to the \gls<<2635>>COST<<2635>> 259 family of benchmarks, specifically the Siemens1, Siemens2, Siemens3 and Siemens4 problems. The results are presented in table~\ref<<2636>>TSCOST259<<2636>>. The values presented are scalar and indicate the total interference of the frequency plan. The lower the interference value is, the better the frequency plan.
\begin<<2637>>table<<2637>>[H]
\begin<<3466>>tex2html_deferred<<3466>>\centering\end<<3467>>tex2html_deferred<<3467>>
	\begin<<2638>>tabular<<2638>><<2639>>| c | c | c |<<2639>>
		\hline
		Problem instance ;SPMamp; \gls<<2640>>TS<<2640>> with \gls<<2641>>HMT<<2641>> ;SPMamp; Best \gls<<2642>>COST<<2642>> 259 \\  \hline
		Siemens1 ;SPMamp; 2.7692 ;SPMamp; 2.200 \\  \hline
		Siemens2 ;SPMamp; 14.9360 ;SPMamp; 14.280 \\  \hline
		Siemens3 ;SPMamp; 6.6496 ;SPMamp; 5.19 \\  \hline
		Siemens4 ;SPMamp; 110.9725 ;SPMamp; 81.89 \\  \hline
	\end<<2643>>tabular<<2643>>
\begin<<3468>>tex2html_deferred<<3468>>\caption<<3116>>Results of applying \gls<<2644>>TS<<2644>> with \gls<<2645>>HMT<<2645>> on \gls<<2646>>COST<<2646>> 259<<3116>>\end<<3469>>tex2html_deferred<<3469>>
\label<<2647>>TSCOST259<<2647>>
\end<<2648>>table<<2648>>
As can be observed from the results obtained by the authors, the \gls<<2649>>TS<<2649>> algorithm with HMT produces results that rank very favourably against other algorithms also applied to the \gls<<2650>>COST<<2650>> 259.
\begin<<3470>>tex2html_deferred<<3470>>\par\end<<3471>>tex2html_deferred<<3471>>
When critically reasoning about the \gls<<2651>>TS<<2651>> algorithm with regard to applying it to the \gls<<2652>>FAP<<2652>>, the following disadvantages in theory can be identified:
\paragraph<<2653>>Search based on a single solution<<2653>>
--- The \gls<<2654>>TS<<2654>> algorithm at any moment in time only searches in the vicinity of \emph<<2655>>one<<2655>> current solution for possible neighbours that might be the current solution for the next iteration. \gls<<2656>>FAP<<2656>>s have huge search spaces due to their NP-Complete nature. Therefore, only searching for possible rewarding neighbours from only potential solutions seems to be terribly inefficient. A better strategy would be to use the notion of population-based algorithms and have multiple solutions from which more rewarding neighbours are searched.
\paragraph<<2657>>Neighbourhood generation<<2657>>
--- The \gls<<2658>>TS<<2658>> algorithm defines no set process for generating a neighbouring solution given a starting solution. Generating neighbours from a solution is a critical process in the \gls<<2659>>TS<<2659>> algorithm, for it is the only means by which the algorithm considers other solutions, i.e. it is the mechanism by which the algorithm searches. Generating a new neighbour can be as simple as changing only one value from the current solution or it can be very complex and incorporate other algorithms together with mathematics formulae. Regardless of the complexity of the neighbour generation that is used, care must be taken to ensure that the algorithm is able to produce a wide diversity of neighbours and is also able to intensify on the most optimal solution.
\paragraph<<2660>>Tabu lifetime<<2660>>
--- The \gls<<2661>>TS<<2661>> algorithm only operates on a single solution at a time and at most only considers one potential neighbour as its next possible current solution. Therefore a difficult choice needs to be made as to how long a solution stays tabu. In the \gls<<2662>>FAP<<2662>> a solution might be entered into the tabu list early in the search process of the algorithm. A large majority of the neighbours of this solution are vastly superior solutions compared with any of the current solutions produced by the algorithm. Due to the solution with these neighbours being in the tabu list, these neighbours will not be reconsidered until much later when the solution is removed from the list. A possible option to allow the \gls<<2663>>TS<<2663>> to reconsider the tabu solutions is to increase the aspiration criterion. Increasing the aspiration criterion does have its risks. A high aspiration criterion and the algorithm might be too eager to select just any solution even though it is Tabu. A low aspiration criterion and the algorithm will be too strict in selecting a tabu solution.
\begin<<3472>>tex2html_deferred<<3472>>\par\end<<3473>>tex2html_deferred<<3473>>
In the next section the simulated annealing algorithm is discussed.
\section<<2664>>Simulated Annealing<<2664>>
\label<<2665>>sec:simulatedannealing<<2665>>
\begin<<3474>>tex2html_deferred<<3474>>\par\end<<3475>>tex2html_deferred<<3475>>
\subsection<<2666>>Introduction<<2666>>
\label<<2667>>sec:SAIntroduction<<2667>>
\gls<<2668>>SA<<2668>> is a metaheuristic search technique proposed in the 1980s by Kirkpatrick to solve combinatorial optimisation problems. The technique is based on a natural process which is known in metallurgy as annealing \cite<<2669>>SASingleMultiObj,TempCyclingSA<<2669>>. Kirkpatrick was the first to use \gls<<2670>>SA<<2670>> to solve optimisations problems but the basic algorithm structure was defined by Metropolis et al.\ in 1953 \cite<<2671>>CurveFittingSA,VeryFastSAImageEnchancement<<2671>>.
\begin<<3476>>tex2html_deferred<<3476>>\par\end<<3477>>tex2html_deferred<<3477>>
Annealing is the natural process of crystallisation when a solid is heated to a high temperature and then systematically cooled to a lower temperature to reach a crystallised form \cite<<2672>>NewSAs,ConstantTempSA<<2672>>. The crystallised form of the solid is known to be the global minimum of the solid's internal energy state. 
\begin<<3478>>tex2html_deferred<<3478>>\par\end<<3479>>tex2html_deferred<<3479>>
When the solid is rapidly cooled from a high temperature, the molecules have no time to reach a thermodynamic equilibrium stage \cite<<2673>>MobileRobotSA,ConstantTempSA<<2673>>. Therefore the molecules of the solid have high energy and the resultant structure has no real crystalline form; thus the solid energy is at a local minimum\cite<<2674>>CurveFittingSA,NewSAs,MobileRobotSA<<2674>>. When the solid is slowly cooled in a controlled manner, the molecules are able to reach a thermal equilibrium at each temperature \cite<<2675>>ChaosSA,CurveFittingSA,NewSAs<<2675>>.
\begin<<3480>>tex2html_deferred<<3480>>\par\end<<3481>>tex2html_deferred<<3481>>
In the algorithm the energy state is the \emph<<2676>>cost function<<2676>> that needs to be minimised, and the molecules are the \emph<<2677>>variables<<2677>>, which represent the solutions, and thus their state needs to be optimised to reach the desired energy state.
\begin<<3482>>tex2html_deferred<<3482>>\par\end<<3483>>tex2html_deferred<<3483>>
The \gls<<2678>>SA<<2678>> algorithm is able to purposely move to a worse solution in the search space\cite<<2679>>EcoEquilSA<<2679>>. In the research when an algorithm moves to a worse solution it is classified as an \emph<<2680>>uphill<<2680>> move\cite<<2681>>AIModernApproach<<2681>>. Similarly, when a move is made by the algorithm which results in a better solution the move is classified as an \emph<<2682>>downhill<<2682>> move\cite<<2683>>AIModernApproach<<2683>>.
\begin<<3484>>tex2html_deferred<<3484>>\par\end<<3485>>tex2html_deferred<<3485>>
The following equation is the standard probability function that is used to determine when an uphill move is performed by the algorithm. This function is known in the literature as the \emph<<2684>>metropolis criterion<<2684>>. 
\begin<<2685>>equation<<2685>>
\label<<2686>>eq:saprobability<<2686>>
	M_<<2687>>AC<<2687>> =
	\begin<<2688>>cases<<2688>>
	1, ;SPMamp;\text<<2689>>if \begin<<3186>>tex2html_wrap_inline<<3186>>$f(y) \leq f(x)$\end<<3187>>tex2html_wrap_inline<<3187>><<2689>>\\ 
	e^<<3117>>-\frac<<2690>>\Delta E<<2690>><<2691>>T_k<<2691>><<3117>> , ;SPMamp;\text<<2692>>otherwise<<2692>>\\ 
	\end<<2693>>cases<<2693>>
\end<<2694>>equation<<2694>>
The function \begin<<3188>>tex2html_wrap_inline<<3188>>$f$\end<<3189>>tex2html_wrap_inline<<3189>>\space is the objective function or a function that determines the state of a given position in solution space\cite<<2695>>EcoEquilSA<<2695>>. The parameter \begin<<3190>>tex2html_wrap_inline<<3190>>$T_k$\end<<3191>>tex2html_wrap_inline<<3191>>\space is the temperature of the algorithm at iteration \begin<<3192>>tex2html_wrap_inline<<3192>>$k$\end<<3193>>tex2html_wrap_inline<<3193>>\space \cite<<2696>>EcoEquilSA<<2696>>. Finally, \begin<<3194>>tex2html_wrap_inline<<3194>>$\Delta E$\end<<3195>>tex2html_wrap_inline<<3195>>\space is the change in ``energy'' between two solutions \begin<<3196>>tex2html_wrap_inline<<3196>>$x$\end<<3197>>tex2html_wrap_inline<<3197>>\space and \begin<<3198>>tex2html_wrap_inline<<3198>>$y$\end<<3199>>tex2html_wrap_inline<<3199>>\space \cite<<2697>>EcoEquilSA<<2697>>.
\begin<<3486>>tex2html_deferred<<3486>>\par\end<<3487>>tex2html_deferred<<3487>>
The main purpose of the \gls<<2698>>SA<<2698>> algorithm (like most optimisation algorithms) is to minimise or maximise the cost function \cite<<2699>>SASingleMultiObj<<2699>>. This cost function evaluates a solution desirability compared with other solutions in the immediate \emph<<2700>>neighbourhood<<2700>> of the algorithm's current position \cite<<2701>>TheoPraticalSA<<2701>>. 
\begin<<3488>>tex2html_deferred<<3488>>\par\end<<3489>>tex2html_deferred<<3489>>
The immediate neighbourhood of solutions is generated based on a heuristic implemented by the algorithm designer\cite<<2702>>AIModernApproach<<2702>>. This heuristic, as with the \gls<<2703>>TS<<2703>> algorithm, can be simple or complex.
\begin<<3490>>tex2html_deferred<<3490>>\par\end<<3491>>tex2html_deferred<<3491>>
A neighbouring solution is only selected as the new best state if its desirability ranks higher than the current solution. When the algorithm moves to a better solution from the previous solution, the move is referred in the literature as a \emph<<2704>>downhill<<2704>> move \cite<<2705>>CurveFittingSA<<2705>>.
\begin<<3492>>tex2html_deferred<<3492>>\par\end<<3493>>tex2html_deferred<<3493>>
The best state is not always selected; in some cases the algorithm is also able to move to solutions that are worse than the current solution. A worse solution is only selected based on a probability which is controlled by the \emph<<2706>>annealing temperature<<2706>> of the algorithm \cite<<2707>>TheoPraticalSA<<2707>>. 
\begin<<3494>>tex2html_deferred<<3494>>\par\end<<3495>>tex2html_deferred<<3495>>
At a high annealing temperature the probability that the algorithm will select a bad solution is very good. As the annealing temperature decreases so does the probability that a bad solution will be selected \cite<<2708>>CurveFittingSA<<2708>>. When the algorithm moves to a worse solution, the move is referred to in the literature as an \emph<<2709>>uphill<<2709>> move \cite<<2710>>CurveFittingSA<<2710>>. Uphill moves allow the algorithm to break out of local minima and can lead the algorithm down a different path, which may ultimately result in obtaining the global optimum \cite<<2711>>SASingleMultiObj<<2711>>. 
\begin<<3496>>tex2html_deferred<<3496>>\par\end<<3497>>tex2html_deferred<<3497>>
As with the \gls<<2712>>TS<<2712>> algorithm, the standard \gls<<2713>>SA<<2713>> algorithm does not define a set neighbourhood generation mechanism; instead it is up to the algorithm designer to implement a suitable generation mechanism that will allow the algorithm to adequately explore the search space\cite<<2714>>VariousCoolingSA<<2714>>. 
\begin<<3498>>tex2html_deferred<<3498>>\par\end<<3499>>tex2html_deferred<<3499>>
The following section presents a discussion on various characteristics of the \gls<<2715>>SA<<2715>> algortihm.
\subsection<<2716>>Important Simulated Annealing Characteristics<<2716>>
There are four characteristics of the \gls<<2717>>SA<<2717>> algorithm that make the algorithm unique. One of the most important is the cooling schedule. 
\begin<<3500>>tex2html_deferred<<3500>>\par\end<<3501>>tex2html_deferred<<3501>>
\subsubsection<<2718>>Cooling Schedule<<2718>>
The cooling schedule/annealing Schedule is the most defining characteristic of the \gls<<2719>>SA<<2719>> algorithm. It is the procedure where the natural annealing process is mimicked. The temperature of the \gls<<2720>>SA<<2720>> algorithm is a control parameter that defines how much the algorithm moves around in the search space.
\begin<<3502>>tex2html_deferred<<3502>>\par\end<<3503>>tex2html_deferred<<3503>>
After each iteration, whether the algorithm has selected a new best solution or not, the temperature is reduced by a certain amount. This amount is determined by the \emph<<2721>>cooling schedule<<2721>>.
\begin<<3504>>tex2html_deferred<<3504>>\par\end<<3505>>tex2html_deferred<<3505>>
In general, when the \gls<<2722>>SA<<2722>> algorithm temperature has a very high value most solutions that are produced from the neighbourhood are accepted \cite<<2723>>ClusterSA<<2723>>. Thus the algorithm moves freely in the search space with little constraint. As the temperature decreases, the probability that the algorithm will select a bad or just any solution decreases\cite<<2724>>ClusterSA<<2724>>. When the temperature is very low, the \gls<<2725>>SA<<2725>> algorithm is similar to a greedy algorithm in the sense that it only accepts downhill movements\cite<<2726>>ClusterSA<<2726>>.
\begin<<3506>>tex2html_deferred<<3506>>\par\end<<3507>>tex2html_deferred<<3507>>
The cooling schedule provides the \gls<<2727>>SA<<2727>> algorithm with the critical ability to control the rate the algorithm transitions from the diversification phase (high temperature) to the intensification phase (low temperature)\cite<<2728>>ClusterSA<<2728>>. By controlling this rate, one is able to direct the algorithm to explore more early on to locate the more promosing areas for possible solutions. These promoosing areas can then be used by the algorithm in its intensification phase to find more promosing solutions.
\begin<<3508>>tex2html_deferred<<3508>>\par\end<<3509>>tex2html_deferred<<3509>>
In the literature there are three annealing schedules in common use, namely \emph<<2729>>the logarithmic schedule<<2729>>, the \emph<<2730>>geometric schedule<<2730>> and the \emph<<2731>>Cauchy schedule<<2731>>\cite<<2732>>VeryFastSAImageEnchancement,SASingleMultiObj<<2732>>. 
\begin<<3510>>tex2html_deferred<<3510>>\par\end<<3511>>tex2html_deferred<<3511>>
The standard and most commonly used schedule is known as the logarithmic schedule and is based on Boltzmann annealing \cite<<2733>>VeryFastSAImageEnchancement<<2733>>. The main disadvantage of this schedule is that is slow due to its logarithmic nature \cite<<2734>>VeryFastSAImageEnchancement<<2734>>. It also requires moves to be generated from a Gaussian distribution for it to be able to reach the global minimum\cite<<2735>>SASingleMultiObj<<2735>>. The logarithmic annealing function has the following form:
\begin<<2736>>equation<<2736>>
\label<<2737>>eq:logcooling<<2737>>
	T_k = \frac<<2738>>T_0<<2738>><<2739>>ln(k)<<2739>>,\text<<2740>>where k is the iteration value and <<2740>> k \neq 0\\ 
\end<<2741>>equation<<2741>>
Where \begin<<3200>>tex2html_wrap_inline<<3200>>$T_k$\end<<3201>>tex2html_wrap_inline<<3201>>\space is the temperature at iteration \begin<<3202>>tex2html_wrap_inline<<3202>>$k$\end<<3203>>tex2html_wrap_inline<<3203>>.
\begin<<3512>>tex2html_deferred<<3512>>\par\end<<3513>>tex2html_deferred<<3513>>
The Cauchy schedule is faster than the logarithmic schedule. Similar to the logarithmic, this schedule also has a movement requirement. Moves must be generated from a Cauchy distribution for the algorithm to be able to reach the global minimum \cite<<2742>>SASingleMultiObj,VeryFastSAImageEnchancement<<2742>>. The Cauchy schedule is also referred to as fast annealing\cite<<2743>>VeryFastSAImageEnchancement<<2743>>. The schedule has the following form:
\begin<<2744>>equation<<2744>>
\label<<2745>>eq:cauchycooling<<2745>>
	T_k = \frac<<2746>>T_0<<2746>><<2747>>k<<2747>> ~, k \neq 0
\end<<2748>>equation<<2748>>
\begin<<3514>>tex2html_deferred<<3514>>\par\end<<3515>>tex2html_deferred<<3515>>
Finally, the fastest annealing schedule is known as the geometric or exponential annealing schedule \cite<<2749>>SASingleMultiObj<<2749>>. The schedule has no move generation requirement to reach the global minimum, since there is no regorous proof in the literature \cite<<2750>>SASingleMultiObj<<2750>>. By using the geometric schedule the \gls<<2751>>SA<<2751>> temperatures are rescaled which is called \emph<<2752>>re-annealing<<2752>>\cite<<2753>>VeryFastSAImageEnchancement<<2753>>.
\begin<<3516>>tex2html_deferred<<3516>>\par\end<<3517>>tex2html_deferred<<3517>>
The geometric schedule has the following form:
\begin<<2754>>equation<<2754>>
\label<<2755>>eq:geocooling<<2755>>
	T(k)=T_0e^<<2756>>-C_k<<2756>>,\text<<2757>>where C is a constant<<2757>>
\end<<2758>>equation<<2758>>
\begin<<3518>>tex2html_deferred<<3518>>\par\end<<3519>>tex2html_deferred<<3519>>
\subsubsection<<2759>>Initial Temperature<<2759>>
The initial temperature is a very important parameter to define in the \gls<<2760>>SA<<2760>> algorithm, since it defines a point from which the cooling schedule will start\cite<<2761>>VariousCoolingSA<<2761>>. Therefore, depending on what the initial value of the temperature, is the final result that the algorithm will produce can be influenced\cite<<2762>>SALongestCommon,AutoConfigSA<<2762>>.
\begin<<3520>>tex2html_deferred<<3520>>\par\end<<3521>>tex2html_deferred<<3521>>
When the initial temperature is set to a very high value, the algorithm takes a long time to reach a result since the search space is being explored more\cite<<2763>>SALongestCommon,VariousCoolingSA<<2763>>. More exploration is favourable for \gls<<2764>>SA<<2764>> as it lets the algorithm be less susceptible to local minimum. 
\begin<<3522>>tex2html_deferred<<3522>>\par\end<<3523>>tex2html_deferred<<3523>>
If the initial temperature is set to a very low temperature, the algorithm might converge too quickly and thus produce a result which may be the local minimum\cite<<2765>>SALongestCommon,VariousCoolingSA,AutoConfigSA<<2765>>.
\begin<<3524>>tex2html_deferred<<3524>>\par\end<<3525>>tex2html_deferred<<3525>>
The initial temperature together with the cooling factor allows the algorithm designer to define the time window for the algorithm to escape local minima, as well as the rate of convergence to an optimum solution\cite<<2766>>SALongestCommon,VariousCoolingSA<<2766>>.
\begin<<3526>>tex2html_deferred<<3526>>\par\end<<3527>>tex2html_deferred<<3527>>
A low initial temperature together with a low cooling factor makes the time window for the algorithm to leave a local optimum very small\cite<<2767>>SALongestCommon<<2767>>. With a high initial temperature and cooling factor value that is almost 1, the time window for the algorithm to leave the local optimum is much larger \cite<<2768>>SALongestCommon<<2768>>. 
\begin<<3528>>tex2html_deferred<<3528>>\par\end<<3529>>tex2html_deferred<<3529>>
When the algorithm is near a global optimum, a low initial temperature and low cooling factor will allow the algorithm to reach the optimum faster in the search space\cite<<2769>>SALongestCommon<<2769>>. In contrast, if a high temperature and a very low cooling factor are used, the algorithm will take longer to reach the optimum even though it is near the global optimum\cite<<2770>>SALongestCommon<<2770>>.
\begin<<3530>>tex2html_deferred<<3530>>\par\end<<3531>>tex2html_deferred<<3531>>
\subsubsection<<2771>>Move Generation<<2771>>
Most of the research done on the \gls<<2772>>SA<<2772>> algorithm focuses on the annealing schedule and not so much on the move/solution/neighbourhood generation. Typically an initial solution is generated and then small changes are made to the solution to represent a new solution. The solution is said to be perturbed to the next solution.
\begin<<3532>>tex2html_deferred<<3532>>\par\end<<3533>>tex2html_deferred<<3533>>
Move generation is the phase where neighbouring solutions to the current solution are generated. It is the ideal section for an algorithm designer to embed domain-specific knowledge which will allow the algorithm to generate better possible solutions.
\begin<<3534>>tex2html_deferred<<3534>>\par\end<<3535>>tex2html_deferred<<3535>>
In research done by Tseung and Lin \cite<<2773>>CurveFittingSA<<2773>> an initial solution is not modified, but a move generation technique known as \emph<<2774>>pattern<<2774>> search is used. Pattern search has two forms of movement, namely the exploratory move and the patten move. The exploratory move continually changes the certain variables of a solution \cite<<2775>>CurveFittingSA<<2775>>. This is done so that the technique can rapidly find and identify a ``downhill'' move. The pattern move uses the information gathered by the exploratory move to move towards the minimum of the function \cite<<2776>>CurveFittingSA<<2776>>.
\subsubsection<<2777>>Algorithm Efficiency<<2777>>
The algorithm is also efficient with regard to CPU cycles when compared with the genetic algorithm. \gls<<2778>>SA<<2778>> only has to evaluate a certain number of moves each iteration, instead of a whole population of individuals each iteration. The genetic algorithm is discussed in section~\ref<<2779>>sec:geneticalgorithm<<2779>>.
\begin<<3536>>tex2html_deferred<<3536>>\par\end<<3537>>tex2html_deferred<<3537>>
Unlike \gls<<2780>>TS<<2780>>, the basic \gls<<2781>>SA<<2781>> algorithm does not keep any memory and is therefore memory efficient, but in contrast suffers the risk that the solution may cycle. The more iterations spent at a temperature, the longer the algorithm spends at a certain temperature and therefore the higher the probability that solutions may cycle.
\begin<<3538>>tex2html_deferred<<3538>>\par\end<<3539>>tex2html_deferred<<3539>>
In next section the flow of the \gls<<2782>>SA<<2782>> algorithm is discussed and pseudo code for the \gls<<2783>>SA<<2783>> algortihm is presented.
\begin<<3540>>tex2html_deferred<<3540>>\par\end<<3541>>tex2html_deferred<<3541>>
\subsection<<2784>>Flow of the Algorithm<<2784>>
In an attempt to better understand how the \gls<<2785>>SA<<2785>> algorithm operates, a general discussion on the flow of the algorithm will now be given using algorithm~\ref<<2786>>alg:SA<<2786>> as a reference point.
\begin<<2787>>algorithm<<2787>>[H]
\begin<<3542>>tex2html_deferred<<3542>>\caption<<3118>>Basic Simulated Annealing Algorithm\cite<<2788>>VeryFastSAImageEnchancement,ChaosSA<<2788>><<3118>>\end<<3543>>tex2html_deferred<<3543>>
\label<<2789>>alg:SA<<2789>>
	\begin<<2790>>algorithmic<<2790>>[1]
		\State Initialize parameters
		\State Set starting temperature \begin<<3204>>tex2html_wrap_inline<<3204>>$T(0)$\end<<3205>>tex2html_wrap_inline<<3205>>
    \State \begin<<3206>>tex2html_wrap_inline<<3206>>$\hat<<2791>>x_0<<2791>> \leftarrow$\end<<3207>>tex2html_wrap_inline<<3207>>\space Generate initial starting solution
		\While<<2792>>Stopping criterion not met<<2792>>
    \State \begin<<3208>>tex2html_wrap_inline<<3208>>$\hat<<2793>>y_i<<2793>> \leftarrow$\end<<3209>>tex2html_wrap_inline<<3209>>\space Generate neighbouring solutions to \begin<<3210>>tex2html_wrap_inline<<3210>>$\hat<<2794>>x_i<<2794>>$\end<<3211>>tex2html_wrap_inline<<3211>>
    \State Evaluate \begin<<3212>>tex2html_wrap_inline<<3212>>$\hat<<2795>>y_i<<2795>>$\end<<3213>>tex2html_wrap_inline<<3213>>\space neighbours with fitness function \begin<<3214>>tex2html_wrap_inline<<3214>>$f(\hat<<2796>>y_i<<2796>>)$\end<<3215>>tex2html_wrap_inline<<3215>>
    \State Calculate probability \begin<<3216>>tex2html_wrap_inline<<3216>>$\hat<<2797>>p_i<<2797>>$\end<<3217>>tex2html_wrap_inline<<3217>>\space of \begin<<3218>>tex2html_wrap_inline<<3218>>$\hat<<2798>>y_i<<2798>>$\end<<3219>>tex2html_wrap_inline<<3219>>\space neighbours with equation~\ref<<2799>>eq:saprobability<<2799>>
    \State \begin<<3220>>tex2html_wrap_inline<<3220>>$\hat<<2800>>x_i<<2800>> \leftarrow$\end<<3221>>tex2html_wrap_inline<<3221>>\space Select \begin<<3222>>tex2html_wrap_inline<<3222>>$\hat<<2801>>y_i<<2801>>$\end<<3223>>tex2html_wrap_inline<<3223>>\space neighbour based on probability \begin<<3224>>tex2html_wrap_inline<<3224>>$p_i$\end<<3225>>tex2html_wrap_inline<<3225>>
			\State Reduce temperature \begin<<3226>>tex2html_wrap_inline<<3226>>$T(i)$\end<<3227>>tex2html_wrap_inline<<3227>>\space based on cooling schedule
		\EndWhile
    \State Return best solution \begin<<3228>>tex2html_wrap_inline<<3228>>$\hat<<2802>>x_i<<2802>>$\end<<3229>>tex2html_wrap_inline<<3229>>
	\end<<2803>>algorithmic<<2803>>
\end<<2804>>algorithm<<2804>>
\begin<<3544>>tex2html_deferred<<3544>>\par\end<<3545>>tex2html_deferred<<3545>>
From lines 1 -- 3, the \gls<<2805>>SA<<2805>> algorithm is initialised. The most important step here is setting the starting temperature for the annealing process to start. As mentioned in the introduction, the temperature of the annealing process plays a critical role in the potential solution selection process. After the algorithm has been initialised the search phase of the algorithm starts which ranges from lines 4 -- 10. Like the \gls<<2806>>TS<<2806>> algorithm, the \gls<<2807>>SA<<2807>> algorithm starts the search phase by generating a number of neighbours to the current solution held by the algorithm as can be observed in line 5.
\begin<<3546>>tex2html_deferred<<3546>>\par\end<<3547>>tex2html_deferred<<3547>>
Before selecting a neighbour the algorithm first needs to evaluate the generated neighbours. It evaluates each neighbour by applying a fitness function \begin<<3230>>tex2html_wrap_inline<<3230>>$f(y_i)$\end<<3231>>tex2html_wrap_inline<<3231>>\space in order to determine its fitness.
Once the fitness of all the generated neighbours has been determined, the algorithm uses equation~\ref<<2808>>eq:saprobability<<2808>> to calculate the probability of selecting a particular neighbour for all the generated neighbours as well. The probability calculation can be observed to occur in line 7. The algorithm then selects the neighbour with the highest probability to be the current solution, as observed in line 9. 
\begin<<3548>>tex2html_deferred<<3548>>\par\end<<3549>>tex2html_deferred<<3549>>
Before the algorithm advances to the next iteration the temperature needs to be lowered. As discussed, the temperature is lowered according to a particular cooling schedule. In the algorithm the process of lowering the temperature occurs in line 10. 
\begin<<3550>>tex2html_deferred<<3550>>\par\end<<3551>>tex2html_deferred<<3551>>
This concludes the discussion on the flow of the SA algorithm. In the section that follows a discussion is given on when \gls<<2809>>SA<<2809>> is applied to the \gls<<2810>>FAP<<2810>>.
\subsection<<3119>>Simulated Annealing on the \gls<<2811>>FAP<<2811>><<3119>>
The \gls<<2812>>SA<<2812>> algorithm, as with the \gls<<2813>>TS<<2813>> algorithm, has achieved good results in other optimisation problems, as mentioned in section \ref<<2814>>sec:SAIntroduction<<2814>>. Due to its success on other NP-Complete optimisation problems, the \gls<<2815>>SA<<2815>> algorithm has also been applied to the \gls<<2816>>FAP<<2816>>.
\begin<<3552>>tex2html_deferred<<3552>>\par\end<<3553>>tex2html_deferred<<3553>>
In literature by Carlo Mannino and Gianpaolo Oriolo\cite<<2817>>SolvingSuperIntervalGraphs<<2817>> the \gls<<2818>>SA<<2818>> algorithm is applied to the \gls<<2819>>FAP<<2819>>. The resulting \gls<<2820>>SA<<2820>> algorithm was benchmarked on the \gls<<2821>>COST<<2821>> 259 Siemens benchmark instances. The results obtained by the authors are presented in table~\ref<<2822>>tab:SA<<2822>>. 
\begin<<3554>>tex2html_deferred<<3554>>\par\end<<3555>>tex2html_deferred<<3555>>
\begin<<2823>>table<<2823>>[H]
\begin<<3556>>tex2html_deferred<<3556>>\centering\end<<3557>>tex2html_deferred<<3557>>
	\begin<<2824>>tabular<<2824>><<2825>>| c | c | c | c |<<2825>>
	\hline
	Problem instance ;SPMamp; \gls<<2826>>SA<<2826>> ;SPMamp; \gls<<2827>>COST<<2827>> 259 (old) ;SPMamp; Best \gls<<2828>>COST<<2828>> 259 \\  \hline
	Siemens1 ;SPMamp; 22.96 ;SPMamp; 23.00 ;SPMamp; 2.200\\  \hline
	Siemens2 ;SPMamp; 14.72 ;SPMamp; 14.75 ;SPMamp; 14.280\\  \hline
	Siemens3 ;SPMamp; 52.43 ;SPMamp; 52.55 ;SPMamp; 5.19\\  \hline
	Siemens4 ;SPMamp; 80.96 ;SPMamp; 80.80 ;SPMamp; 81.89\\  \hline
	\end<<2829>>tabular<<2829>>
\begin<<3558>>tex2html_deferred<<3558>>\caption<<3120>>SA on \gls<<2830>>COST<<2830>> 259 Benchmark<<3120>>\end<<3559>>tex2html_deferred<<3559>>
\label<<2831>>tab:SA<<2831>>
\end<<2832>>table<<2832>>
\begin<<3560>>tex2html_deferred<<3560>>\par\end<<3561>>tex2html_deferred<<3561>>
Values represent the total interference generated by the frequency assignment. Note that the values represented in the SA column, are the values as presented in the reasearch by the authors. Also, note that the column \gls<<2833>>COST<<2833>> 259 (old) represents the best obtained results to the instances as it was when the authors presented their research.
An overview of the \gls<<2834>>SA<<2834>> algorithm along with its unique characteristics have now been given. Utilising the knowledge that was gained from understanding how the \gls<<2835>>SA<<2835>> algorithm operates a critical evaluation can be presented. In the following paragraphs, a theoretical critical evaluation is presented that lists the various characteristics which would be problematic if the \gls<<2836>>SA<<2836>> algorithm would be applied to the \gls<<2837>>FAP<<2837>>.
\begin<<3562>>tex2html_deferred<<3562>>\par\end<<3563>>tex2html_deferred<<3563>>
\paragraph<<2838>>Cooling Schedule<<2838>>
--- Depending on the cooling schedule selected, the algorithm might converge too quickly. As discussed previously the cooling schedule reduces the temperature. The temperature plays a large part in the determination of whether a particular solution will be moved to or not in an iteration. Thus early on the algorithm will explore a lot more (diversification) and later on will exploit more (intensification). In the \gls<<2839>>FAP<<2839>>, the algorithm must not only be able to explore and exploit, but also be able to return to an exploration phase if need be.
As the temperature becomes colder the \gls<<2840>>SA<<2840>> algorithm exploits more and therefore will not easily move to a worse off solution. In the \gls<<2841>>FAP<<2841>>, it might be desirable to rather move a worse off solution later on, as the particular current solution is a local minimum and yields bad neighbours as potential next solutions. With the cooling schedule this is simply not possible, unless the temperature and schedule are reset. Resetting the temperature and schedule is not ideal, since the algorithm keeps no history and might risk making the same faults as before the reset.
\paragraph<<2842>>Neighbourhood generation<<2842>>
--- The \gls<<2843>>SA<<2843>> algorithm, as with the \gls<<2844>>TS<<2844>> algorithm, has no set process that defines how neighbours should be generated. As discussed, neighbourhood generation is the primary means by which the \gls<<2845>>SA<<2845>> algorithm moves about the search space in search of an optimal solution. Therefore applying the \gls<<2846>>SA<<2846>> algorithm would require a custom neighbourhood generation scheme. A desireble neighbourhood generation scheme would be one that keeps track of where the algorithm has been previously. By keeping history, the algorithm will be able to avoid previously explored areas in the search space.
\paragraph<<2847>>Single solution based search<<2847>>
--- The \gls<<2848>>SA<<2848>> algorithm is similar to the \gls<<2849>>TS<<2849>> algorithm in the sense that it only searches from one solution per iteration. It searches by generating neighbours around the current solution of the algorithm. The \gls<<2850>>FAP<<2850>> search space is huge; hence it would be more efficient to have multiple current solutions from which neighbours are generated. This enables the algorithm to explore the search space much more efficiently at the expense of more computational resources.
\begin<<3564>>tex2html_deferred<<3564>>\par\end<<3565>>tex2html_deferred<<3565>>
\section<<2851>>Genetic Algorithm<<2851>>
\label<<2852>>sec:geneticalgorithm<<2852>>
\subsection<<2853>>Introduction<<2853>>
The genetic algorithm (GA) is a stochastic search method that is based on the natural process of genetic evolution and the Darwinian concept of ``survival of the fittest'' \cite<<2854>>DistributedHierarchicalGA,AcceleratingGA,AdaptiveSAGA,FamilyGA<<2854>>. The \gls<<2855>>GA<<2855>> was first proposed by Fraser, but it was not till the research presented by Holland that \gls<<2856>>GA<<2856>>'s became popular\cite<<2857>>CompuIntelligenceIntro<<2857>>. Holland initially applied the algorithm to adaptive systems but has been widely used in the optimisation field of study due to its success on multidimensional problems\cite<<2858>>ParallelGASA,DistributedHierarchicalGA,FamilyGA<<2858>>. The \gls<<2859>>GA<<2859>> developed by Holland is referred to in the literature as the Canonical \gls<<2860>>GA<<2860>> (CGA)\cite<<2861>>CompuIntelligenceIntro<<2861>>.
\begin<<3566>>tex2html_deferred<<3566>>\par\end<<3567>>tex2html_deferred<<3567>>
The wide use of the \gls<<2862>>GA<<2862>> can also be attributed to its generic algorithm structure as well as the ease of implementation \cite<<2863>>FamilyGA,AdaptiveSAGA<<2863>>. The \gls<<2864>>GA<<2864>> defines generic operators that select, create and mutate individuals for the next population of the next generation\cite<<2865>>CompuIntelligenceIntro<<2865>>. It should be noted that initially the mutation operator was not a required to be part of the \gls<<2866>>GA<<2866>>. Only after successive implementations of the \gls<<2867>>GA<<2867>> showcased the explorative power that the mutation operator brings tot he search capability of the \gls<<2868>>GA<<2868>> was the mutation operator considered to be important\cite<<2869>>CompuIntelligenceIntro<<2869>>. 
\begin<<3568>>tex2html_deferred<<3568>>\par\end<<3569>>tex2html_deferred<<3569>>
The \gls<<2870>>GA<<2870>> search procedure involves searching the solution space through artificial evolution and natural selection\cite<<2871>>FamilyGA,MultiPopGA,HybridIntelliGA<<2871>>. An individual or point in the search space is known as a \emph<<2872>>chromosome<<2872>> \cite<<2873>>HumanPassiveGA<<2873>>. An initial set of chromosomes (referred to in the research as the \emph<<2874>>population<<2874>>) is randomly generated to form the starting population\cite<<2875>>FamilyGA,HybridIntelliGA,AcceleratingGA,MultiPopGA<<2875>>. 
\begin<<3570>>tex2html_deferred<<3570>>\par\end<<3571>>tex2html_deferred<<3571>>
A chromosome consists of a sequence of smaller parts called \emph<<2876>>genes<<2876>>\cite<<2877>>CompuIntelligenceIntro<<2877>>. The sequence upon which these genes appear in the chromosome determines the characteristics of an individual\cite<<2878>>CompuIntelligenceIntro<<2878>>. In the \gls<<2879>>GA<<2879>> a single gene represents a single variable of a solution\cite<<2880>>FamilyGA,AcceleratingGA<<2880>>. A whole chromosome therefore represents a solution\cite<<2881>>FamilyGA,AcceleratingGA<<2881>>. Exactly how best to represent a chromosome as a solution is problem dependent\cite<<2882>>CompuIntelligenceIntro<<2882>>.
\begin<<3572>>tex2html_deferred<<3572>>\par\end<<3573>>tex2html_deferred<<3573>>
According to the evolution theory proposed by Darwin individuals of a population with the best chromosome have the best chance to survive and to reproduce\cite<<2883>>CompuIntelligenceIntro<<2883>>. These individuals are reffered to as the fittest individuals of the population. In a \gls<<2884>>GA<<2884>> population each individual is ``rated'' to determine how good the solution its chromosome represents \cite<<2885>>CompuIntelligenceIntro<<2885>>. The rating of a chromosome is referred to as its fitness value and is also problem dependent\cite<<2886>>CompuIntelligenceIntro<<2886>>.
\begin<<3574>>tex2html_deferred<<3574>>\par\end<<3575>>tex2html_deferred<<3575>>
Determining the fitness value of a chromosome is achieved by means of a \emph<<2887>>fitness function<<2887>>. The fitness function is a mathematical function which maps the chromosome representation to a scalar value\cite<<2888>>CompuIntelligenceIntro<<2888>>. An optimisation problem has a objective function which calculates how good a solution is, therefore in the \gls<<2889>>GA<<2889>> the fitness function represents the objective function\cite<<2890>>CompuIntelligenceIntro<<2890>>.
\begin<<3576>>tex2html_deferred<<3576>>\par\end<<3577>>tex2html_deferred<<3577>>
When the fitness of individuals in a population have been determined the \gls<<2891>>GA<<2891>> probablisticly selects individuals for the next generation\cite<<2892>>CompuIntelligenceIntro<<2892>>. The selection probability is referred to as the \emph<<2893>>selective pressure<<2893>>\cite<<2894>>CompuIntelligenceIntro<<2894>>. The individuals that are selected by the operator enter the reproduction phase of the \gls<<2895>>GA<<2895>> where offspring are created\cite<<2896>>CompuIntelligenceIntro<<2896>>.
\begin<<3578>>tex2html_deferred<<3578>>\par\end<<3579>>tex2html_deferred<<3579>>
Offspring are created by combining parts of one or more chromosomes to form a new chromosome this procedure is called the \emph<<2897>>crossover<<2897>>\cite<<2898>>CompuIntelligenceIntro<<2898>>. The chromomsomes used in the production of the offspring are referred to as the parent chromosomes\cite<<2899>>CompuIntelligenceIntro<<2899>>.
\begin<<3580>>tex2html_deferred<<3580>>\par\end<<3581>>tex2html_deferred<<3581>>
With regard to how offspring and parents are handled, there are two forms of the \gls<<2900>>GA<<2900>> \cite<<2901>>FamilyGA<<2901>>. One form is called the \emph<<2902>>generational<<2902>> \gls<<2903>>GA<<2903>>  and the other form is known as the \emph<<2904>>steady-state<<2904>> \gls<<2905>>GA<<2905>> \cite<<2906>>GeostatisticalGA,FamilyGA<<2906>>.
\begin<<3582>>tex2html_deferred<<3582>>\par\end<<3583>>tex2html_deferred<<3583>>
With the generational \gls<<2907>>GA<<2907>> the offspring are not immediately used in the next generation; instead they are kept in a pool until the pool reaches a required size \cite<<2908>>FamilyGA<<2908>>. The offspring are then used to replace the parents entirely in the next generation \cite<<2909>>FamilyGA<<2909>>. In the steady-state \gls<<2910>>GA<<2910>> once offspring has been created a selection operator is applied once more. The operator selects individuals from the old population and from the offspring to form the new population for the next generation \cite<<2911>>GeostatisticalGA,FamilyGA<<2911>>.
\begin<<3584>>tex2html_deferred<<3584>>\par\end<<3585>>tex2html_deferred<<3585>>
The \gls<<2912>>GA<<2912>> search process moves around in the search space using probabilistic rules rather than deterministic rules \cite<<2913>>FamilyGA<<2913>>. The probabilistic transition rules aid the algorithm to avoid local optima regions in the search space \cite<<2914>>HybridIntelliGA<<2914>>. Note that by even using the probabilistic transition rules there is no guarantee that the \gls<<2915>>GA<<2915>> will completely avoid local optima\cite<<2916>>CompuIntelligenceIntro<<2916>>.
\begin<<3586>>tex2html_deferred<<3586>>\par\end<<3587>>tex2html_deferred<<3587>>
The \gls<<2917>>GA<<2917>> makes no assumptions about the search space and primarily works on the information provided by the chromosomes and the representation of solutions used\cite<<2918>>CompuIntelligenceIntro,ConstrainedGA,HybridIntelliGA<<2918>>. Unlike the \gls<<2919>>SA<<2919>> and \gls<<2920>>TS<<2920>> algorithms, the \gls<<2921>>GA<<2921>> if properly initialized, starts with a number of search points that cover a wide area of the search space. Depending on the operators used, diversity can be quickly lost and the population can become homogeneous very quickly\cite<<2922>>DistributedHierarchicalGA,FamilyGA,HybridIntelliGA<<2922>>\label<<2923>>GASearchPoints<<2923>>.
\begin<<3588>>tex2html_deferred<<3588>>\par\end<<3589>>tex2html_deferred<<3589>>
The different operators used by the GA along with their probalities are not specific. Each operator and probability can be changed the suit the problem domain the GA is being applied to. Therefore the particular probability and operators the GA uses is problem dependant.
\begin<<3590>>tex2html_deferred<<3590>>\par\end<<3591>>tex2html_deferred<<3591>>
As discussed earlier the defined operators consists of a selection operator, crossover operator and mutation operator\cite<<2924>>SelfAdaptiveGA,MultiPopGA<<2924>>. In the section that follows each operator is discussed.
\begin<<3592>>tex2html_deferred<<3592>>\par\end<<3593>>tex2html_deferred<<3593>>
\subsection<<2925>>Important Genetic Algorithm Characteristics<<2925>>
The various operators used by the \gls<<2926>>GA<<2926>> makes the procedure unique and is one of its defining characteristics. The first is the characteristic to be discussed is the selection operator.
\begin<<3594>>tex2html_deferred<<3594>>\par\end<<3595>>tex2html_deferred<<3595>>
\subsubsection<<2927>>Selection Operator<<2927>>
The selection operator is the first operator to be applied to the population after each generation. The operator determines which individuals will be used to create offspring \cite<<2928>>CoactiveFuzzyGA,CombinedBranchBoundGA,ConstrainedGA<<2928>>.
\begin<<3596>>tex2html_deferred<<3596>>\par\end<<3597>>tex2html_deferred<<3597>>
The individuals are selected based on the selective pressure and are moved to the ``mating pool''. Individuals that are selected by selection operator are used by the crossover and mutation operators (in the reproduction phase), to generate offspring from the mating pool\cite<<2929>>AdaptiveSAGA,AcceleratingGA<<2929>>.
\begin<<3598>>tex2html_deferred<<3598>>\par\end<<3599>>tex2html_deferred<<3599>>
By favouring high fitness individuals above low fitness individuals the operator is said to have a high selective pressure\cite<<2930>>CompuIntelligenceIntro<<2930>>. Care must be taken if the operator uses a high selective pressure. Since high fitness individuals are preferred, diversity among the individuals in the population will deteriorate and thus result in premature convergence\cite<<2931>>ConstrainedGA, CompuIntelligenceIntro<<2931>>.
\begin<<3600>>tex2html_deferred<<3600>>\par\end<<3601>>tex2html_deferred<<3601>>
If the search space is known to have only one optimum, then a high selection pressure is feasible\cite<<2932>>ConstrainedGA<<2932>>. The selection operator directs the search into a gradient-based direction that converges on a single optimum solution \cite<<2933>>ConstrainedGA<<2933>>. 
\begin<<3602>>tex2html_deferred<<3602>>\par\end<<3603>>tex2html_deferred<<3603>>
In contrast, with a search space that is known to have multiple optima a low selection pressure is more benificial to the \gls<<2934>>GA<<2934>>\cite<<2935>>ConstrainedGA<<2935>>. Low selection pressure allows the population of the \gls<<2936>>GA<<2936>> to explore the search space more\cite<<2937>>ConstrainedGA<<2937>>.
\begin<<3604>>tex2html_deferred<<3604>>\par\end<<3605>>tex2html_deferred<<3605>>
The means by which the selection operator selects individuals is referred to as the selection scheme. The most widely adopted selection scheme is known as the \emph<<2938>>roulette wheel<<2938>> selection scheme \cite<<2939>>ConstrainedGA,GeostatisticalGA,HybridBaldwinGA,CoactiveFuzzyGA<<2939>>. With this scheme an individual is selected based on a probability defined by the fitness of the individual divided by the collective fitness of the population \cite<<2940>>GeostatisticalGA<<2940>>.
\begin<<3606>>tex2html_deferred<<3606>>\par\end<<3607>>tex2html_deferred<<3607>>
Depending on how complicated the objective function is and how large the population is, the selection phase may be the most computationally expensive as well as time consuming \cite<<2941>>AcceleratingGA<<2941>>. The selection process as described below  can also be applied by the crossover operator. This process was opted to be split from the crossover operator as to provide a better overview of how the algorithm works
\begin<<3608>>tex2html_deferred<<3608>>\par\end<<3609>>tex2html_deferred<<3609>>
After the offspring has been generated, a selection operator is applied again\cite<<2942>>CompuIntelligenceIntro<<2942>>. The purpose of this selection operator is to determine the population of the next generation\cite<<2943>>CompuIntelligenceIntro<<2943>>. Within this selection process replacement policies can \emph<<2944>>optionally<<2944>> be applied\cite<<2945>>CompuIntelligenceIntro<<2945>>. 
\begin<<3610>>tex2html_deferred<<3610>>\par\end<<3611>>tex2html_deferred<<3611>>
These replacement policies define which individuals of the current population should be replaced by the newly generated offspring\cite<<2946>>CompuIntelligenceIntro<<2946>>. For instance, the policy can define that the entire current population should be replaced by the offspring\cite<<2947>>CompuIntelligenceIntro<<2947>>. Using this policy is not ideal as some generated offspring could have much worse fitness than their parents but due to the replacement the solutions represented by the good parents are lost\cite<<2948>>CompuIntelligenceIntro<<2948>>. 
\begin<<3612>>tex2html_deferred<<3612>>\par\end<<3613>>tex2html_deferred<<3613>>
Other replacement policies include replacing the worst individual in the current population with offspring on the premise that the offspring has a better fitness\cite<<2949>>CompuIntelligenceIntro<<2949>>. By replacing older chromosomes with better performing chromosomes in the population replacement strategy, the \gls<<2950>>GA<<2950>> achieves an hill-climbing ability. The reader that is interested in more information on different selection operators is directed to the suvery by Engelbrecht\cite<<2951>>CompuIntelligenceIntro<<2951>>.
\subsubsection<<2952>>Crossover Operator<<2952>>
\label<<2953>>sec:crossover<<2953>>
The crossover operator is the first operator applied to the population in the reproduction phase. The crossover operates exclusively on the chromosomes in the mating pool. Crossover works by interchanging genes from one or more parent chromosomes. The parent chromosomes are selected with a probability from the mating pool to produce a single chromosome known as the offspring \cite<<2954>>FamilyGA,HumanPassiveGA,CoactiveFuzzyGA<<2954>>. 
\begin<<3614>>tex2html_deferred<<3614>>\par\end<<3615>>tex2html_deferred<<3615>>
The probability is known as the \emph<<2955>>crossover probability<<2955>> and is problem dependent\cite<<2956>>CompuIntelligenceIntro<<2956>>. By defining a high crossver probability the good genes that form the current population are retained in the next population\cite<<2957>>CompuIntelligenceIntro<<2957>>. With good genes being retained more historical information is passed onto the next generation's population\cite<<2958>>FamilyGA<<2958>>.
\begin<<3616>>tex2html_deferred<<3616>>\par\end<<3617>>tex2html_deferred<<3617>>
There are a variety of ways in which genes are interchanged between chromosomes in the crossover operation i.e. fixed point crossover, two point crossover, uniform crossover and gaussian crossover\cite<<2959>>CompuIntelligenceIntro<<2959>>. All of these crossovers operate on the premise that a byte representation is used for the chromosomes. 
\begin<<3618>>tex2html_deferred<<3618>>\par\end<<3619>>tex2html_deferred<<3619>>
With the byte representation, each chromosome is a byte. Each byte is made up of sequence of bits. As discussed previously each chromosome is made out of genes and therefore each bit is a gene\cite<<2960>>CompuIntelligenceIntro<<2960>>.
\begin<<3620>>tex2html_deferred<<3620>>\par\end<<3621>>tex2html_deferred<<3621>>
Fixed point crossover operates on binary parents whereby a point is selected in one parent and then all other bits are replaced by the other parents' bits \cite<<2961>>HumanPassiveGA<<2961>>. Two point crossover generates two random indices which dictate a certain segment in the one parent to be interchanged with the other parent \cite<<2962>>ConstrainedGA<<2962>>. Where a segment consists of a subset of genes from the chromosome. 
\begin<<3622>>tex2html_deferred<<3622>>\par\end<<3623>>tex2html_deferred<<3623>>
The uniform crossover is the most basic of all crossovers since it randomly selects bits from one parent to be replaced by another parent's bits\cite<<2963>>ParallelGASA,GeostatisticalGA<<2963>>. Finally, the Gaussian crossover also uses the byte representation. The Gaussian crossover interchanges bits between parents based on a Gaussian distribution \cite<<2964>>ParallelGASA,GeostatisticalGA<<2964>>. Depending on the state of the algorithm, crossover operators can also be interchanged or even paired if the algorithm needs better search performance for large or small solution spaces \cite<<2965>>HetergeneousGA,ParallelGASA<<2965>>.
\begin<<3624>>tex2html_deferred<<3624>>\par\end<<3625>>tex2html_deferred<<3625>>
Caution should be exercised by the crossover operator when selecting chromosomes for reproduction. It is possible for the operator to select the same chromosome twice to be the parent of a single offspring\cite<<2966>>CompuIntelligenceIntro<<2966>>. The operator is also at risk of selecting the same chromosome multiple times for reproduction\cite<<2967>>CompuIntelligenceIntro<<2967>>. The opertator must therefore incorporate a test to detect unenecessary repeated usage of a chromosome\cite<<2968>>CompuIntelligenceIntro<<2968>>. Finally, it is important to note that there exist strategies for both Gaussian crossover and mutation based operators for use with continuous-valued representations\cite<<2969>>FundamentalSwarm<<2969>>.
\begin<<3626>>tex2html_deferred<<3626>>\par\end<<3627>>tex2html_deferred<<3627>>
\subsubsection<<2970>>Mutation Operator<<2970>>
The mutation operator is a probabilistic operator and is applied to individials in the offspring population with a probability referred to as the mutation rate\cite<<2971>>CompuIntelligenceIntro<<2971>>. The purpose of the mutation operator is to increase the diversity of the genes of a individual chromosomes\cite<<2972>>CompuIntelligenceIntro<<2972>>. By introducing new genes into an individual the diversity of the populations characteristics are increased\cite<<2973>>CoactiveFuzzyGA,AcceleratingGA,ConstrainedGA<<2973>>.
\begin<<3628>>tex2html_deferred<<3628>>\par\end<<3629>>tex2html_deferred<<3629>>
The mutation operator has no previous information on the chromosome it is mutating; thus it is entirely possible that the mutation may modify the chromosome for the worse \cite<<2974>>AcceleratingGA<<2974>>. A worse solution might lead the algorithm out of local optima or lead it down a new path to find the global optima, but this is not always the case \cite<<2975>>AdaptiveSAGA,FamilyGA,ConstrainedGA<<2975>>. It is for this reason that it is recommended that the mutation rate be set to a low value to ensure good solutions are not distored too much\cite<<2976>>CompuIntelligenceIntro<<2976>>.
\begin<<3630>>tex2html_deferred<<3630>>\par\end<<3631>>tex2html_deferred<<3631>>
In a survey done by Engelbrecht\cite<<2977>>CompuIntelligenceIntro<<2977>> another mutation operator is discussed. Instead of mutating a small part of randomly selected chromosomes, this operator generates new offspring to be inserted back into the population. The operator randomly generates a new chromosome and then uses any of the previously discussed crossover operators (see page~\pageref<<2978>>sec:crossover<<2978>>).
\begin<<3632>>tex2html_deferred<<3632>>\par\end<<3633>>tex2html_deferred<<3633>>
The mutation operator is not always a basic random replacement of genes operation. In research done by Il-kwon Jeong and Ju-jang Lee\cite<<2979>>AdaptiveSAGA<<2979>>, a mutation operator is presented that uses the \gls<<2980>>SA<<2980>> algorithm to determine the genes that need to be replaced. The \gls<<2981>>SA<<2981>> mutation operator generates a new chromosome from which genes are used to replace in the chromosome being mutated \cite<<2982>>AdaptiveSAGA<<2982>>. Addition mutation operators are discussed in Engelbrecht\cite<<2983>>CompuIntelligenceIntro<<2983>>.
\subsubsection<<2984>>Initial Population Generation<<2984>>
Initial population generation is the very first activity that the \gls<<2985>>GA<<2985>> performs. Out of this population potential mating candidates are selected based on their fitness, which indicates desirability. The initial population is generated by means of randomisation \cite<<2986>>SelfAdaptiveGA<<2986>>. Since the algorithm searches multiple points simultaneously in the search space, it is desirable that the initial population have a wide diversity with regard to the problem search space\cite<<2987>>CombinedBranchBoundGA,DistributedHierarchicalGA<<2987>>. By controlling the initial population generation we can control, to a small degree, the amount of exploration the algorithm does initially\cite<<2988>>CombinedBranchBoundGA<<2988>>. Therefore care must be taken in the selection of the particular randomisation scheme that will be used to generate chromosomes for individuals.
\begin<<3634>>tex2html_deferred<<3634>>\par\end<<3635>>tex2html_deferred<<3635>>
In a survey done by Andrea Reese\cite<<2989>>RandomNumberGA<<2989>>, two randomisation schemes were defined, namely  \glspl<<2990>>PRNG<<2990>> and \glspl<<2991>>QRNG<<2991>>. \glspl<<2992>>PRNG<<2992>> were found to be heavily problem dependent, improving the search efficiency in some instances and in other instances having no considerable impact. QRNGs, on the other hand, were shown to significantly improve the final solution produced by the \gls<<2993>>GA<<2993>> as well as lower the number of generations for the solution to be obtained \cite<<2994>>RandomNumberGA<<2994>>.
\begin<<3636>>tex2html_deferred<<3636>>\par\end<<3637>>tex2html_deferred<<3637>>
\subsubsection<<2995>>Algorithm Efficiency<<2995>>
The \gls<<2996>>GA<<2996>> is a powerful, yet simple algorithm and tends to find good solutions given enough time, it does have its disadvantages. One of the major disadvantages occurs when the \gls<<2997>>GA<<2997>> is applied to problems that have very large solution spaces. In these problems, the population size is a very sensitive parameter\cite<<2998>>AdaptiveSAGA,HetergeneousGA,SelfAdaptiveDataMiningGA,PatternDetectionGA<<2998>>. If the population is too small the algorithm will not have enough diversity to search and will tend to converge prematurely. 
\begin<<3638>>tex2html_deferred<<3638>>\par\end<<3639>>tex2html_deferred<<3639>>
A large population is preferred in large search spaces in order to get good chromosome diversity among individuals. Hence, the population size must be fine-tuned to achieve optimal performance in large search spaces \cite<<2999>>AdaptiveSAGA,CompuIntelligenceIntro<<2999>>. Note an increase in population does not guarantee good chromomsome diversity among the population. As discussed in initial populaiton generation it is also dependant on the random generation scheme used as well. 
\begin<<3640>>tex2html_deferred<<3640>>\par\end<<3641>>tex2html_deferred<<3641>>
In the next section the pseudo code for the \gls<<3000>>GA<<3000>> algorithm is presented and the flow of the \gls<<3001>>GA<<3001>> algorithm is discussed
\subsection<<3002>>Flow of the Algorithm<<3002>>
The core concepts of the genetic algorithm were introduced in the previous section. To better understand the algorithm, a general overview of the algorithm will be presented in this section using algorithm~\ref<<3003>>alg:GA<<3003>> as a reference point.
\begin<<3004>>algorithm<<3004>>[H]
\begin<<3642>>tex2html_deferred<<3642>>\caption<<3121>>Basic Genetic Algorithm Algorithm\cite<<3005>>FamilyGA,AdaptiveSAGA,DistributedHierarchicalGA,SelfAdaptiveGA<<3005>><<3121>>\end<<3643>>tex2html_deferred<<3643>>
\label<<3006>>alg:GA<<3006>>
	\begin<<3007>>algorithmic<<3007>>[1]
		\State \begin<<3232>>tex2html_wrap_inline<<3232>>$pop_n\leftarrow$\end<<3233>>tex2html_wrap_inline<<3233>>Initialize population
		\While<<3008>>Stopping criteria is not met<<3008>>
    \State Evaluate individuals of population with fitness function \begin<<3234>>tex2html_wrap_inline<<3234>>$f(\hat<<3009>>x_i<<3009>>)$\end<<3235>>tex2html_wrap_inline<<3235>>
    \State \begin<<3236>>tex2html_wrap_inline<<3236>>$\hat<<3010>>y_k<<3010>> \leftarrow$\end<<3237>>tex2html_wrap_inline<<3237>>\space Select parent individuals from population using selection operator
		\Repeat
    \For<<3129>>Each chromosome \begin<<3238>>tex2html_wrap_inline<<3238>>$\hat<<3011>>g_i<<3011>>$\end<<3239>>tex2html_wrap_inline<<3239>>\space in \begin<<3240>>tex2html_wrap_inline<<3240>>$\hat<<3122>>y_<<3012>>k-1<<3012>><<3122>>$\end<<3241>>tex2html_wrap_inline<<3241>><<3129>>
    \State \begin<<3242>>tex2html_wrap_inline<<3242>>$\hat<<3013>>c_i<<3013>> \leftarrow$\end<<3243>>tex2html_wrap_inline<<3243>>\space calculate crossover porbability for \begin<<3244>>tex2html_wrap_inline<<3244>>$\hat<<3014>>g_i<<3014>>$\end<<3245>>tex2html_wrap_inline<<3245>>
    \If<<3123>>\begin<<3246>>tex2html_wrap_inline<<3246>>$\hat<<3015>>c_i<<3015>> \geq$\end<<3247>>tex2html_wrap_inline<<3247>>\space Crossover threshold<<3123>>
    \State \begin<<3248>>tex2html_wrap_inline<<3248>>$\hat<<3016>>g_i<<3016>> \leftarrow$\end<<3249>>tex2html_wrap_inline<<3249>>\space Apply crossover operator to \begin<<3250>>tex2html_wrap_inline<<3250>>$\hat<<3017>>g_i<<3017>>$\end<<3251>>tex2html_wrap_inline<<3251>>
				\EndIf
        \State \begin<<3252>>tex2html_wrap_inline<<3252>>$\hat<<3018>>o_i<<3018>> \leftarrow \hat<<3019>>g_i<<3019>>$\end<<3253>>tex2html_wrap_inline<<3253>>
        \State \begin<<3254>>tex2html_wrap_inline<<3254>>$\hat<<3020>>m_i<<3020>>\leftarrow$\end<<3255>>tex2html_wrap_inline<<3255>>\space Calculate Mutation probability for \begin<<3256>>tex2html_wrap_inline<<3256>>$\hat<<3021>>o_i<<3021>>$\end<<3257>>tex2html_wrap_inline<<3257>>
        \If<<3124>>\begin<<3258>>tex2html_wrap_inline<<3258>>$\hat<<3022>>m_i<<3022>> \geq$\end<<3259>>tex2html_wrap_inline<<3259>>\space Mutation threshold<<3124>>
        \State Apply mutation operator to \begin<<3260>>tex2html_wrap_inline<<3260>>$\hat<<3023>>o_i<<3023>>$\end<<3261>>tex2html_wrap_inline<<3261>>
				\EndIf
        \State Add offspring chromosome \begin<<3262>>tex2html_wrap_inline<<3262>>$\hat<<3024>>o_i<<3024>>$\end<<3263>>tex2html_wrap_inline<<3263>>\space to \begin<<3264>>tex2html_wrap_inline<<3264>>$new_<<3025>>pop<<3025>>$\end<<3265>>tex2html_wrap_inline<<3265>>
			\EndFor
		\Until<<3125>>\begin<<3266>>tex2html_wrap_inline<<3266>>$size(new_<<3026>>pop<<3026>>) = size(pop_n)$\end<<3267>>tex2html_wrap_inline<<3267>><<3125>>
		\State \begin<<3268>>tex2html_wrap_inline<<3268>>$pop_n \leftarrow$\end<<3269>>tex2html_wrap_inline<<3269>>\space select new population from \begin<<3270>>tex2html_wrap_inline<<3270>>$pop_n$\end<<3271>>tex2html_wrap_inline<<3271>>\space and \begin<<3272>>tex2html_wrap_inline<<3272>>$new_<<3027>>pop<<3027>>$\end<<3273>>tex2html_wrap_inline<<3273>>
		\EndWhile
    \State \begin<<3274>>tex2html_wrap_inline<<3274>>$\hat<<3028>>x_i<<3028>> \leftarrow$\end<<3275>>tex2html_wrap_inline<<3275>>\space Determine best chromosome in \begin<<3276>>tex2html_wrap_inline<<3276>>$pop_n$\end<<3277>>tex2html_wrap_inline<<3277>>
    \State Return best solution \begin<<3278>>tex2html_wrap_inline<<3278>>$\hat<<3029>>x_i<<3029>>$\end<<3279>>tex2html_wrap_inline<<3279>>
	\end<<3030>>algorithmic<<3030>>
\end<<3031>>algorithm<<3031>>
The \gls<<3032>>GA<<3032>> algorithm is a population-based algorithm and therefore needs to initialise its population. Each individual of the population represents a potential solution. Population initialisation occurs in line 1 of algorithm~\ref<<3033>>alg:GA<<3033>> on page~\pageref<<3034>>alg:GA<<3034>>. 
\begin<<3644>>tex2html_deferred<<3644>>\par\end<<3645>>tex2html_deferred<<3645>>
The amount of individuals in a population to be generated is predefined and is known as the population size. In algorithm~\ref<<3035>>alg:GA<<3035>> on page~\pageref<<3036>>alg:GA<<3036>> the population size is represented by the subscript \begin<<3280>>tex2html_wrap_inline<<3280>>$n$\end<<3281>>tex2html_wrap_inline<<3281>>\space in \begin<<3282>>tex2html_wrap_inline<<3282>>$pop_n$\end<<3283>>tex2html_wrap_inline<<3283>>, where \begin<<3284>>tex2html_wrap_inline<<3284>>$n ;SPMgt; 0$\end<<3285>>tex2html_wrap_inline<<3285>>.
\begin<<3646>>tex2html_deferred<<3646>>\par\end<<3647>>tex2html_deferred<<3647>>
Before the algorithm can start \emph<<3037>>evolving<<3037>> its population, it first needs to determine each individual in the population's fitness. The fitness of an individual is calculated using a fitness function \begin<<3286>>tex2html_wrap_inline<<3286>>$f(\hat<<3038>>x_i<<3038>>)$\end<<3287>>tex2html_wrap_inline<<3287>>.
\begin<<3648>>tex2html_deferred<<3648>>\par\end<<3649>>tex2html_deferred<<3649>>
After each individual within the initial population has been evaluated, the algorithm is able to start its searching process, which starts at line 3 and ends at line 17 of algorithm~\ref<<3039>>alg:GA<<3039>> on page~\pageref<<3040>>alg:GA<<3040>>.
\begin<<3650>>tex2html_deferred<<3650>>\par\end<<3651>>tex2html_deferred<<3651>>
Since each individual has a fitness value after being evaluated, the selection operator is applied. The selection operator used on line 4 determine which individuals will form part of the parents used to generate offspring for the next population.
\begin<<3652>>tex2html_deferred<<3652>>\par\end<<3653>>tex2html_deferred<<3653>>
Once the selection operator has selected the parent individuals needed for the next population, the algorithm is ready to enter the reproduction phase, which ranges from lines 5 -- 17 of algorithm~\ref<<3041>>alg:GA<<3041>> on page~\pageref<<3042>>alg:GA<<3042>>.
\begin<<3654>>tex2html_deferred<<3654>>\par\end<<3655>>tex2html_deferred<<3655>>
In the reproduction phase the crossover and mutation operators are applied probabilisticly. For the crossover operator a crossover probability is calculated on line 7. Depending if the calculated probability satisfies the crossover probability the crossover operator is applied on line 9. Depending on the crossover used, offspring are generated using one or more individuals as parents. The resulting offspring from the crossover operation is assigned to \begin<<3288>>tex2html_wrap_inline<<3288>>$\hat<<3043>>g_i<<3043>>$\end<<3289>>tex2html_wrap_inline<<3289>>.
\begin<<3656>>tex2html_deferred<<3656>>\par\end<<3657>>tex2html_deferred<<3657>>
On line 11 the value of \begin<<3290>>tex2html_wrap_inline<<3290>>$\hat<<3044>>g_i<<3044>>$\end<<3291>>tex2html_wrap_inline<<3291>>\space is assigned to the variable \begin<<3292>>tex2html_wrap_inline<<3292>>$\hat<<3045>>o_i<<3045>>$\end<<3293>>tex2html_wrap_inline<<3293>>\space which represents the offspring. From lines 7 -- 11 it can be observed that a particular individual does not have to take part in a crossover operation to be carried over to the new population. Thus knowledge gained from the current population is persisted based on the crossover propbability.
\begin<<3658>>tex2html_deferred<<3658>>\par\end<<3659>>tex2html_deferred<<3659>>
The second step of the reproduction phase is where the mutation operator is applied. For each of the offspring a mutation probability is calculated. If the calculated probability for a particular offspring are high enough, the algorithm enters the mutation phase. In the mutation phase an individual is selected and the mutation operator is applied. Application of the mutation operator can be seen to occur in lines 13 -- 15 of algorithm~\ref<<3046>>alg:GA<<3046>> on page~\pageref<<3047>>alg:GA<<3047>>.
\begin<<3660>>tex2html_deferred<<3660>>\par\end<<3661>>tex2html_deferred<<3661>>
Regardless of whether the offspring has been mutated or not, the resulting offspring are added to the new population. The reproduction phase continually loops, until the new population equals the size of the initial starting population. Once the amount of individuals in the new population has reached the population size \begin<<3294>>tex2html_wrap_inline<<3294>>$n$\end<<3295>>tex2html_wrap_inline<<3295>>, the algorithm moves on to its next iteration.
\begin<<3662>>tex2html_deferred<<3662>>\par\end<<3663>>tex2html_deferred<<3663>>
After the algorithm has completed the reproduction phase the algorithm selects the new population to be used in the next generation. The new population is selected from the current population and the created offspring.
\begin<<3664>>tex2html_deferred<<3664>>\par\end<<3665>>tex2html_deferred<<3665>>
The algorithm continually generates a new population for each generation until a predetermined stopping criterion has been met. Once the criterion has been met, the algorithm selects the individual with the best fitness in the current population as its most optimal solution. The following section discusses literature where the \gls<<3048>>GA<<3048>> has been applied to the \gls<<3049>>FAP<<3049>>.
\begin<<3666>>tex2html_deferred<<3666>>\par\end<<3667>>tex2html_deferred<<3667>>
\subsection<<3126>>Genetic Algorithm on the \gls<<3050>>FAP<<3050>><<3126>>
Continuing the trend of the \gls<<3051>>SA<<3051>> and \gls<<3052>>TS<<3052>> algorithms, the \gls<<3053>>GA<<3053>> has also been applied to a wide variety of problems. These problems include: solving nonconvex nonlinear programming problems\cite<<3054>>GANonConvex<<3054>>, data mining \cite<<3055>>SelfAdaptiveDataMiningGA<<3055>> and auto configuring metaheuristic algorithms for complex combinatorial problems \cite<<3056>>AutoComplexMeta<<3056>>.
\begin<<3668>>tex2html_deferred<<3668>>\par\end<<3669>>tex2html_deferred<<3669>>
Colombo and Allen\cite<<3057>>ProblemDecompMIFAP<<3057>> have developed a \gls<<3058>>GA<<3058>> to be applied on the \gls<<3059>>FAP<<3059>>. The authors decomposed the \gls<<3060>>FAP<<3060>> into smaller subproblems. On average the solution quality is improved by using the technique but at the expense of more complex and taxing evaluations that have to be performed\cite<<3061>>ProblemDecompMIFAP<<3061>>. 
\begin<<3670>>tex2html_deferred<<3670>>\par\end<<3671>>tex2html_deferred<<3671>>
In table~\ref<<3062>>tab:GA<<3062>> the results obtained by authors on the \gls<<3063>>COST<<3063>> 259 benchmarks are compared. The listed values are scalars representing the total interference generated by the frequency plan.
\begin<<3064>>table<<3064>>[H]
\begin<<3672>>tex2html_deferred<<3672>>\centering\end<<3673>>tex2html_deferred<<3673>>
	\begin<<3065>>tabular<<3065>><<3066>>| c | c | c |<<3066>>
	\hline
	Problem instance ;SPMamp; \gls<<3067>>GA<<3067>> ;SPMamp; Best \gls<<3068>>COST<<3068>> 259 \\  \hline
	Siemens 1 ;SPMamp; 2.60 ;SPMamp; 2.20 \\  \hline
	Siemens 2 ;SPMamp; 16.34 ;SPMamp; 14.280 \\  \hline
	Siemens 3 ;SPMamp; 6.37 ;SPMamp; 5.19 \\  \hline
	Siemens 4 ;SPMamp; 84.08 ;SPMamp; 81.89 \\  \hline
	\end<<3069>>tabular<<3069>>
\begin<<3674>>tex2html_deferred<<3674>>\caption<<3127>>GA on \gls<<3070>>COST<<3070>> 259 Benchmark<<3127>>\end<<3675>>tex2html_deferred<<3675>>
\label<<3071>>tab:GA<<3071>>
\end<<3072>>table<<3072>>
\begin<<3676>>tex2html_deferred<<3676>>\par\end<<3677>>tex2html_deferred<<3677>>
As per the results in table~\ref<<3073>>tab:GA<<3073>> the \gls<<3074>>GA<<3074>> produces good results coming close to the best obtained results in the benchmark. By critically reasoning about the \gls<<3075>>GA<<3075>> if it were applied to the \gls<<3076>>FAP<<3076>> in theory, the following disadvantages can be identified:
\begin<<3678>>tex2html_deferred<<3678>>\par\end<<3679>>tex2html_deferred<<3679>>
\paragraph<<3077>>Diversity<<3077>>
--- The \gls<<3078>>GA<<3078>> continually operates on a set population that is randomly initialised at the beginning of the algorithm. It therefore only has this set of generated genes in the initialised population to therefore evolve successive populations.
If one disregards mutation, the \gls<<3079>>GA<<3079>> is a process by which the optimal combinations of the starting genes are found. Thus, the \gls<<3080>>GA<<3080>> purifies the starting population genes in an attempt to find those individual genes, which if combined into a single individual, will produce an optimal individual, i.e. solution. Therefore the \gls<<3081>>GA<<3081>> is very reliant on the quality of the random generator used. The probability of the algorithm finding a particular desirable gene that is exhibited by the starting population is directly related to the random number generator used to initialise the population. 
\paragraph<<3082>>Crossover<<3082>>
--- The crossover operation in the \gls<<3083>>GA<<3083>> is the only means by which successive populations are generated and can therefore be regarded as the primary means by which the algorithm performs its search. As the crossover is defined in the standard algorithm, certain parts of both parents are copied and combined to form a new individual. With regard to the \gls<<3084>>FAP<<3084>>, if each individual represents a frequency plan, the crossover operation would copy certain cells from the two parent plans. This is not desirable, since a single channel within a cell can generate major interference which overshadows the rest of the channels that generate low interference in the cell. Thus, for the \gls<<3085>>GA<<3085>> to generate high quality solutions on the \gls<<3086>>FAP<<3086>>, the algorithm would be better off utilising a crossover operation which works on individual channels assigned rather than cells. Crossover operation is also a memory and computationally expensive operation since individuals need to be constantly created and values need to be copied to these new individuals from the respective parents.
\paragraph<<3087>>Mutation<<3087>>
--- Mutation is a means by which more diversity is introduced into the chromosomes of the invididuals.  As discussed, the mutation operator introduces new genes to existing chromosome which can lead to an excellent solution being distorted and becoming one of the worst solutions. With regard to the \gls<<3088>>FAP<<3088>>, the low probability of mutation is not desirable, as the \gls<<3089>>FAP<<3089>> search space is huge and therefore requires constant diversity to be introduced to accurately explore it. A possible good mutation would be one that is slightly more intelligent than the standard mutation operator, which just randomly modifies a selected individual. An intelligent mutation would be one that takes into account the recent history of the individual as well as the history of the population and, based on the collective knowledge alters or \emph<<3090>>mutates<<3090>> a particular individual. Each chromosome would therefore be required to keep history of changes made to itself, to allow the mutation operator to take it into account. Another means of applying the mutation operator is to set the mutation rate proprtional to an individuals fitness. By modifying the mutation rate in this manner, mutating good individuals can be avoided.
\section <<3091>>Summary<<3091>>
In this chapter a description was given of metaheuristic algorithms. What it means for an algorithm to be classified as being of a metaheuristic nature was explained as well as the characteristics these algorithms exhibit were identified.
\begin<<3680>>tex2html_deferred<<3680>>\par\end<<3681>>tex2html_deferred<<3681>>
Three metaheuristic algorithms were discussed in this chapter. For every algorithm discussed an explanation was given of how the algorithm works as well as the various characteristics that make the algorithm unique.
\begin<<3682>>tex2html_deferred<<3682>>\par\end<<3683>>tex2html_deferred<<3683>>
For each algorithm, a brief overview of studies using the particular algorithm was given as well as some of the disadvantages or challenges that would be faced when applying the particular algorithm to the \gls<<3092>>FAP<<3092>>.
\begin<<3684>>tex2html_deferred<<3684>>\par\end<<3685>>tex2html_deferred<<3685>>
The first algorithm discussed was the tabu search algorithm and the second was the simulated annealing algorithm. The chapter concluded with the genetic algorithm. 
\begin<<3686>>tex2html_deferred<<3686>>\par\end<<3687>>tex2html_deferred<<3687>>
In the table~\ref<<3093>>tab:summary1<<3093>>, the algorithms discussed in this chapter and their performance on the \gls<<3094>>COST<<3094>> 259 set of benchmarks have been summarised.
\begin<<3095>>table<<3095>>[H]
\label<<3096>>tbl:summaryMetaTable<<3096>>
\begin<<3097>>center<<3097>>
	\begin<<3098>>tabular<<3098>><<3099>>| c | c | c | c | c |<<3099>>
	\hline
	Problem instance ;SPMamp; \gls<<3100>>TS<<3100>> ;SPMamp; \gls<<3101>>SA<<3101>> ;SPMamp; \gls<<3102>>GA<<3102>> ;SPMamp; \gls<<3103>>COST<<3103>> 259 \\  \hline
	Siemens1 ;SPMamp; 2.7692 ;SPMamp; 23.00 ;SPMamp; 2.60 ;SPMamp; 2.20 \\  \hline
	Siemens2 ;SPMamp; 14.9360 ;SPMamp; 14.75 ;SPMamp; 16.34 ;SPMamp; 14.280 \\  \hline
	Siemens3 ;SPMamp; 6.6496 ;SPMamp; 52.55 ;SPMamp; 6.37 ;SPMamp; 5.19 \\  \hline
	Siemens4 ;SPMamp; 110.9725 ;SPMamp; 80.80 ;SPMamp; 84.08 ;SPMamp; 81.89 \\  \hline
	\end<<3104>>tabular<<3104>>
\begin<<3688>>tex2html_deferred<<3688>>\caption<<3128>>Summary of algorithm performance on the \gls<<3105>>COST<<3105>> 259 benchmark<<3128>>\end<<3689>>tex2html_deferred<<3689>>
\label<<3106>>tab:summary1<<3106>>
\end<<3107>>center<<3107>>
\end<<3108>>table<<3108>>
\begin<<3690>>tex2html_deferred<<3690>>\par\end<<3691>>tex2html_deferred<<3691>>
In the next chapter the class of algorithm that is relatively new to optimisation research, namely the swarm intelligence class of algorithms, is discussed.
<tex2html_endfile>#./chpt4.tex#
